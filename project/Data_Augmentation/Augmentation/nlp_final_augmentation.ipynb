{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bWWYFOPlh23R"
      },
      "source": [
        "# Data Augmentation for Document Information Extraction\n",
        "\n",
        "This notebook performs data augmentation on a dataset for document information extraction. The goal is to create synthetic variations of the original documents to increase the size and diversity of the training data, which can help improve model robustness.\n",
        "\n",
        "The augmentation techniques implemented include:\n",
        "- **Entity Swapping:** Replacing mentions of entities with other mentions of the same type and length within the document.\n",
        "- **Mask and Fill:** Masking a non-entity word and using a language model (BERT) to predict a replacement.\n",
        "- **Paraphrasing:** Using a large language model (GPT-3.5-turbo) to rewrite sentences while preserving entities and meaning."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hsVrQRO1h90t"
      },
      "source": [
        "## Setup and Dependencies\n",
        "\n",
        "This section installs the required libraries and downloads necessary models for the augmentation process.\n",
        "\n",
        "- `transformers`: Used for the BERT masked language model pipeline.\n",
        "- `spacy`: Used for tokenization, sentence boundary detection, and potentially other NLP tasks (though primarily tokenization/sentencization here).\n",
        "- `openai`: Used for the LLM paraphrasing function.\n",
        "\n",
        "We also download the `en_core_web_sm` spaCy model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JkjQf_TnhqKB",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers spacy tqdm\n",
        "!spacy download en_core_web_sm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nZoAEFtZhqKC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "pip install openai==0.28"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-29T07:31:19.662751Z",
          "iopub.status.busy": "2025-05-29T07:31:19.662225Z",
          "iopub.status.idle": "2025-05-29T07:31:19.666115Z",
          "shell.execute_reply": "2025-05-29T07:31:19.665479Z",
          "shell.execute_reply.started": "2025-05-29T07:31:19.662724Z"
        },
        "id": "2-2CYcSHhqKC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ[\"OPENAI_API_KEY\"] = \"key\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j0Y7zEAbiH4F"
      },
      "source": [
        "## Model and NLP Initialization\n",
        "\n",
        "Here, we initialize the natural language processing tools used in the augmentation functions:\n",
        "\n",
        "- **spaCy (`en_core_web_sm`):** Loaded for basic tokenization and crucially, for sentence boundary detection using the 'sentencizer' component, which is important for the paraphrasing function.\n",
        "- **BERT (`bert-base-uncased`):** Loaded and configured as a `fill-mask` pipeline. This is used in the `mask_and_fill` function to suggest replacement words.\n",
        "\n",
        "The OpenAI API key is loaded from environment variables, and the paraphrasing function is only enabled if the key is available."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uBRFfBIFiNiH"
      },
      "source": [
        "## Augmentation Helpers\n",
        "\n",
        "These functions implement the core data augmentation logic:\n",
        "\n",
        "- `swap_entities(doc)`: Swaps entity mentions within a document. It identifies mentions of the same type and length and randomly replaces a portion of them.\n",
        "- `mask_and_fill(doc)`: Selects a non-entity word in the document (within the first 128 words), masks it, and uses the BERT `fill-mask` pipeline to predict a replacement word.\n",
        "- `paraphrase_llm(doc)`: Uses the OpenAI API to paraphrase the document sentence by sentence, ensuring that tagged entities are preserved and not altered by the LLM.\n",
        "- Helper functions like `_collect_mentions`, `_wrap_mentions`, and `_unwrap_mentions` assist in processing entity data and preparing text for the LLM."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "duYPisSFiQKm"
      },
      "source": [
        "## Main Driver (`augment_file`)\n",
        "\n",
        "The `augment_file` function orchestrates the augmentation process for a single input JSON file.\n",
        "\n",
        "For each document in the input file:\n",
        "1. The original document is included in the output list.\n",
        "2. Copies of the document are passed to each of the augmentation helper functions (`swap_entities`, `mask_and_fill`, `paraphrase_llm`).\n",
        "3. If an augmentation function successfully generates a new document, it is added to the output list.\n",
        "\n",
        "The augmented data for each input file is saved to a corresponding output file. The `overwrite` flag controls whether existing output files are overwritten."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-zxHlWoQiUrR"
      },
      "source": [
        "## Execution and File Processing\n",
        "\n",
        "The script defines a list of datasets (train and dev) and their corresponding input and output directories.\n",
        "\n",
        "It then iterates through each dataset:\n",
        "- It finds all JSON files ending with `_all_examples.json` in the input directory.\n",
        "- For each found file, it calls the `augment_file` function to perform the augmentation.\n",
        "\n",
        "Finally, the script zips the entire `/kaggle/working/` directory, which contains the augmented data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "colab": {
          "referenced_widgets": [
            "cfe08a8e0654495ba32b6091c997e1e9",
            "96dd1a034d1b4a3d8a7411074a7be728",
            "7f5b94d27e6b4ff3ba9737fda4aa8b39",
            "cbac7bf52fd04a60a0d61556429a5074",
            "2a53fdd5f9ac46438b06fa4284568724"
          ]
        },
        "execution": {
          "iopub.execute_input": "2025-05-29T07:31:21.257856Z",
          "iopub.status.busy": "2025-05-29T07:31:21.257341Z",
          "iopub.status.idle": "2025-05-29T07:47:21.571240Z",
          "shell.execute_reply": "2025-05-29T07:47:21.570584Z",
          "shell.execute_reply.started": "2025-05-29T07:31:21.257832Z"
        },
        "id": "es1wKiBThqKC",
        "outputId": "1ad7331f-46a8-456a-923c-31ef376448e6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2025-05-29 07:31:32.295821: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1748503892.466012     108 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1748503892.512578     108 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "Loading spaCy model …\n",
            "Added 'sentencizer' to spaCy pipeline.\n",
            "Loading BERT fill‑mask pipeline …\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cfe08a8e0654495ba32b6091c997e1e9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96dd1a034d1b4a3d8a7411074a7be728",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "7f5b94d27e6b4ff3ba9737fda4aa8b39",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "cbac7bf52fd04a60a0d61556429a5074",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2a53fdd5f9ac46438b06fa4284568724",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Device set to use cuda:0\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "==================================================\n",
            "Processing train set...\n",
            "Input dir: /kaggle/input/nlp-second-try/DocIE_dataset_final_version/train\n",
            "Output dir: /kaggle/working/aug_train\n",
            "==================================================\n",
            "\n",
            "Found 5 JSON files to process for train set.\n",
            "\n",
            "Starting augmentation for file: Communication_all_examples.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Communication_all_examples.json: 100%|██████████| 10/10 [02:23<00:00, 14.39s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WRITE] /kaggle/working/aug_train/Communication_all_examples.json (×40 docs)\n",
            "\n",
            "Starting augmentation for file: Government_all_examples.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Government_all_examples.json:   0%|          | 0/9 [00:00<?, ?it/s]You seem to be using the pipelines sequentially on GPU. In order to maximize efficiency please use a dataset\n",
            "Government_all_examples.json:  11%|█         | 1/9 [00:36<04:51, 36.38s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] Paraphrased chunk too short for: The fall of the [[E213]]Derg[[/E213]] was a milita... Using original.\n",
            "[WARN] Paraphrased chunk too short for: Subsequently, they retook military outpost of [[E1... Using original.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Government_all_examples.json:  22%|██▏       | 2/9 [00:53<02:57, 25.34s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] Paraphrased chunk too short for: [[E2[[E153]]5[[/E153]]]]Fullmetal [[E88]]Alc[[E1[[... Using original.\n",
            "[WARN] Paraphrased chunk too short for: E83]]Tim [[E114]]Marcoh[[/E114]][[/E83]][[/E[[E153... Using original.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Government_all_examples.json:  33%|███▎      | 3/9 [01:07<01:58, 19.75s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] Paraphrased chunk too short for: E147]]He[[/E147]] added that [[E1[[E153]]5[[/E153]... Using original.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Government_all_examples.json: 100%|██████████| 9/9 [02:09<00:00, 14.43s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] Paraphrased chunk too short for: In [[E141]]1940[[/E141]], [[E9]][[E116]][[E125]]Ge... Using original.\n",
            "[WRITE] /kaggle/working/aug_train/Government_all_examples.json (×36 docs)\n",
            "\n",
            "Starting augmentation for file: Entertainment_all_examples.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entertainment_all_examples.json:  25%|██▌       | 3/12 [00:32<01:40, 11.17s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] Paraphrased chunk too short for: [[E59]]Dumb Ways to Die[[/E59]] is an [[E127]][[E1... Using original.\n",
            "[WARN] Paraphrased chunk too short for: It featured characters known as \"[[E160]]Beans[[/E... Using original.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Entertainment_all_examples.json: 100%|██████████| 12/12 [02:31<00:00, 12.66s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WRITE] /kaggle/working/aug_train/Entertainment_all_examples.json (×48 docs)\n",
            "\n",
            "Starting augmentation for file: Energy_all_examples.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Energy_all_examples.json: 100%|██████████| 10/10 [01:51<00:00, 11.17s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WRITE] /kaggle/working/aug_train/Energy_all_examples.json (×40 docs)\n",
            "\n",
            "Starting augmentation for file: Education_all_examples.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Education_all_examples.json:  10%|█         | 1/10 [00:09<01:25,  9.55s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WARN] Paraphrased chunk too short for: [[E[[E43]]1[[/E43]]5]]Campus [[E[[E44]]2[[/E44]]0]... Using original.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Education_all_examples.json: 100%|██████████| 10/10 [01:37<00:00,  9.71s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WRITE] /kaggle/working/aug_train/Education_all_examples.json (×40 docs)\n",
            "\n",
            "==================================================\n",
            "Processing dev set...\n",
            "Input dir: /kaggle/input/nlp-second-try/DocIE_dataset_final_version/dev\n",
            "Output dir: /kaggle/working/aug_dev\n",
            "==================================================\n",
            "\n",
            "Found 2 JSON files to process for dev set.\n",
            "\n",
            "Starting augmentation for file: Human_behavior_all_examples.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Human_behavior_all_examples.json: 100%|██████████| 13/13 [03:43<00:00, 17.22s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WRITE] /kaggle/working/aug_dev/Human_behavior_all_examples.json (×51 docs)\n",
            "\n",
            "Starting augmentation for file: Internet_all_examples.json\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Internet_all_examples.json: 100%|██████████| 10/10 [01:15<00:00,  7.50s/it]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[WRITE] /kaggle/working/aug_dev/Internet_all_examples.json (×37 docs)\n",
            "\n",
            "\n",
            "Data augmentation process finished.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import copy\n",
        "import json\n",
        "import os\n",
        "import random\n",
        "import re\n",
        "import sys\n",
        "from pathlib import Path\n",
        "from typing import Dict, List\n",
        "\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "\n",
        "try:\n",
        "    import openai  # paraphrase\n",
        "except ImportError:  # pragma: no cover\n",
        "    openai = None  # type: ignore\n",
        "\n",
        "from transformers import AutoModelForMaskedLM, AutoTokenizer, pipeline\n",
        "\n",
        "\n",
        "############################################################\n",
        "# Model & NLP initialisation                               #\n",
        "############################################################\n",
        "\n",
        "FILL_MODEL_NAME = \"bert-base-uncased\"\n",
        "MASK_PROBABILITY = 0.3  # 30 % of entities to swap\n",
        "FIRST_N_WORDS_FOR_MASK = 128 # Defines the scope within which to pick a word for masking\n",
        "\n",
        "print(\"Loading spaCy model …\", file=sys.stderr)\n",
        "NLP = spacy.load(\"en_core_web_sm\", disable=[\"parser\", \"ner\"])  # Start with tokenizer\n",
        "# Add the sentencizer component to the pipeline for sentence boundary detection\n",
        "if not NLP.has_pipe(\"sentencizer\"):\n",
        "    try:\n",
        "        NLP.add_pipe(\"sentencizer\")\n",
        "        print(\"Added 'sentencizer' to spaCy pipeline.\", file=sys.stderr)\n",
        "    except Exception as e:\n",
        "        print(f\"Could not add 'sentencizer'. Trying to re-enable parser for sentence boundaries if needed: {e}\", file=sys.stderr)\n",
        "        # Fallback or further investigation might be needed if add_pipe fails\n",
        "        # For now, this should generally work.\n",
        "else:\n",
        "    print(\"Sentencizer already in spaCy pipeline.\", file=sys.stderr)\n",
        "\n",
        "\n",
        "print(\"Loading BERT fill‑mask pipeline …\", file=sys.stderr)\n",
        "_fill_tokenizer = AutoTokenizer.from_pretrained(FILL_MODEL_NAME)\n",
        "_fill_model = AutoModelForMaskedLM.from_pretrained(FILL_MODEL_NAME)\n",
        "FILL_MASK = pipeline(\"fill-mask\", model=_fill_model, tokenizer=_fill_tokenizer, top_k=5)\n",
        "MASK_TOKEN = _fill_tokenizer.mask_token\n",
        "\n",
        "OPENAI_ENABLED = openai is not None and os.getenv(\"OPENAI_API_KEY\")\n",
        "if not OPENAI_ENABLED:\n",
        "    print(\"[INFO] OpenAI not available or key missing – paraphrase step will be skipped\", file=sys.stderr)\n",
        "\n",
        "########################################################################\n",
        "# Augmentation helpers                                                 #\n",
        "########################################################################\n",
        "\n",
        "def _collect_mentions(entities: List[Dict]) -> List[Dict]:\n",
        "    \"\"\"Flatten entity dicts into a list with text, type and len, plus indices.\"\"\"\n",
        "    flat = []\n",
        "    for ent_idx, ent in enumerate(entities):\n",
        "        for m_idx, mention in enumerate(ent.get(\"mentions\", [])):\n",
        "            flat.append({\n",
        "                \"text\": mention,\n",
        "                \"type\": ent[\"type\"],\n",
        "                \"len\": len(mention),\n",
        "                \"ent_idx\": ent_idx,\n",
        "                \"m_idx\": m_idx,\n",
        "            })\n",
        "    return flat\n",
        "\n",
        "\n",
        "def swap_entities(doc: Dict) -> Dict | None:\n",
        "    \"\"\"Swap 30 % of entity mentions with same‑type, same‑length alternates in‑doc.\"\"\"\n",
        "    text = doc[\"doc\"]\n",
        "    ents = copy.deepcopy(doc[\"entities\"])  # deep‑copy because we mutate it\n",
        "    mentions = _collect_mentions(ents)\n",
        "    if len(mentions) < 2:\n",
        "        return None\n",
        "    random.shuffle(mentions)\n",
        "    num_to_swap = max(1, int(len(mentions) * MASK_PROBABILITY))\n",
        "    swap_map = {}\n",
        "    for m in mentions[:num_to_swap]:\n",
        "        cands = [c for c in mentions if c[\"type\"] == m[\"type\"] and c[\"len\"] == m[\"len\"] and c[\"text\"] != m[\"text\"]]\n",
        "        if not cands:\n",
        "            continue\n",
        "        repl = random.choice(cands)\n",
        "        swap_map[m[\"text\"]] = repl[\"text\"]\n",
        "        ents[m[\"ent_idx\"]][\"mentions\"][m[\"m_idx\"]] = repl[\"text\"]  # keep metadata aligned\n",
        "\n",
        "    if not swap_map:\n",
        "        return None\n",
        "\n",
        "    pattern = re.compile(r\"|\".join(map(re.escape, swap_map.keys())))\n",
        "    swapped_text = pattern.sub(lambda match: swap_map[match.group(0)], text)\n",
        "\n",
        "    out = copy.deepcopy(doc)\n",
        "    out[\"doc\"] = swapped_text\n",
        "    out[\"entities\"] = ents\n",
        "    out[\"aug_type\"] = \"swap\"\n",
        "    return out\n",
        "\n",
        "\n",
        "def mask_and_fill(doc: Dict) -> Dict | None:\n",
        "    text = doc[\"doc\"]\n",
        "    words = text.split() # Simple space-based tokenization for word list\n",
        "    if len(words) < 5:\n",
        "        return None\n",
        "\n",
        "    scope = min(len(words), FIRST_N_WORDS_FOR_MASK)\n",
        "\n",
        "    entity_spans = []\n",
        "    processed_mentions_in_doc = set()\n",
        "    for ent in doc[\"entities\"]:\n",
        "        for mention_text in ent.get(\"mentions\", []):\n",
        "            start_offset = 0\n",
        "            while True:\n",
        "                start = text.find(mention_text, start_offset)\n",
        "                if start == -1:\n",
        "                    break\n",
        "                entity_spans.append((start, start + len(mention_text)))\n",
        "                start_offset = start + 1\n",
        "\n",
        "\n",
        "    token_offsets_in_scope = []\n",
        "    current_offset = 0\n",
        "    for i, w in enumerate(words[:scope]):\n",
        "        start = text.find(w, current_offset)\n",
        "        if start == -1:\n",
        "            current_offset += len(w) + 1\n",
        "            continue\n",
        "        end = start + len(w)\n",
        "        token_offsets_in_scope.append({\"start\": start, \"end\": end, \"idx_in_words\": i})\n",
        "        current_offset = end\n",
        "\n",
        "    non_ent_indices_in_words = []\n",
        "    for token_info in token_offsets_in_scope:\n",
        "        s, e, original_idx = token_info[\"start\"], token_info[\"end\"], token_info[\"idx_in_words\"]\n",
        "        is_entity = False\n",
        "        for es, ee in entity_spans:\n",
        "            if (es <= s < ee) or (es < e <= ee) or (s <= es and e >= ee):\n",
        "                is_entity = True\n",
        "                break\n",
        "        if not is_entity:\n",
        "            non_ent_indices_in_words.append(original_idx)\n",
        "\n",
        "    if not non_ent_indices_in_words:\n",
        "        return None\n",
        "\n",
        "    idx_to_mask = random.choice(non_ent_indices_in_words)\n",
        "    original_token = words[idx_to_mask]\n",
        "\n",
        "    temp_words = list(words)\n",
        "    temp_words[idx_to_mask] = MASK_TOKEN\n",
        "\n",
        "    context_window_size = 250\n",
        "\n",
        "    start_window = max(0, idx_to_mask - context_window_size // 2)\n",
        "    end_window = min(len(temp_words), idx_to_mask + context_window_size // 2 + 1)\n",
        "\n",
        "    if idx_to_mask < context_window_size // 2:\n",
        "        end_window = min(len(temp_words), context_window_size)\n",
        "    elif idx_to_mask > len(temp_words) - (context_window_size // 2):\n",
        "        start_window = max(0, len(temp_words) - context_window_size)\n",
        "\n",
        "    context_words_for_bert = temp_words[start_window:end_window]\n",
        "    masked_context_for_bert = \" \".join(context_words_for_bert)\n",
        "\n",
        "    try:\n",
        "        preds = FILL_MASK(masked_context_for_bert)\n",
        "    except Exception as exc:\n",
        "        print(f\"[WARN] fill-mask failed on context for doc '{doc.get('title', 'Untitled')[:30]}...': {exc}\")\n",
        "        return None\n",
        "\n",
        "    replacement = next((p[\"token_str\"].strip() for p in preds if p[\"token_str\"].strip().lower() != original_token.lower()), None)\n",
        "\n",
        "    if not replacement:\n",
        "        return None\n",
        "\n",
        "    words[idx_to_mask] = replacement\n",
        "    filled_sentence = \" \".join(words)\n",
        "\n",
        "    out = copy.deepcopy(doc)\n",
        "    out[\"doc\"] = filled_sentence\n",
        "    out[\"aug_type\"] = \"mask\"\n",
        "    return out\n",
        "\n",
        "\n",
        "# ------------------------ paraphrase helpers -------------------------\n",
        "\n",
        "TAG_RE = re.compile(r\"\\[\\[/?E\\d+]]\")\n",
        "\n",
        "def _wrap_mentions(text: str, mentions: List[str]) -> str:\n",
        "    tagged = text\n",
        "    for i, m in enumerate(sorted(list(set(mentions)), key=len, reverse=True)):\n",
        "        tagged = re.sub(re.escape(m), f\"[[E{i}]]{m}[[/E{i}]]\", tagged)\n",
        "    return tagged\n",
        "\n",
        "\n",
        "def _unwrap_mentions(text: str) -> str:\n",
        "    return TAG_RE.sub(\"\", text)\n",
        "\n",
        "\n",
        "PROMPT_HEADER = (\n",
        "    \"You are a meticulous rewriting assistant. Paraphrase the given paragraph \"\n",
        "    \"in fluent English while preserving **all facts and meaning**.\\n\"\n",
        "    \"The paragraph contains entity placeholders wrapped in double‑bracket tags.\\n\"\n",
        "    \"**Do not alter, remove, reorder, or add anything inside tags like [[E0]]…[[/E0]].**\\n\"\n",
        "    \"Keep roughly the same length (no summarisation).\\n\"\n",
        ")\n",
        "\n",
        "PROMPT_EXAMPLE = (\n",
        "    \"### Example\\n\"\n",
        "    \"INPUT:  [[E0]]Barack Obama[[/E0]] was born in Hawaii and became the 44th president of the United States.\\n\"\n",
        "    \"OUTPUT: [[E0]]Barack Obama[[/E0]] was born in Hawaii before going on to serve as the 44th president of the United States.\\n\\n\"\n",
        ")\n",
        "\n",
        "\n",
        "def paraphrase_llm(doc: Dict) -> Dict | None:\n",
        "    if not OPENAI_ENABLED:\n",
        "        return None\n",
        "\n",
        "    text = doc[\"doc\"]\n",
        "    mentions = [m_text for ent in doc[\"entities\"] for m_text in ent.get(\"mentions\", [])]\n",
        "\n",
        "    tagged_full_text = _wrap_mentions(text, mentions)\n",
        "\n",
        "    try:\n",
        "        nlp_doc = NLP(tagged_full_text)\n",
        "    except Exception as e:\n",
        "        print(f\"[WARN] spaCy NLP processing failed for paraphrase_llm on doc '{doc.get('title', 'Untitled')[:30]}...': {e}\")\n",
        "        return None # Cannot proceed with chunking if NLP fails\n",
        "\n",
        "    sentences = []\n",
        "    try:\n",
        "        # This is where the E030 error occurred. Should be fixed by adding 'sentencizer'.\n",
        "        sentences = [s.text for s in nlp_doc.sents]\n",
        "        if not sentences: # If somehow no sentences are found (e.g. empty input after tagging)\n",
        "             print(f\"[WARN] No sentences found by spaCy for doc '{doc.get('title', 'Untitled')[:30]}...' in paraphrase_llm. Text: {tagged_full_text[:100]}\")\n",
        "             return None\n",
        "    except Exception as e: # Catch if .sents fails for any other reason\n",
        "        print(f\"[WARN] Error accessing sentences (doc.sents) for '{doc.get('title', 'Untitled')[:30]}...': {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "    paraphrased_text_parts = []\n",
        "    current_chunk_sentences = []\n",
        "    current_chunk_word_count = 0\n",
        "    MAX_WORDS_PER_CHUNK_FOR_LLM = 300\n",
        "\n",
        "    for i, sent in enumerate(sentences):\n",
        "        sent_word_count = len(sent.split())\n",
        "\n",
        "        # Determine if the current chunk should be processed\n",
        "        process_this_chunk = False\n",
        "        if current_chunk_sentences: # If there's something in the current chunk\n",
        "            if (current_chunk_word_count + sent_word_count > MAX_WORDS_PER_CHUNK_FOR_LLM) or (i == len(sentences) - 1):\n",
        "                process_this_chunk = True\n",
        "\n",
        "        # If it's the last sentence, it must be added to a chunk to be processed\n",
        "        if i == len(sentences) - 1 and not process_this_chunk:\n",
        "             current_chunk_sentences.append(sent)\n",
        "             current_chunk_word_count += sent_word_count\n",
        "             process_this_chunk = True # Mark for processing as it's the very end\n",
        "\n",
        "        if process_this_chunk:\n",
        "            chunk_to_paraphrase = \" \".join(current_chunk_sentences)\n",
        "            if not chunk_to_paraphrase.strip(): # Avoid processing empty chunks\n",
        "                current_chunk_sentences = [sent] if not (i == len(sentences) - 1 and (current_chunk_word_count + sent_word_count > MAX_WORDS_PER_CHUNK_FOR_LLM)) else []\n",
        "                current_chunk_word_count = len(sent.split()) if current_chunk_sentences else 0\n",
        "                continue\n",
        "\n",
        "            prompt_for_chunk = (\n",
        "                PROMPT_HEADER\n",
        "                + PROMPT_EXAMPLE\n",
        "                + \"### Paragraph to rewrite (this might be a segment of a larger document)\\\\n\"\n",
        "                + chunk_to_paraphrase\n",
        "            )\n",
        "\n",
        "            chunk_words_list = chunk_to_paraphrase.split()\n",
        "            calculated_max_tokens = max(32, int(len(chunk_words_list) * 1.5))\n",
        "            llm_max_tokens_for_chunk = min(calculated_max_tokens, 1500)\n",
        "\n",
        "            try:\n",
        "                resp = openai.ChatCompletion.create(\n",
        "                    model=\"gpt-3.5-turbo\",\n",
        "                    messages=[{\"role\": \"user\", \"content\": prompt_for_chunk}],\n",
        "                    temperature=0.7,\n",
        "                    max_tokens=llm_max_tokens_for_chunk,\n",
        "                )\n",
        "                paraphrased_chunk_content = resp.choices[0].message.content.strip()\n",
        "\n",
        "                if len(paraphrased_chunk_content) < len(chunk_to_paraphrase) * 0.3:\n",
        "                     print(f\"[WARN] Paraphrased chunk too short for: {chunk_to_paraphrase[:50]}... Using original.\")\n",
        "                     paraphrased_text_parts.append(chunk_to_paraphrase)\n",
        "                else:\n",
        "                     paraphrased_text_parts.append(paraphrased_chunk_content)\n",
        "\n",
        "            except Exception as exc_chunk:\n",
        "                print(f\"[WARN] OpenAI paraphrase failed for chunk: {chunk_to_paraphrase[:50]}... Error: {exc_chunk}. Using original.\")\n",
        "                paraphrased_text_parts.append(chunk_to_paraphrase)\n",
        "\n",
        "            # Reset for next chunk, or start new chunk with current sentence if it wasn't part of the processed one\n",
        "            if i < len(sentences) - 1: # If not the last sentence\n",
        "                 if (current_chunk_word_count + sent_word_count > MAX_WORDS_PER_CHUNK_FOR_LLM) : # current 'sent' starts a new chunk\n",
        "                    current_chunk_sentences = [sent]\n",
        "                    current_chunk_word_count = sent_word_count\n",
        "                 else: # current 'sent' was part of the processed chunk (because it was the last one that fit or the final sentence)\n",
        "                    current_chunk_sentences = [] # Handled by the append below if it was the last one\n",
        "                    current_chunk_word_count = 0\n",
        "            else: # Processed the last chunk\n",
        "                current_chunk_sentences = []\n",
        "                current_chunk_word_count = 0\n",
        "\n",
        "        # Add current sentence to the next chunk if it wasn't processed\n",
        "        if not process_this_chunk and i < len(sentences): # Check i < len(sentences) to be safe\n",
        "            current_chunk_sentences.append(sent)\n",
        "            current_chunk_word_count += sent_word_count\n",
        "\n",
        "\n",
        "    paraphrased_full_tagged_text = \" \".join(paraphrased_text_parts)\n",
        "\n",
        "    cleaned_paraphrased_text = _unwrap_mentions(paraphrased_full_tagged_text)\n",
        "\n",
        "    if not cleaned_paraphrased_text.strip() or len(cleaned_paraphrased_text) < len(text) * 0.3:\n",
        "        print(f\"[WARN] Final paraphrased text significantly shorter than original or empty for doc: {doc.get('title', 'Untitled')[:30]}...\")\n",
        "        return None\n",
        "\n",
        "    out = copy.deepcopy(doc)\n",
        "    out[\"doc\"] = cleaned_paraphrased_text\n",
        "    out[\"aug_type\"] = \"para\"\n",
        "    return out\n",
        "\n",
        "########################################################################\n",
        "# Main driver                                                          #\n",
        "########################################################################\n",
        "\n",
        "def augment_file(path: Path, out_dir: Path, overwrite: bool = False):\n",
        "    out_path = out_dir / path.name\n",
        "    if out_path.exists() and not overwrite:\n",
        "        print(f\"[SKIP] {out_path} exists\")\n",
        "        return\n",
        "\n",
        "    try:\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as fh:\n",
        "            data = json.load(fh)\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to read or parse JSON file {path}: {e}\")\n",
        "        return\n",
        "\n",
        "\n",
        "    augmented: List[Dict] = []\n",
        "    for row_idx, row in enumerate(tqdm(data, desc=path.name)):\n",
        "        if not isinstance(row, dict) or \"doc\" not in row or \"entities\" not in row:\n",
        "            print(f\"[WARN] Skipping invalid row (index {row_idx}) in {path.name}: Missing 'doc' or 'entities'. Row content: {str(row)[:100]}\")\n",
        "            continue\n",
        "\n",
        "        row_orig = copy.deepcopy(row)\n",
        "        row_orig[\"aug_type\"] = \"orig\"\n",
        "        augmented.append(row_orig)\n",
        "\n",
        "        doc_to_augment = copy.deepcopy(row)\n",
        "\n",
        "        for func in (swap_entities, mask_and_fill, paraphrase_llm):\n",
        "            try:\n",
        "                new_row = func(copy.deepcopy(doc_to_augment))\n",
        "                if new_row is not None:\n",
        "                    augmented.append(new_row)\n",
        "            except Exception as exc:\n",
        "                title_info = row.get('title', row.get('doc_id', f'doc_idx_{row_idx}'))\n",
        "                print(f\"[WARN] Augmentation function {func.__name__} failed on '{str(title_info)[:40]}…': {exc}\")\n",
        "\n",
        "\n",
        "    if not augmented:\n",
        "        print(f\"[INFO] No data was augmented or generated for {path.name} (possibly all input rows were invalid or empty).\")\n",
        "        return\n",
        "\n",
        "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
        "    try:\n",
        "        with open(out_path, \"w\", encoding=\"utf-8\") as fh:\n",
        "            json.dump(augmented, fh, ensure_ascii=False, indent=2)\n",
        "        print(f\"[WRITE] {out_path} (×{len(augmented)} docs)\")\n",
        "    except Exception as e:\n",
        "        print(f\"[ERROR] Failed to write augmented data to {out_path}: {e}\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    datasets = [\n",
        "        {\n",
        "            \"name\": \"train\",\n",
        "            \"input_dir\": Path(\"/kaggle/input/nlp-second-try/DocIE_dataset_final_version/train\"),\n",
        "            \"output_dir\": Path(\"/kaggle/working/aug_train\")\n",
        "        },\n",
        "        {\n",
        "            \"name\": \"dev\",\n",
        "            \"input_dir\": Path(\"/kaggle/input/nlp-second-try/DocIE_dataset_final_version/dev\"),\n",
        "            \"output_dir\": Path(\"/kaggle/working/aug_dev\")\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    for dataset in datasets:\n",
        "        print(f\"\\n{'='*50}\")\n",
        "        print(f\"Processing {dataset['name']} set...\")\n",
        "        print(f\"Input dir: {dataset['input_dir']}\")\n",
        "        print(f\"Output dir: {dataset['output_dir']}\")\n",
        "        print(f\"{'='*50}\\n\")\n",
        "\n",
        "        if not dataset['input_dir'].exists():\n",
        "            print(f\"[ERROR] Input directory not found: {dataset['input_dir']}\")\n",
        "            continue\n",
        "\n",
        "        json_files = list(dataset['input_dir'].glob(\"*_all_examples.json\"))\n",
        "        if not json_files:\n",
        "            print(f\"No input JSON files found (ending with _all_examples.json) for {dataset['name']} set in {dataset['input_dir']}.\")\n",
        "            continue\n",
        "\n",
        "        print(f\"Found {len(json_files)} JSON files to process for {dataset['name']} set.\")\n",
        "\n",
        "        for fp in json_files:\n",
        "            print(f\"\\nStarting augmentation for file: {fp.name}\")\n",
        "            augment_file(fp, dataset['output_dir'], overwrite=False)\n",
        "\n",
        "    print(\"\\n\\nData augmentation process finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2025-05-29T07:48:28.007092Z",
          "iopub.status.busy": "2025-05-29T07:48:28.005958Z",
          "iopub.status.idle": "2025-05-29T07:48:28.248286Z",
          "shell.execute_reply": "2025-05-29T07:48:28.247579Z",
          "shell.execute_reply.started": "2025-05-29T07:48:28.007062Z"
        },
        "id": "jfKjZNhfhqKF",
        "outputId": "f9165783-b026-4fbd-ba95-8f6c4f880cb6",
        "trusted": true
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  adding: kaggle/working/aug_dev/ (stored 0%)\n",
            "  adding: kaggle/working/aug_dev/Human_behavior_all_examples.json (deflated 90%)\n",
            "  adding: kaggle/working/aug_dev/Internet_all_examples.json (deflated 91%)\n",
            "  adding: kaggle/working/aug_train/ (stored 0%)\n",
            "  adding: kaggle/working/aug_train/Entertainment_all_examples.json (deflated 89%)\n",
            "  adding: kaggle/working/aug_train/Communication_all_examples.json (deflated 90%)\n",
            "  adding: kaggle/working/aug_train/Energy_all_examples.json (deflated 89%)\n",
            "  adding: kaggle/working/aug_train/Government_all_examples.json (deflated 89%)\n",
            "  adding: kaggle/working/aug_train/Education_all_examples.json (deflated 91%)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "!zip -r /kaggle/working/working_dir.zip /kaggle/working/*\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "datasetId": 7542374,
          "sourceId": 11991456,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 31041,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.11"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
