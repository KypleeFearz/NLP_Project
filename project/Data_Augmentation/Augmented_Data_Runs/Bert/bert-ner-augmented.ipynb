{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1b3b2c0b-a612-4843-9938-d6257d1551e4",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "97e862ba-0f63-4b9b-864e-7a59dbdf7dbf",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T08:55:11.357060Z",
     "iopub.status.busy": "2025-05-29T08:55:11.356799Z",
     "iopub.status.idle": "2025-05-29T08:55:11.361242Z",
     "shell.execute_reply": "2025-05-29T08:55:11.360638Z",
     "shell.execute_reply.started": "2025-05-29T08:55:11.357041Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca55bab6-edcf-4182-9175-0b60f6c8cfd3",
   "metadata": {},
   "source": [
    "## 2. DATA LOADING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "81bdc49b-5d25-4ca9-a49e-80aae9b6e89a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T08:55:50.363828Z",
     "iopub.status.busy": "2025-05-29T08:55:50.363543Z",
     "iopub.status.idle": "2025-05-29T08:55:51.253945Z",
     "shell.execute_reply": "2025-05-29T08:55:51.253235Z",
     "shell.execute_reply.started": "2025-05-29T08:55:50.363799Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes - Train: 204, Dev: 88, Test: 248\n"
     ]
    }
   ],
   "source": [
    "# Deine lokalen Windows-Pfade\n",
    "TRAIN_DIR = Path(r\"/kaggle/input/nlp-augmentedset/aug_train\")\n",
    "DEV_DIR   = Path(r\"/kaggle/input/nlp-augmentedset/aug_dev\")\n",
    "TEST_DIR  = Path(r\"/kaggle/input/nlp-augmentedset/test\")\n",
    "\n",
    "assert TRAIN_DIR.exists(), f\"Train-Ordner nicht gefunden: {TRAIN_DIR}\"\n",
    "assert DEV_DIR.exists(),   f\"Dev-Ordner nicht gefunden:   {DEV_DIR}\"\n",
    "assert TEST_DIR.exists(),  f\"Test-Ordner nicht gefunden:  {TEST_DIR}\"\n",
    "def load_docie_docs(folder: Path, recursive: bool = False):\n",
    "    \"\"\"Load DocIE documents from JSON files.\"\"\"\n",
    "    docs = []\n",
    "    pattern = \"**/*.json\" if recursive else \"*.json\"\n",
    "    for file in folder.glob(pattern):\n",
    "        data = json.loads(file.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(data, list):\n",
    "            docs.extend(data)\n",
    "        else:\n",
    "            docs.append(data)\n",
    "    return docs\n",
    "\n",
    "# Load data\n",
    "train_docs = load_docie_docs(TRAIN_DIR)\n",
    "dev_docs = load_docie_docs(DEV_DIR)\n",
    "test_docs = load_docie_docs(TEST_DIR, recursive=True)\n",
    "\n",
    "print(f\"Dataset sizes - Train: {len(train_docs)}, Dev: {len(dev_docs)}, Test: {len(test_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fb7ba20-3745-4c32-931d-5ae68654edfd",
   "metadata": {},
   "source": [
    "## 3. LABEL MAPPING SETUP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "96443f17-e814-4ad1-8a2d-6127988ce06d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T08:55:56.609763Z",
     "iopub.status.busy": "2025-05-29T08:55:56.609067Z",
     "iopub.status.idle": "2025-05-29T08:55:56.615523Z",
     "shell.execute_reply": "2025-05-29T08:55:56.614858Z",
     "shell.execute_reply.started": "2025-05-29T08:55:56.609738Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of NER labels: 39\n",
      "Entity types: ['CARDINAL', 'DATE', 'EVENT', 'FAC', 'GPE', 'LANGUAGE', 'LAW', 'LOC', 'MISC', 'MONEY', 'NORP', 'ORDINAL', 'ORG', 'PERCENT', 'PERSON', 'PRODUCT', 'QUANTITY', 'TIME', 'WORK_OF_ART']\n"
     ]
    }
   ],
   "source": [
    "# Extract entity types from training data\n",
    "entity_types = set()\n",
    "for doc in train_docs:\n",
    "    if \"entity_label_set\" in doc:\n",
    "        entity_types.update(doc[\"entity_label_set\"])\n",
    "    elif \"NER_label_set\" in doc:\n",
    "        entity_types.update(doc[\"NER_label_set\"])\n",
    "\n",
    "entity_types = sorted(list(entity_types))\n",
    "\n",
    "# Create BIO labels\n",
    "ner_labels = [\"O\"]\n",
    "for entity_type in entity_types:\n",
    "    ner_labels.extend([f\"B-{entity_type}\", f\"I-{entity_type}\"])\n",
    "\n",
    "# Create label mappings\n",
    "label2id = {label: idx for idx, label in enumerate(ner_labels)}\n",
    "id2label = {idx: label for label, idx in label2id.items()}\n",
    "\n",
    "print(f\"Number of NER labels: {len(ner_labels)}\")\n",
    "print(f\"Entity types: {entity_types}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d83eb915-fccb-4d89-abe6-4df65db5927e",
   "metadata": {},
   "source": [
    "## 4. TOKENIZATION AND DATASET PREPARATION"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5396329",
   "metadata": {},
   "source": [
    "## Summary of Updates to Tokenization Cell over the original pipeline \n",
    "\n",
    "\n",
    "\n",
    "**Key Changes and Why They Were Made:**\n",
    "\n",
    "1.  **Metadata Preservation (e.g., `aug_type`)**:\n",
    "    * **Change**: Added `DESIRED_PASSTHROUGH_COLUMNS` to specify metadata (like `aug_type`) to carry over. The `tokenize_and_align_labels` function now tiles these values for each tokenized chunk.\n",
    "\n",
    "2.  **Explicit Output Schema (`features`)**:\n",
    "    * **Change**: The `.map()` function now uses an explicitly defined `output_features` schema (using `datasets.Features`). This schema details all columns in the output, including passthrough metadata.\n",
    "\n",
    "3.  **Dynamic Column Removal**:\n",
    "    * **Change**: Columns to be removed from the original dataset are now determined dynamically based on what's *not* in the `output_features`, with `doc` and `entities` always being removed post-processing.\n",
    "\n",
    "4.  **Improved Special Token Labeling**:\n",
    "    * **Change**: A more explicit loop in `tokenize_and_align_labels` assigns the `-100` label to special tokens (like `[CLS]`, `[SEP]`) based on their `offset_mapping == (0,0)`.\n",
    "\n",
    "5.  **Controlled `.map()` Execution**:\n",
    "    * **Change**: `batch_size=100` and `load_from_cache_file=False` are now explicitly set in `.map()` calls.\n",
    "\n",
    "These updates lead to a more reliable and flexible tokenization pipeline, critical for complex NER tasks involving augmented data or detailed metadata tracking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db1a830d-7aae-447c-952b-7e13764048ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T08:56:00.025998Z",
     "iopub.status.busy": "2025-05-29T08:56:00.025301Z",
     "iopub.status.idle": "2025-05-29T08:56:06.247271Z",
     "shell.execute_reply": "2025-05-29T08:56:06.246371Z",
     "shell.execute_reply.started": "2025-05-29T08:56:00.025973Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db288fba57a341699064e9ca975e7604",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c98133128aed4af08cfac4585ac64208",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "92d5107d0c844ab0ab2bf12628058086",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63fcf6f0e5ba44fdb186c1956766234f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defined output features for .map(): {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), 'aug_type': Value(dtype='string', id=None)}\n",
      "Columns to be removed from hf_train after mapping: ['doc', 'domain', 'entities', 'entity_label_set', 'label_set', 'title', 'triples']\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc2d40570cd742559055db228c318862",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f1a4b182db24f43823fdc2c69339769",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized dataset sizes - Train: 630, Dev: 272\n"
     ]
    }
   ],
   "source": [
    "# Initialize tokenizer\n",
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# Tokenization parameters\n",
    "max_length = 512\n",
    "stride = 128\n",
    "\n",
    "# --- Define which original columns to keep and explicitly tile ---\n",
    "\n",
    "DESIRED_PASSTHROUGH_COLUMNS = [\"aug_type\"]\n",
    "\n",
    "# --- Define output_features for the .map() function ---\n",
    "# Start with features for generated columns\n",
    "feature_definitions_for_map_output = {\n",
    "    \"input_ids\": Sequence(Value(\"int32\")),\n",
    "    \"attention_mask\": Sequence(Value(\"int8\")),\n",
    "    \"labels\": Sequence(Value(\"int64\")),\n",
    "}\n",
    "# Add features for the desired passthrough columns\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"Tokenize text, align NER labels, and explicitly tile DESIRED passthrough columns.\"\"\"\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_labels = []\n",
    "    \n",
    "    # Initialize lists for the desired passthrough columns\n",
    "    tiled_desired_passthrough_data = {\n",
    "        col_name: [] for col_name in DESIRED_PASSTHROUGH_COLUMNS if col_name in examples\n",
    "    }\n",
    "\n",
    "    num_input_docs_in_batch = len(examples[\"doc\"])\n",
    "\n",
    "    for doc_idx_in_batch in range(num_input_docs_in_batch):\n",
    "        doc_text = examples[\"doc\"][doc_idx_in_batch]\n",
    "        entities_for_doc = examples[\"entities\"][doc_idx_in_batch]\n",
    "\n",
    "        # Get the values of desired passthrough columns for the current document\n",
    "        current_doc_passthrough_values = {}\n",
    "        for col_name in DESIRED_PASSTHROUGH_COLUMNS:\n",
    "            if col_name in examples:\n",
    "                current_doc_passthrough_values[col_name] = examples[col_name][doc_idx_in_batch]\n",
    "        \n",
    "        if not isinstance(doc_text, str):\n",
    "            # If a doc is invalid, skip it\n",
    "            continue\n",
    "\n",
    "        tokenized = tokenizer(\n",
    "            doc_text, # Use doc_text\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            return_overflowing_tokens=True,\n",
    "        )\n",
    "        \n",
    "        num_chunks_for_this_doc = len(tokenized[\"input_ids\"])\n",
    "\n",
    "        for i in range(num_chunks_for_this_doc):\n",
    "            input_ids = tokenized[\"input_ids\"][i]\n",
    "            attention_mask = tokenized[\"attention_mask\"][i]\n",
    "            offsets = tokenized[\"offset_mapping\"][i]\n",
    "            \n",
    "            chunk_labels = [\"O\"] * len(input_ids)\n",
    "            \n",
    "            for entity in entities_for_doc:\n",
    "                entity_type = entity[\"type\"]\n",
    "                for mention in entity.get(\"mentions\", []):\n",
    "                    start_char = doc_text.find(mention) # Use doc_text\n",
    "                    if start_char == -1:\n",
    "                        continue\n",
    "                    end_char = start_char + len(mention)\n",
    "                    \n",
    "                    for token_idx, (token_start, token_end) in enumerate(offsets):\n",
    "                        if token_idx < len(chunk_labels):\n",
    "                            if token_start >= start_char and token_end <= end_char:\n",
    "                                if token_start == start_char:\n",
    "                                    chunk_labels[token_idx] = f\"B-{entity_type}\"\n",
    "                                else:\n",
    "                                    chunk_labels[token_idx] = f\"I-{entity_type}\"\n",
    "            \n",
    "            label_ids = [label2id.get(label, label2id[\"O\"]) for label in chunk_labels]\n",
    "            \n",
    "            final_label_ids = []\n",
    "            for k_idx in range(len(input_ids)):\n",
    "                if k_idx < len(offsets) and offsets[k_idx] == (0, 0):\n",
    "                     final_label_ids.append(-100)\n",
    "                elif k_idx < len(label_ids):\n",
    "                     final_label_ids.append(label_ids[k_idx])\n",
    "                else:\n",
    "                     final_label_ids.append(label2id[\"O\"]) \n",
    "\n",
    "            all_input_ids.append(input_ids)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "            all_labels.append(final_label_ids)\n",
    "\n",
    "            # Append the explicitly desired passthrough values for each chunk\n",
    "            for col_name in DESIRED_PASSTHROUGH_COLUMNS:\n",
    "                if col_name in examples:\n",
    "                    tiled_desired_passthrough_data[col_name].append(current_doc_passthrough_values[col_name])\n",
    "    \n",
    "    output_dict = {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_mask,\n",
    "        \"labels\": all_labels,\n",
    "    }\n",
    "    for col_name, data_list in tiled_desired_passthrough_data.items():\n",
    "        output_dict[col_name] = data_list\n",
    "        \n",
    "    return output_dict\n",
    "\n",
    "# Create HuggingFace datasets\n",
    "hf_train = Dataset.from_list(train_docs)\n",
    "hf_dev = Dataset.from_list(dev_docs)\n",
    "\n",
    "\n",
    "# --- Refine output_features definition using hf_train.features ---\n",
    "if hf_train:\n",
    "    original_features = hf_train.features\n",
    "    for col_name in DESIRED_PASSTHROUGH_COLUMNS:\n",
    "        if col_name in original_features:\n",
    "            feature_definitions_for_map_output[col_name] = original_features[col_name]\n",
    "        elif col_name == \"aug_type\" and \"aug_type\" not in original_features :\n",
    "            feature_definitions_for_map_output[col_name] = Value(\"string\")\n",
    "\n",
    "\n",
    "output_features = Features(feature_definitions_for_map_output)\n",
    "print(f\"Defined output features for .map(): {output_features}\")\n",
    "# Define columns to remove from the ORIGINAL dataset..\n",
    "columns_to_remove_from_original_dataset = [\n",
    "    col for col in hf_train.column_names \n",
    "    if col not in output_features.keys()\n",
    "]\n",
    "\n",
    "if 'doc' not in columns_to_remove_from_original_dataset:\n",
    "    columns_to_remove_from_original_dataset.append('doc')\n",
    "if 'entities' not in columns_to_remove_from_original_dataset:\n",
    "    columns_to_remove_from_original_dataset.append('entities')\n",
    "\n",
    "# Ensure no duplicates if columns_to_remove already contained them.\n",
    "final_train_cols_to_remove = sorted(list(set(columns_to_remove_from_original_dataset)))\n",
    "final_dev_cols_to_remove = [col for col in final_train_cols_to_remove if col in hf_dev.column_names]\n",
    "\n",
    "print(f\"Columns to be removed from hf_train after mapping: {final_train_cols_to_remove}\")\n",
    "\n",
    "tokenized_train = hf_train.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=final_train_cols_to_remove, \n",
    "    load_from_cache_file=False,\n",
    "    features=output_features\n",
    ")\n",
    "\n",
    "tokenized_dev = hf_dev.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=final_dev_cols_to_remove,\n",
    "    load_from_cache_file=False,\n",
    "    features=output_features\n",
    ")\n",
    "\n",
    "print(f\"Tokenized dataset sizes - Train: {len(tokenized_train)}, Dev: {len(tokenized_dev)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ad21fc8-a4a1-4566-9257-46ed82a1f0aa",
   "metadata": {},
   "source": [
    "## 5. METRICS AND DATA COLLATOR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9f4fe3bf-6a30-4a04-bfa2-6ac969369207",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T08:56:10.515273Z",
     "iopub.status.busy": "2025-05-29T08:56:10.514989Z",
     "iopub.status.idle": "2025-05-29T08:56:10.520255Z",
     "shell.execute_reply": "2025-05-29T08:56:10.519603Z",
     "shell.execute_reply.started": "2025-05-29T08:56:10.515253Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data collator for padding\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "def compute_metrics(eval_pred):\n",
    "    \"\"\"Compute precision, recall, and F1 for NER.\"\"\"\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=-1)\n",
    "    \n",
    "    # Flatten and remove special tokens (-100)\n",
    "    predictions = predictions.flatten()\n",
    "    labels = labels.flatten()\n",
    "    mask = labels != -100\n",
    "    \n",
    "    predictions = predictions[mask]\n",
    "    labels = labels[mask]\n",
    "    \n",
    "    # Compute metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels, predictions, average='micro', zero_division=0\n",
    "    )\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    \n",
    "    return {\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "752ede33-d531-4515-b3ba-21aa11f1345c",
   "metadata": {},
   "source": [
    "## 6. BASELINE MODEL TRAINING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "41fc40e6-23df-4872-ac9e-48386eab3048",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T09:00:47.352751Z",
     "iopub.status.busy": "2025-05-29T09:00:47.352046Z",
     "iopub.status.idle": "2025-05-29T09:02:58.944090Z",
     "shell.execute_reply": "2025-05-29T09:02:58.943487Z",
     "shell.execute_reply.started": "2025-05-29T09:00:47.352728Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "TRAINING BASELINE MODEL\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/1897915768.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  baseline_trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 02:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.543038</td>\n",
       "      <td>0.891699</td>\n",
       "      <td>0.891699</td>\n",
       "      <td>0.891699</td>\n",
       "      <td>0.891699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.474552</td>\n",
       "      <td>0.892774</td>\n",
       "      <td>0.892774</td>\n",
       "      <td>0.892774</td>\n",
       "      <td>0.892774</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.857100</td>\n",
       "      <td>0.453883</td>\n",
       "      <td>0.895956</td>\n",
       "      <td>0.895956</td>\n",
       "      <td>0.895956</td>\n",
       "      <td>0.895956</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Baseline Results:\n",
      "F1: 0.8960\n",
      "Precision: 0.8960\n",
      "Recall: 0.8960\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"TRAINING BASELINE MODEL\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Initialize model\n",
    "baseline_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Training arguments\n",
    "# In cell 9\n",
    "baseline_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bert-ner-baseline\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=5e-5,\n",
    "    weight_decay=0.01,\n",
    "    eval_strategy=\"epoch\", # Changed from evaluation_strategy\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"logs\",\n",
    "    logging_steps=50,\n",
    "    save_total_limit=2,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"f1\",\n",
    "    greater_is_better=True,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "baseline_trainer = Trainer(\n",
    "    model=baseline_model,\n",
    "    args=baseline_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train\n",
    "baseline_trainer.train()\n",
    "\n",
    "# Evaluate\n",
    "baseline_results = baseline_trainer.evaluate()\n",
    "print(f\"\\nBaseline Results:\")\n",
    "print(f\"F1: {baseline_results['eval_f1']:.4f}\")\n",
    "print(f\"Precision: {baseline_results['eval_precision']:.4f}\")\n",
    "print(f\"Recall: {baseline_results['eval_recall']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e95daf6b-c17e-4674-8e91-d2fe16e3abd5",
   "metadata": {},
   "source": [
    "## 7. FINE-TUNING WITH BEST HYPERPARAMETERS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2046f559-c3f2-409b-bd96-93c12a734edb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T09:04:58.937427Z",
     "iopub.status.busy": "2025-05-29T09:04:58.937127Z",
     "iopub.status.idle": "2025-05-29T09:04:58.942355Z",
     "shell.execute_reply": "2025-05-29T09:04:58.941591Z",
     "shell.execute_reply.started": "2025-05-29T09:04:58.937409Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Based on your hyperparameter tuning results:\n",
    "best_hyperparams = {\n",
    "    \"full_ft\": {\n",
    "        \"learning_rate\": 4.358610256985791e-05,\n",
    "        \"batch_size\": 16,\n",
    "        \"method_name\": \"Full Fine-Tuning\"\n",
    "    },\n",
    "    \"lora\": {\n",
    "        \"learning_rate\": 2.220149951658828e-05,\n",
    "        \"r\": 16,\n",
    "        \"alpha\": 16,\n",
    "        \"dropout\": 0.015844640852335577,\n",
    "        \"batch_size\": 8,\n",
    "        \"method_name\": \"LoRA\"\n",
    "    },\n",
    "    \"partial_freeze\": {\n",
    "        \"freeze_pct\": 0.5,\n",
    "        \"learning_rate\": 3.2302001133689886e-05,\n",
    "        \"batch_size\": 16,\n",
    "        \"method_name\": \"Partial Freeze\"\n",
    "    }\n",
    "}\n",
    "\n",
    "results_summary = {\n",
    "    \"baseline\": baseline_results\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4923174-fe12-4d94-9e3e-602ac0601dff",
   "metadata": {},
   "source": [
    "## 7.1 FULL FINE-TUNING\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f5ef2c0-334d-448a-b7e8-dda5f51f5fd4",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T09:06:34.078166Z",
     "iopub.status.busy": "2025-05-29T09:06:34.077362Z",
     "iopub.status.idle": "2025-05-29T09:10:21.022995Z",
     "shell.execute_reply": "2025-05-29T09:10:21.022367Z",
     "shell.execute_reply.started": "2025-05-29T09:06:34.078140Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FULL FINE-TUNING WITH BEST PARAMETERS\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/1824228818.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  ft_trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:38, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.336200</td>\n",
       "      <td>0.593735</td>\n",
       "      <td>0.891699</td>\n",
       "      <td>0.891699</td>\n",
       "      <td>0.891699</td>\n",
       "      <td>0.891699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.713900</td>\n",
       "      <td>0.474320</td>\n",
       "      <td>0.891838</td>\n",
       "      <td>0.891838</td>\n",
       "      <td>0.891838</td>\n",
       "      <td>0.891838</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.590200</td>\n",
       "      <td>0.425129</td>\n",
       "      <td>0.898983</td>\n",
       "      <td>0.898983</td>\n",
       "      <td>0.898983</td>\n",
       "      <td>0.898983</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.513000</td>\n",
       "      <td>0.406340</td>\n",
       "      <td>0.899716</td>\n",
       "      <td>0.899716</td>\n",
       "      <td>0.899716</td>\n",
       "      <td>0.899716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.468000</td>\n",
       "      <td>0.394827</td>\n",
       "      <td>0.899154</td>\n",
       "      <td>0.899154</td>\n",
       "      <td>0.899154</td>\n",
       "      <td>0.899154</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Full Fine-Tuning Results:\n",
      "F1: 0.8992\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FULL FINE-TUNING WITH BEST PARAMETERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create fresh model\n",
    "ft_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Training arguments with best hyperparameters\n",
    "ft_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bert-ner-full-ft-final\",\n",
    "    max_steps=100,  # Fixed budget for fair comparison\n",
    "    per_device_train_batch_size=best_hyperparams[\"full_ft\"][\"batch_size\"],\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=best_hyperparams[\"full_ft\"][\"learning_rate\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=20,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Train\n",
    "ft_trainer = Trainer(\n",
    "    model=ft_model,\n",
    "    args=ft_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "ft_trainer.train()\n",
    "ft_results = ft_trainer.evaluate()\n",
    "results_summary[\"full_ft\"] = ft_results\n",
    "\n",
    "print(f\"\\nFull Fine-Tuning Results:\")\n",
    "print(f\"F1: {ft_results['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1af03c2c-992f-452c-b3c0-530de911c5a1",
   "metadata": {},
   "source": [
    "## 7.2 LoRA FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3b78fd6-dc6c-41ff-b2a4-64278bc19674",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T09:10:21.024391Z",
     "iopub.status.busy": "2025-05-29T09:10:21.024113Z",
     "iopub.status.idle": "2025-05-29T09:12:16.443474Z",
     "shell.execute_reply": "2025-05-29T09:12:16.442862Z",
     "shell.execute_reply.started": "2025-05-29T09:10:21.024363Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "LoRA FINE-TUNING WITH BEST PARAMETERS\n",
      "==================================================\n",
      "trainable params: 619,815 || all params: 109,541,454 || trainable%: 0.5658\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/3089146369.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  lora_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:47, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.691900</td>\n",
       "      <td>3.598104</td>\n",
       "      <td>0.018711</td>\n",
       "      <td>0.018711</td>\n",
       "      <td>0.018711</td>\n",
       "      <td>0.018711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.578600</td>\n",
       "      <td>3.487993</td>\n",
       "      <td>0.053910</td>\n",
       "      <td>0.053910</td>\n",
       "      <td>0.053910</td>\n",
       "      <td>0.053910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.485700</td>\n",
       "      <td>3.400384</td>\n",
       "      <td>0.108219</td>\n",
       "      <td>0.108219</td>\n",
       "      <td>0.108219</td>\n",
       "      <td>0.108219</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.426200</td>\n",
       "      <td>3.343504</td>\n",
       "      <td>0.157726</td>\n",
       "      <td>0.157726</td>\n",
       "      <td>0.157726</td>\n",
       "      <td>0.157726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.388200</td>\n",
       "      <td>3.323048</td>\n",
       "      <td>0.178016</td>\n",
       "      <td>0.178016</td>\n",
       "      <td>0.178016</td>\n",
       "      <td>0.178016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "LoRA Results:\n",
      "F1: 0.1780\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"LoRA FINE-TUNING WITH BEST PARAMETERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "from peft import LoraConfig, get_peft_model, TaskType\n",
    "\n",
    "# Create fresh model\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# LoRA configuration\n",
    "lora_config = LoraConfig(\n",
    "    task_type=TaskType.TOKEN_CLS,\n",
    "    inference_mode=False,\n",
    "    r=best_hyperparams[\"lora\"][\"r\"],\n",
    "    lora_alpha=best_hyperparams[\"lora\"][\"alpha\"],\n",
    "    lora_dropout=best_hyperparams[\"lora\"][\"dropout\"],\n",
    "    target_modules=[\"query\", \"value\"],  # BERT-specific\n",
    ")\n",
    "\n",
    "# Apply LoRA\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "lora_model.print_trainable_parameters()\n",
    "\n",
    "# Training arguments\n",
    "lora_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bert-ner-lora-final\",\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=best_hyperparams[\"lora\"][\"batch_size\"],\n",
    "    per_device_eval_batch_size=16,\n",
    "    learning_rate=best_hyperparams[\"lora\"][\"learning_rate\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=20,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Train\n",
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=lora_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "lora_trainer.train()\n",
    "lora_results = lora_trainer.evaluate()\n",
    "results_summary[\"lora\"] = lora_results\n",
    "\n",
    "print(f\"\\nLoRA Results:\")\n",
    "print(f\"F1: {lora_results['eval_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699308d0-094b-41a8-b33a-7a1f43129b2a",
   "metadata": {},
   "source": [
    "## 7.3 PARTIAL FREEZE FINE-TUNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "827e90ed-fcb3-469e-bcfb-916b10583c02",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T09:12:16.444404Z",
     "iopub.status.busy": "2025-05-29T09:12:16.444171Z",
     "iopub.status.idle": "2025-05-29T09:15:17.399862Z",
     "shell.execute_reply": "2025-05-29T09:15:17.399225Z",
     "shell.execute_reply.started": "2025-05-29T09:12:16.444388Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PARTIAL FREEZE FINE-TUNING WITH BEST PARAMETERS\n",
      "==================================================\n",
      "Trainable parameters: 23,867,175 / 108,921,639 (21.91%)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_35/1425522905.py:45: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  freeze_trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.819600</td>\n",
       "      <td>3.732594</td>\n",
       "      <td>0.010442</td>\n",
       "      <td>0.010442</td>\n",
       "      <td>0.010442</td>\n",
       "      <td>0.010442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.655500</td>\n",
       "      <td>3.583329</td>\n",
       "      <td>0.039692</td>\n",
       "      <td>0.039692</td>\n",
       "      <td>0.039692</td>\n",
       "      <td>0.039692</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.532800</td>\n",
       "      <td>3.474048</td>\n",
       "      <td>0.095092</td>\n",
       "      <td>0.095092</td>\n",
       "      <td>0.095092</td>\n",
       "      <td>0.095092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.445900</td>\n",
       "      <td>3.406168</td>\n",
       "      <td>0.148009</td>\n",
       "      <td>0.148009</td>\n",
       "      <td>0.148009</td>\n",
       "      <td>0.148009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.400900</td>\n",
       "      <td>3.382259</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.170122</td>\n",
       "      <td>0.170122</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Partial Freeze Results:\n",
      "F1: 0.1701\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"PARTIAL FREEZE FINE-TUNING WITH BEST PARAMETERS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create fresh model\n",
    "freeze_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "\n",
    "# Freeze bottom 50% of layers\n",
    "freeze_pct = best_hyperparams[\"partial_freeze\"][\"freeze_pct\"]\n",
    "num_layers = len([n for n, _ in freeze_model.named_parameters() if \"encoder.layer\" in n]) // 2\n",
    "freeze_until = int(num_layers * freeze_pct)\n",
    "\n",
    "for name, param in freeze_model.named_parameters():\n",
    "    if \"encoder.layer\" in name:\n",
    "        layer_num = int(name.split(\".\")[3])\n",
    "        if layer_num < freeze_until:\n",
    "            param.requires_grad = False\n",
    "\n",
    "# Count trainable parameters\n",
    "trainable_params = sum(p.numel() for p in freeze_model.parameters() if p.requires_grad)\n",
    "total_params = sum(p.numel() for p in freeze_model.parameters())\n",
    "print(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)\")\n",
    "\n",
    "# Training arguments\n",
    "freeze_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bert-ner-freeze-final\",\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=best_hyperparams[\"partial_freeze\"][\"batch_size\"],\n",
    "    per_device_eval_batch_size=32,\n",
    "    learning_rate=best_hyperparams[\"partial_freeze\"][\"learning_rate\"],\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    logging_steps=20,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "# Train\n",
    "freeze_trainer = Trainer(\n",
    "    model=freeze_model,\n",
    "    args=freeze_args,\n",
    "    train_dataset=tokenized_train,\n",
    "    eval_dataset=tokenized_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "freeze_trainer.train()\n",
    "freeze_results = freeze_trainer.evaluate()\n",
    "results_summary[\"partial_freeze\"] = freeze_results\n",
    "\n",
    "print(f\"\\nPartial Freeze Results:\")\n",
    "print(f\"F1: {freeze_results['eval_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926a17fd-16f9-404e-8ccf-e07a414d7be0",
   "metadata": {},
   "source": [
    "## 8. RESULTS COMPARISON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "3f39c796-5fc8-4355-980a-604e60841ae9",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T09:15:17.401818Z",
     "iopub.status.busy": "2025-05-29T09:15:17.401609Z",
     "iopub.status.idle": "2025-05-29T09:15:17.418955Z",
     "shell.execute_reply": "2025-05-29T09:15:17.418218Z",
     "shell.execute_reply.started": "2025-05-29T09:15:17.401800Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "RESULTS COMPARISON\n",
      "==================================================\n",
      "        Method    F1  Precision  Recall  Accuracy\n",
      "      Baseline 89.60      89.60   89.60     89.60\n",
      "       Full Ft 89.92      89.92   89.92     89.92\n",
      "          Lora 17.80      17.80   17.80     17.80\n",
      "Partial Freeze 17.01      17.01   17.01     17.01\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"RESULTS COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Create comparison table\n",
    "import pandas as pd\n",
    "\n",
    "comparison_data = []\n",
    "for method, results in results_summary.items():\n",
    "    comparison_data.append({\n",
    "        \"Method\": method.replace(\"_\", \" \").title(),\n",
    "        \"F1\": results.get(\"eval_f1\", 0) * 100,\n",
    "        \"Precision\": results.get(\"eval_precision\", 0) * 100,\n",
    "        \"Recall\": results.get(\"eval_recall\", 0) * 100,\n",
    "        \"Accuracy\": results.get(\"eval_accuracy\", 0) * 100,\n",
    "    })\n",
    "\n",
    "comparison_df = pd.DataFrame(comparison_data)\n",
    "comparison_df = comparison_df.round(2)\n",
    "print(comparison_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36460f2-1d1c-4ad3-9cd0-430eb62f6a3c",
   "metadata": {},
   "source": [
    "## 9. VISUALIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bfa9117c-5417-439d-b59c-5e63802f6411",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T09:15:17.419960Z",
     "iopub.status.busy": "2025-05-29T09:15:17.419761Z",
     "iopub.status.idle": "2025-05-29T09:15:17.963928Z",
     "shell.execute_reply": "2025-05-29T09:15:17.963221Z",
     "shell.execute_reply.started": "2025-05-29T09:15:17.419944Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Bar plot for F1 scores\n",
    "plt.figure(figsize=(10, 6))\n",
    "methods = comparison_df[\"Method\"].tolist()\n",
    "f1_scores = comparison_df[\"F1\"].tolist()\n",
    "\n",
    "bars = plt.bar(methods, f1_scores, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728'])\n",
    "plt.ylabel('F1 Score (%)', fontsize=12)\n",
    "plt.title('BERT NER Performance Comparison', fontsize=14)\n",
    "plt.ylim(0, 100)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar, score in zip(bars, f1_scores):\n",
    "    plt.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1, \n",
    "             f'{score:.1f}%', ha='center', va='bottom')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outputs/bert_ner_comparison.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c527174d-89e7-463c-b972-a506bcb37aa3",
   "metadata": {},
   "source": [
    "## 10. TEST SET EVALUATION (FINAL MODEL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "539a8921-be3a-471e-b61f-5cd6f170e03e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T09:15:17.964825Z",
     "iopub.status.busy": "2025-05-29T09:15:17.964629Z",
     "iopub.status.idle": "2025-05-29T09:15:33.934821Z",
     "shell.execute_reply": "2025-05-29T09:15:33.934201Z",
     "shell.execute_reply.started": "2025-05-29T09:15:17.964809Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "FINAL TEST SET EVALUATION\n",
      "==================================================\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ad68afd3cb141fbb8cc5590965da0e6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/248 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated predictions for 731 test examples\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"FINAL TEST SET EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Use the best performing model (full fine-tuning based on your results)\n",
    "# Note: For test set, we need to handle the different structure\n",
    "test_examples = []\n",
    "for doc in test_docs:\n",
    "    # Test docs have different structure\n",
    "    text = doc.get(\"document\", doc.get(\"doc\", \"\"))\n",
    "    if text:\n",
    "        test_examples.append({\n",
    "            \"doc\": text,\n",
    "            \"entities\": [],  # No labels in test set\n",
    "            \"id\": doc.get(\"id\", \"\")\n",
    "        })\n",
    "\n",
    "hf_test_for_inference = Dataset.from_list(test_examples)\n",
    "\n",
    "# Tokenize test set\n",
    "tokenized_test = hf_test_for_inference.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=hf_test_for_inference.column_names\n",
    ")\n",
    "\n",
    "# Get predictions from best model\n",
    "test_predictions = ft_trainer.predict(tokenized_test)\n",
    "predictions = np.argmax(test_predictions.predictions, axis=-1)\n",
    "\n",
    "# Convert predictions back to labels\n",
    "predicted_labels = []\n",
    "for pred_seq in predictions:\n",
    "    labels = [id2label[p] for p in pred_seq if p != -100]\n",
    "    predicted_labels.append(labels)\n",
    "\n",
    "print(f\"Generated predictions for {len(predicted_labels)} test examples\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61dc1ef4-c26f-4de8-bf54-a38bdc3879ea",
   "metadata": {},
   "source": [
    "## 11. SAVE RESULTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "e9a3bb3e-23fe-487f-9b48-b4e4f700bd9d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T09:15:33.935800Z",
     "iopub.status.busy": "2025-05-29T09:15:33.935564Z",
     "iopub.status.idle": "2025-05-29T09:15:34.680994Z",
     "shell.execute_reply": "2025-05-29T09:15:34.680219Z",
     "shell.execute_reply.started": "2025-05-29T09:15:33.935774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "✅ All results saved to outputs/\n",
      "📊 Visualization saved as bert_ner_comparison.png\n",
      "💾 Best model saved to outputs/bert-ner-best-model/\n"
     ]
    }
   ],
   "source": [
    "# Save comparison table\n",
    "comparison_df.to_csv(\"outputs/bert_ner_results.csv\", index=False)\n",
    "\n",
    "# Save best model\n",
    "ft_trainer.save_model(\"outputs/bert-ner-best-model\")\n",
    "tokenizer.save_pretrained(\"outputs/bert-ner-best-model\")\n",
    "\n",
    "# Save predictions\n",
    "import json\n",
    "with open(\"outputs/bert_ner_test_predictions.json\", \"w\") as f:\n",
    "    json.dump({\n",
    "        \"predictions\": predicted_labels[:10],  # Sample\n",
    "        \"model\": \"bert-base-uncased\",\n",
    "        \"task\": \"NER\",\n",
    "        \"best_f1\": float(ft_results['eval_f1'])\n",
    "    }, f, indent=2)\n",
    "\n",
    "print(\"\\n✅ All results saved to outputs/\")\n",
    "print(\"📊 Visualization saved as bert_ner_comparison.png\")\n",
    "print(\"💾 Best model saved to outputs/bert-ner-best-model/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd851544-22a3-48e2-be25-90abcac0daec",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7543282,
     "sourceId": 11992746,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
