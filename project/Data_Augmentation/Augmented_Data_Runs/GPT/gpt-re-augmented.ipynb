{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11992746,"sourceType":"datasetVersion","datasetId":7543282}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"63d462da-e50a-4b1c-a860-9d3b54cd14ba","cell_type":"markdown","source":"## 1. Imports","metadata":{}},{"id":"d8411ec0-5062-4e20-b705-63edc0163500","cell_type":"code","source":"import os\nimport json\nimport torch\nimport numpy as np\nfrom pathlib import Path\nfrom datasets import Dataset, concatenate_datasets, DatasetDict, Features, Value, Sequence\nfrom collections import Counter\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score\nimport matplotlib.pyplot as plt\nimport pandas as pd\n\nimport optuna\nfrom transformers import (\n    AutoTokenizer,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorWithPadding,\n)\nfrom peft import LoraConfig, get_peft_model, TaskType","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:38:43.345949Z","iopub.execute_input":"2025-05-30T11:38:43.346363Z","iopub.status.idle":"2025-05-30T11:39:23.909458Z","shell.execute_reply.started":"2025-05-30T11:38:43.346314Z","shell.execute_reply":"2025-05-30T11:39:23.908906Z"}},"outputs":[{"name":"stderr","text":"2025-05-30 11:39:06.141277: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1748605146.550960      35 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1748605146.664719      35 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"}],"execution_count":5},{"id":"6481ed12-27f5-4991-8020-102e44898f77","cell_type":"markdown","source":"## 2. Data Paths","metadata":{}},{"id":"0d34dc27-9c74-4b1e-a814-5b2480b87c46","cell_type":"code","source":"TRAIN_DIR = Path(r\"/kaggle/input/nlp-augmentedset/aug_train\")\nDEV_DIR   = Path(r\"/kaggle/input/nlp-augmentedset/aug_dev\")\nTEST_DIR  = Path(r\"/kaggle/input/nlp-augmentedset/test\")\n\nassert TRAIN_DIR.exists(), f\"Train directory not found: {TRAIN_DIR}\"\nassert DEV_DIR.exists(), f\"Dev directory not found: {DEV_DIR}\"\nassert TEST_DIR.exists(), f\"Test directory not found: {TEST_DIR}\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:39:31.757839Z","iopub.execute_input":"2025-05-30T11:39:31.759072Z","iopub.status.idle":"2025-05-30T11:39:31.770048Z","shell.execute_reply.started":"2025-05-30T11:39:31.759044Z","shell.execute_reply":"2025-05-30T11:39:31.769529Z"}},"outputs":[],"execution_count":6},{"id":"c48e7160-1485-4ce7-bb27-a3c5b08d5465","cell_type":"markdown","source":"## 3. Load DocIE Data","metadata":{}},{"id":"cf837b15-f6ac-4b85-9d96-c3b007354e33","cell_type":"code","source":"def load_docie_docs(folder: Path, recursive: bool = False):\n    \"\"\"Load DocIE documents from JSON files.\"\"\"\n    docs = []\n    pattern = \"**/*.json\" if recursive else \"*.json\"\n    for file in folder.glob(pattern):\n        data = json.loads(file.read_text(encoding=\"utf-8\"))\n        if isinstance(data, list):\n            docs.extend(data)\n        else:\n            docs.append(data)\n    return docs\n\ntrain_docs = load_docie_docs(TRAIN_DIR)\ndev_docs = load_docie_docs(DEV_DIR)\ntest_docs = load_docie_docs(TEST_DIR, recursive=True)\n\nprint(f\"Loaded documents - Train: {len(train_docs)}, Dev: {len(dev_docs)}, Test: {len(test_docs)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:39:37.145988Z","iopub.execute_input":"2025-05-30T11:39:37.146255Z","iopub.status.idle":"2025-05-30T11:39:39.304644Z","shell.execute_reply.started":"2025-05-30T11:39:37.146235Z","shell.execute_reply":"2025-05-30T11:39:39.303828Z"}},"outputs":[{"name":"stdout","text":"Loaded documents - Train: 204, Dev: 88, Test: 248\n","output_type":"stream"}],"execution_count":7},{"id":"a28e74b1-649a-423c-9744-d4b7291b98fb","cell_type":"markdown","source":"## 4. Prepare Relation Extraction Data","metadata":{}},{"id":"0193d6dc-2b93-4479-9374-6b1ae365f191","cell_type":"code","source":"def create_re_examples(docs, split_name):\n    \"\"\"Create relation extraction examples from DocIE documents.\"\"\"\n    examples = []\n    \n    for doc in docs:\n        if not doc.get(\"triples\") or not doc.get(\"entities\"):\n            continue\n            \n        text = doc.get(\"doc\") or doc.get(\"document\")\n        if not text:\n            continue\n            \n        entity_mentions = [ent[\"mentions\"][0] for ent in doc[\"entities\"] if ent.get(\"mentions\")]\n        \n        # Create positive examples\n        for triple in doc[\"triples\"]:\n            examples.append({\n                \"text\": f\"{triple['head']} [SEP] {triple['tail']} [SEP] {text}\",\n                \"label\": triple[\"relation\"],\n                \"split\": split_name\n            })\n            \n        # Create negative examples\n        positive_pairs = {(triple[\"head\"], triple[\"tail\"]) for triple in doc[\"triples\"]}\n        negative_count = 0\n        max_attempts = len(doc[\"triples\"]) * 10\n        attempts = 0\n        \n        while negative_count < len(doc[\"triples\"]) and attempts < max_attempts:\n            attempts += 1\n            if len(entity_mentions) < 2:\n                break\n                \n            import random\n            head, tail = random.sample(entity_mentions, 2)\n            if (head, tail) not in positive_pairs and (tail, head) not in positive_pairs:\n                examples.append({\n                    \"text\": f\"{head} [SEP] {tail} [SEP] {text}\",\n                    \"label\": \"no_relation\",\n                    \"split\": split_name\n                })\n                negative_count += 1\n    \n    return examples\n\n# Create examples\ntrain_examples = create_re_examples(train_docs, \"train\")\ndev_examples = create_re_examples(dev_docs, \"dev\")\ntest_examples = create_re_examples(test_docs, \"test\")\n\nprint(f\"RE Examples - Train: {len(train_examples)}, Dev: {len(dev_examples)}, Test: {len(test_examples)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:39:43.661784Z","iopub.execute_input":"2025-05-30T11:39:43.662057Z","iopub.status.idle":"2025-05-30T11:39:43.749811Z","shell.execute_reply.started":"2025-05-30T11:39:43.662036Z","shell.execute_reply":"2025-05-30T11:39:43.749216Z"}},"outputs":[{"name":"stdout","text":"RE Examples - Train: 4888, Dev: 2398, Test: 0\n","output_type":"stream"}],"execution_count":8},{"id":"1e0e55db-e593-454a-a544-54f703f987c6","cell_type":"markdown","source":"## 5. Label Mapping","metadata":{}},{"id":"b89cc44b-a39d-454c-b44f-f59433cb04e7","cell_type":"code","source":"# Get all unique labels\nall_labels = sorted(set(ex[\"label\"] for ex in train_examples + dev_examples))\nlabel2id = {label: i for i, label in enumerate(all_labels)}\nid2label = {i: label for label, i in label2id.items()}\n\nprint(f\"Number of relation types: {len(all_labels)}\")\n\n# Analyze label distribution\nlabel_counts = Counter(ex[\"label\"] for ex in train_examples)\nprint(\"\\nLabel distribution in training data:\")\nfor label, count in label_counts.most_common(10):\n    print(f\"  {label}: {count}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:39:46.247861Z","iopub.execute_input":"2025-05-30T11:39:46.248127Z","iopub.status.idle":"2025-05-30T11:39:46.255057Z","shell.execute_reply.started":"2025-05-30T11:39:46.248105Z","shell.execute_reply":"2025-05-30T11:39:46.254502Z"}},"outputs":[{"name":"stdout","text":"Number of relation types: 76\n\nLabel distribution in training data:\n  no_relation: 2444\n  HasPart: 328\n  HasEffect: 268\n  DiplomaticRelation: 180\n  LocatedIn: 176\n  InterestedIn: 152\n  OwnerOf: 128\n  NominatedFor: 100\n  SaidToBeTheSameAs: 100\n  PartOf: 72\n","output_type":"stream"}],"execution_count":9},{"id":"6ee02d17-23a0-4ed7-b8ad-22835a5bee5a","cell_type":"markdown","source":"## 6. Tokenization","metadata":{}},{"id":"64cb5ef6-1ebc-4f74-a1de-01ca0fbffc40","cell_type":"code","source":"model_name = \"EleutherAI/gpt-neo-125M\"\ntokenizer = AutoTokenizer.from_pretrained(model_name)\n\n# Add padding token for GPT-Neo\nif tokenizer.pad_token is None:\n    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n\nmax_length = 512\n\ndef tokenize_function(examples):\n    return tokenizer(\n        examples[\"text\"],\n        padding=\"max_length\",\n        truncation=True,\n        max_length=max_length\n    )\n\n# Create datasets\ntrain_dataset = Dataset.from_list(train_examples)\ndev_dataset = Dataset.from_list(dev_examples)\n\n# Tokenize\ntrain_dataset = train_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\", \"split\"]\n)\n\ndev_dataset = dev_dataset.map(\n    tokenize_function,\n    batched=True,\n    remove_columns=[\"text\", \"split\"]\n)\n\n# Add labels\ntrain_dataset = train_dataset.map(lambda x: {\"labels\": label2id[x[\"label\"]]})\ndev_dataset = dev_dataset.map(lambda x: {\"labels\": label2id[x[\"label\"]]})\n\n# Remove original label column\ntrain_dataset = train_dataset.remove_columns([\"label\"])\ndev_dataset = dev_dataset.remove_columns([\"label\"])\n\n# Set format\ntrain_dataset.set_format(\"torch\")\ndev_dataset.set_format(\"torch\")\n\nprint(f\"Tokenized datasets - Train: {len(train_dataset)}, Dev: {len(dev_dataset)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:39:48.112204Z","iopub.execute_input":"2025-05-30T11:39:48.112494Z","iopub.status.idle":"2025-05-30T11:40:04.413704Z","shell.execute_reply.started":"2025-05-30T11:39:48.112473Z","shell.execute_reply":"2025-05-30T11:40:04.412991Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"48bbe25c5e48455d97c8124497e87b47"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"abf6818317444f448ac855940f75ba23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12b3223a35804aeb81a491c9757abee6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"03c3e3996b8c4a9ca59cb59715246236"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"39cecbc966c94815ac62db8cb59aa05c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4888 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"52153ec2589949c4997e0062cf386f9a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2398 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cf994ea6bc424e92bf8d0929e19e17de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/4888 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ec3fad8d6dec495fa199c7192a58e804"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/2398 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c2875d424674cbdb73dc787e4d4fe0d"}},"metadata":{}},{"name":"stdout","text":"Tokenized datasets - Train: 4888, Dev: 2398\n","output_type":"stream"}],"execution_count":10},{"id":"eaeaa2a1-a3db-4f46-bf42-674130f38c01","cell_type":"markdown","source":"## 7. Evaluation Metrics","metadata":{}},{"id":"18cb1f4c-956b-4393-a2ba-91b0aa5996f6","cell_type":"code","source":"def compute_metrics(eval_pred):\n    predictions, labels = eval_pred\n    predictions = np.argmax(predictions, axis=1)\n    \n    precision, recall, f1, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted', zero_division=0\n    )\n    accuracy = accuracy_score(labels, predictions)\n    \n    return {\n        \"precision\": precision,\n        \"recall\": recall,\n        \"f1\": f1,\n        \"accuracy\": accuracy\n    }\n\n# Data collator\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:40:06.443826Z","iopub.execute_input":"2025-05-30T11:40:06.444127Z","iopub.status.idle":"2025-05-30T11:40:06.449237Z","shell.execute_reply.started":"2025-05-30T11:40:06.444106Z","shell.execute_reply":"2025-05-30T11:40:06.448617Z"}},"outputs":[],"execution_count":11},{"id":"c3729d03-01a4-448c-a116-ff5b33a3ffa5","cell_type":"markdown","source":"## 8. Baseline Model (3 epochs)","metadata":{}},{"id":"a96a98d2-c172-4891-9a2e-ee171f2988d7","cell_type":"code","source":"def train_baseline():\n    \"\"\"Train GPT-Neo baseline for relation extraction.\"\"\"\n    print(\"Training baseline model...\")\n    \n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=len(all_labels),\n        id2label=id2label,\n        label2id=label2id\n    )\n    \n    # Resize embeddings for padding token\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.pad_token_id = tokenizer.pad_token_id\n    \n    training_args = TrainingArguments(\n        output_dir=\"outputs/gpt-neo-re-baseline\",\n        num_train_epochs=3,\n        per_device_train_batch_size=8,\n        per_device_eval_batch_size=16,\n        eval_strategy=\"epoch\",\n        save_strategy=\"epoch\",\n        logging_steps=50,\n        learning_rate=2e-5,\n        warmup_steps=500,\n        weight_decay=0.01,\n        load_best_model_at_end=True,\n        metric_for_best_model=\"eval_f1\",\n        fp16=torch.cuda.is_available(),\n        report_to=\"none\",\n        \n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=training_args,\n        train_dataset=train_dataset,\n        eval_dataset=dev_dataset,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n    \n    trainer.train()\n    metrics = trainer.evaluate()\n    \n    return trainer, metrics\n\nbaseline_trainer, baseline_metrics = train_baseline()\nprint(f\"\\nBaseline F1: {baseline_metrics['eval_f1']:.4f}\")\nprint(f\"Baseline Accuracy: {baseline_metrics['eval_accuracy']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T10:48:45.037113Z","iopub.execute_input":"2025-05-30T10:48:45.037663Z","iopub.status.idle":"2025-05-30T11:09:13.888118Z","shell.execute_reply.started":"2025-05-30T10:48:45.037640Z","shell.execute_reply":"2025-05-30T11:09:13.887482Z"}},"outputs":[{"name":"stdout","text":"Training baseline model...\n","output_type":"stream"},{"name":"stderr","text":"Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/3467633454.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='918' max='918' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [918/918 19:25, Epoch 3/3]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>2.439700</td>\n      <td>2.924925</td>\n      <td>0.286656</td>\n      <td>0.492911</td>\n      <td>0.337119</td>\n      <td>0.492911</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>1.248500</td>\n      <td>3.216945</td>\n      <td>0.264134</td>\n      <td>0.495830</td>\n      <td>0.340262</td>\n      <td>0.495830</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.603000</td>\n      <td>3.176116</td>\n      <td>0.281865</td>\n      <td>0.481651</td>\n      <td>0.352823</td>\n      <td>0.481651</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:59]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"\nBaseline F1: 0.3528\nBaseline Accuracy: 0.4817\n","output_type":"stream"}],"execution_count":16},{"id":"ecd032cf-4812-4cf4-a6c7-bfd6a4870c98","cell_type":"markdown","source":"## 9. Hyperparameter Tuning - Full Fine-Tuning","metadata":{}},{"id":"a32cb567-7a24-4e7d-9376-2e29114e48a5","cell_type":"code","source":"def ft_objective(trial):\n    \"\"\"Optuna objective for full fine-tuning.\"\"\"\n    lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n    \n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=len(all_labels),\n        id2label=id2label,\n        label2id=label2id\n    )\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.pad_token_id = tokenizer.pad_token_id\n    \n    args = TrainingArguments(\n        output_dir=f\"tmp/gpt-neo-re-ft-{trial.number}\",\n        max_steps=100,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size * 2,\n        eval_strategy=\"steps\",\n        eval_steps=20,\n        save_strategy=\"no\",\n        learning_rate=lr,\n        fp16=torch.cuda.is_available(),\n        logging_steps=20,\n        report_to=\"none\",\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        eval_dataset=dev_dataset,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n    \n    trainer.train()\n    return trainer.evaluate()[\"eval_f1\"]\n\n# Run optimization\nstudy_ft = optuna.create_study(direction=\"maximize\")\nstudy_ft.optimize(ft_objective, n_trials=8)\n\nprint(f\"Best FT params: {study_ft.best_params}\")\nprint(f\"Best FT F1: {study_ft.best_value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T07:41:41.611447Z","iopub.execute_input":"2025-05-30T07:41:41.611724Z","iopub.status.idle":"2025-05-30T08:47:32.611701Z","shell.execute_reply.started":"2025-05-30T07:41:41.611705Z","shell.execute_reply":"2025-05-30T08:47:32.611160Z"}},"outputs":[{"name":"stderr","text":"[I 2025-05-30 07:41:41,615] A new study created in memory with name: no-name-f1263397-589c-4f27-9c11-6acad772f407\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/3301683825.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:30, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>4.232200</td>\n      <td>3.697614</td>\n      <td>0.251698</td>\n      <td>0.463720</td>\n      <td>0.326291</td>\n      <td>0.463720</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.130200</td>\n      <td>3.400391</td>\n      <td>0.256349</td>\n      <td>0.394078</td>\n      <td>0.308444</td>\n      <td>0.394078</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.865700</td>\n      <td>3.197772</td>\n      <td>0.257396</td>\n      <td>0.469558</td>\n      <td>0.329419</td>\n      <td>0.469558</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.722600</td>\n      <td>3.142646</td>\n      <td>0.257786</td>\n      <td>0.456214</td>\n      <td>0.326720</td>\n      <td>0.456214</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.659700</td>\n      <td>3.141193</td>\n      <td>0.256909</td>\n      <td>0.450792</td>\n      <td>0.324751</td>\n      <td>0.450792</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:58]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 07:49:16,760] Trial 0 finished with value: 0.32475089956074776 and parameters: {'learning_rate': 2.718213731996222e-05, 'batch_size': 8}. Best is trial 0 with value: 0.32475089956074776.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/3301683825.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>4.347100</td>\n      <td>3.642421</td>\n      <td>0.254055</td>\n      <td>0.435780</td>\n      <td>0.319077</td>\n      <td>0.435780</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.269500</td>\n      <td>3.486077</td>\n      <td>0.250484</td>\n      <td>0.468307</td>\n      <td>0.326327</td>\n      <td>0.468307</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.231000</td>\n      <td>3.355156</td>\n      <td>0.254000</td>\n      <td>0.463303</td>\n      <td>0.326117</td>\n      <td>0.463303</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>3.075600</td>\n      <td>3.285646</td>\n      <td>0.253657</td>\n      <td>0.465388</td>\n      <td>0.326343</td>\n      <td>0.465388</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.884500</td>\n      <td>3.264959</td>\n      <td>0.297897</td>\n      <td>0.465805</td>\n      <td>0.328259</td>\n      <td>0.465805</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:59]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 07:57:04,887] Trial 1 finished with value: 0.32825857911654466 and parameters: {'learning_rate': 1.7789314534482568e-05, 'batch_size': 8}. Best is trial 1 with value: 0.32825857911654466.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/3301683825.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 08:11, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>3.616900</td>\n      <td>3.328562</td>\n      <td>0.261148</td>\n      <td>0.449958</td>\n      <td>0.323588</td>\n      <td>0.449958</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.832900</td>\n      <td>3.044296</td>\n      <td>0.270130</td>\n      <td>0.472894</td>\n      <td>0.337582</td>\n      <td>0.472894</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.428700</td>\n      <td>2.896502</td>\n      <td>0.343422</td>\n      <td>0.494162</td>\n      <td>0.334799</td>\n      <td>0.494162</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.297500</td>\n      <td>2.834202</td>\n      <td>0.283949</td>\n      <td>0.490409</td>\n      <td>0.338096</td>\n      <td>0.490409</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.234500</td>\n      <td>2.831627</td>\n      <td>0.274495</td>\n      <td>0.492911</td>\n      <td>0.333219</td>\n      <td>0.492911</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:56]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 08:06:19,157] Trial 2 finished with value: 0.3332189572895038 and parameters: {'learning_rate': 4.289287243180345e-05, 'batch_size': 16}. Best is trial 2 with value: 0.3332189572895038.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/3301683825.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>3.746600</td>\n      <td>3.415200</td>\n      <td>0.262125</td>\n      <td>0.445371</td>\n      <td>0.324268</td>\n      <td>0.445371</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.684200</td>\n      <td>3.097220</td>\n      <td>0.296706</td>\n      <td>0.488741</td>\n      <td>0.332239</td>\n      <td>0.488741</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.675600</td>\n      <td>2.977096</td>\n      <td>0.354982</td>\n      <td>0.495830</td>\n      <td>0.334449</td>\n      <td>0.495830</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.541700</td>\n      <td>2.915810</td>\n      <td>0.259955</td>\n      <td>0.484987</td>\n      <td>0.332886</td>\n      <td>0.484987</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.419300</td>\n      <td>2.901097</td>\n      <td>0.264683</td>\n      <td>0.488324</td>\n      <td>0.334194</td>\n      <td>0.488324</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:59]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 08:14:07,039] Trial 3 finished with value: 0.3341938880820841 and parameters: {'learning_rate': 4.904199543864886e-05, 'batch_size': 8}. Best is trial 3 with value: 0.3341938880820841.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/3301683825.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 08:12, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>4.178200</td>\n      <td>3.622427</td>\n      <td>0.249044</td>\n      <td>0.470809</td>\n      <td>0.325703</td>\n      <td>0.470809</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.368000</td>\n      <td>3.366399</td>\n      <td>0.254280</td>\n      <td>0.442869</td>\n      <td>0.321140</td>\n      <td>0.442869</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.910800</td>\n      <td>3.221748</td>\n      <td>0.260570</td>\n      <td>0.466639</td>\n      <td>0.327080</td>\n      <td>0.466639</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.655500</td>\n      <td>3.153152</td>\n      <td>0.265194</td>\n      <td>0.474979</td>\n      <td>0.336328</td>\n      <td>0.474979</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.673900</td>\n      <td>3.129953</td>\n      <td>0.266686</td>\n      <td>0.476230</td>\n      <td>0.337792</td>\n      <td>0.476230</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:57]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 08:23:21,855] Trial 4 finished with value: 0.33779166466164223 and parameters: {'learning_rate': 1.8235724987788577e-05, 'batch_size': 16}. Best is trial 4 with value: 0.33779166466164223.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/3301683825.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:04, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>4.730400</td>\n      <td>4.013316</td>\n      <td>0.256713</td>\n      <td>0.194746</td>\n      <td>0.219653</td>\n      <td>0.194746</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.723300</td>\n      <td>3.628083</td>\n      <td>0.254298</td>\n      <td>0.435780</td>\n      <td>0.319273</td>\n      <td>0.435780</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.341200</td>\n      <td>3.577728</td>\n      <td>0.250151</td>\n      <td>0.460801</td>\n      <td>0.324203</td>\n      <td>0.460801</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>3.190200</td>\n      <td>3.526107</td>\n      <td>0.253552</td>\n      <td>0.459133</td>\n      <td>0.324706</td>\n      <td>0.459133</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.487400</td>\n      <td>3.508117</td>\n      <td>0.253552</td>\n      <td>0.458716</td>\n      <td>0.324602</td>\n      <td>0.458716</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 01:00]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 08:30:30,355] Trial 5 finished with value: 0.32460173690730015 and parameters: {'learning_rate': 1.2793737556400751e-05, 'batch_size': 4}. Best is trial 4 with value: 0.33779166466164223.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/3301683825.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 08:11, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>4.490900</td>\n      <td>3.719385</td>\n      <td>0.260314</td>\n      <td>0.363219</td>\n      <td>0.300403</td>\n      <td>0.363219</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.574900</td>\n      <td>3.499633</td>\n      <td>0.252980</td>\n      <td>0.457882</td>\n      <td>0.323913</td>\n      <td>0.457882</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.181400</td>\n      <td>3.375957</td>\n      <td>0.253685</td>\n      <td>0.444120</td>\n      <td>0.320983</td>\n      <td>0.444120</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.889300</td>\n      <td>3.305497</td>\n      <td>0.254225</td>\n      <td>0.464137</td>\n      <td>0.326513</td>\n      <td>0.464137</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.932400</td>\n      <td>3.283396</td>\n      <td>0.341456</td>\n      <td>0.465805</td>\n      <td>0.327363</td>\n      <td>0.465805</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:57]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 08:39:45,002] Trial 6 finished with value: 0.3273630986696184 and parameters: {'learning_rate': 1.2600448474998054e-05, 'batch_size': 16}. Best is trial 4 with value: 0.33779166466164223.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/3301683825.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>4.119400</td>\n      <td>3.637821</td>\n      <td>0.249056</td>\n      <td>0.476230</td>\n      <td>0.327004</td>\n      <td>0.476230</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.070100</td>\n      <td>3.366903</td>\n      <td>0.258698</td>\n      <td>0.453294</td>\n      <td>0.324283</td>\n      <td>0.453294</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.020200</td>\n      <td>3.220876</td>\n      <td>0.250381</td>\n      <td>0.478732</td>\n      <td>0.328749</td>\n      <td>0.478732</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.863000</td>\n      <td>3.150512</td>\n      <td>0.264496</td>\n      <td>0.473311</td>\n      <td>0.332743</td>\n      <td>0.473311</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.684800</td>\n      <td>3.133572</td>\n      <td>0.273737</td>\n      <td>0.471643</td>\n      <td>0.338398</td>\n      <td>0.471643</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:58]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 08:47:32,608] Trial 7 finished with value: 0.33839777910918706 and parameters: {'learning_rate': 2.467262880195005e-05, 'batch_size': 8}. Best is trial 7 with value: 0.33839777910918706.\n","output_type":"stream"},{"name":"stdout","text":"Best FT params: {'learning_rate': 2.467262880195005e-05, 'batch_size': 8}\nBest FT F1: 0.3384\n","output_type":"stream"}],"execution_count":11},{"id":"66b6fd21-ec64-46c8-ab46-26eda0b80c8f","cell_type":"markdown","source":"## 10. Hyperparameter Tuning - LoRA","metadata":{}},{"id":"b156c0b2-2a73-4745-b593-f6dcd6f7e315","cell_type":"code","source":"def lora_objective(trial):\n    \"\"\"Optuna objective for LoRA fine-tuning.\"\"\"\n    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n    r = trial.suggest_categorical(\"r\", [4, 8, 16])\n    alpha = trial.suggest_categorical(\"alpha\", [16, 32])\n    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n    \n    lora_config = LoraConfig(\n        task_type=TaskType.SEQ_CLS,\n        inference_mode=False,\n        r=r,\n        lora_alpha=alpha,\n        lora_dropout=dropout,\n        target_modules=[\"c_attn\", \"c_proj\"],  # GPT-Neo specific\n    )\n    \n    base_model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=len(all_labels),\n        id2label=id2label,\n        label2id=label2id\n    )\n    base_model.resize_token_embeddings(len(tokenizer))\n    base_model.config.pad_token_id = tokenizer.pad_token_id\n    \n    model = get_peft_model(base_model, lora_config)\n    \n    args = TrainingArguments(\n        output_dir=f\"tmp/gpt-neo-re-lora-{trial.number}\",\n        max_steps=100,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size * 2,\n        eval_strategy=\"steps\",\n        eval_steps=20,\n        save_strategy=\"no\",\n        learning_rate=lr,\n        fp16=torch.cuda.is_available(),\n        logging_steps=20,\n        report_to=\"none\",\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        eval_dataset=dev_dataset,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n    \n    trainer.train()\n    return trainer.evaluate()[\"eval_f1\"]\n\n# Run optimization\nstudy_lora = optuna.create_study(direction=\"maximize\")\nstudy_lora.optimize(lora_objective, n_trials=8)\n\nprint(f\"Best LoRA params: {study_lora.best_params}\")\nprint(f\"Best LoRA F1: {study_lora.best_value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T08:48:07.899990Z","iopub.execute_input":"2025-05-30T08:48:07.900257Z","iopub.status.idle":"2025-05-30T09:48:26.660740Z","shell.execute_reply.started":"2025-05-30T08:48:07.900236Z","shell.execute_reply":"2025-05-30T09:48:26.660112Z"}},"outputs":[{"name":"stderr","text":"[I 2025-05-30 08:48:07,905] A new study created in memory with name: no-name-70d0cb57-538d-4c10-8911-1d4bbb7229bd\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/74789375.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:19, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>3.824000</td>\n      <td>3.386741</td>\n      <td>0.271399</td>\n      <td>0.338198</td>\n      <td>0.289908</td>\n      <td>0.338198</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.693700</td>\n      <td>3.216962</td>\n      <td>0.267140</td>\n      <td>0.429942</td>\n      <td>0.327713</td>\n      <td>0.429942</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.718000</td>\n      <td>3.185081</td>\n      <td>0.351332</td>\n      <td>0.480400</td>\n      <td>0.335047</td>\n      <td>0.480400</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.606300</td>\n      <td>3.131421</td>\n      <td>0.275725</td>\n      <td>0.456214</td>\n      <td>0.334647</td>\n      <td>0.456214</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.412200</td>\n      <td>3.102318</td>\n      <td>0.277296</td>\n      <td>0.457882</td>\n      <td>0.333865</td>\n      <td>0.457882</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:58]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 08:55:30,529] Trial 0 finished with value: 0.3338652593795139 and parameters: {'learning_rate': 0.0005495247793254406, 'r': 8, 'alpha': 32, 'dropout': 0.2649790520673084, 'batch_size': 8}. Best is trial 0 with value: 0.3338652593795139.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/74789375.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 05:53, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.434900</td>\n      <td>5.199249</td>\n      <td>0.210932</td>\n      <td>0.014178</td>\n      <td>0.019206</td>\n      <td>0.014178</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.019400</td>\n      <td>4.747695</td>\n      <td>0.303124</td>\n      <td>0.031276</td>\n      <td>0.048273</td>\n      <td>0.031276</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>4.552300</td>\n      <td>4.322066</td>\n      <td>0.300129</td>\n      <td>0.070475</td>\n      <td>0.107033</td>\n      <td>0.070475</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>4.010900</td>\n      <td>4.049629</td>\n      <td>0.298572</td>\n      <td>0.153461</td>\n      <td>0.193407</td>\n      <td>0.153461</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.020400</td>\n      <td>3.956906</td>\n      <td>0.293527</td>\n      <td>0.179316</td>\n      <td>0.211907</td>\n      <td>0.179316</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 01:00]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 09:02:27,499] Trial 1 finished with value: 0.21190738829388148 and parameters: {'learning_rate': 7.092012897485746e-05, 'r': 8, 'alpha': 16, 'dropout': 0.20374046911342464, 'batch_size': 4}. Best is trial 0 with value: 0.3338652593795139.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/74789375.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 07:24, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>3.696800</td>\n      <td>3.301261</td>\n      <td>0.266233</td>\n      <td>0.413261</td>\n      <td>0.316360</td>\n      <td>0.413261</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.823200</td>\n      <td>3.200393</td>\n      <td>0.277170</td>\n      <td>0.445788</td>\n      <td>0.332803</td>\n      <td>0.445788</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.395300</td>\n      <td>3.100075</td>\n      <td>0.275913</td>\n      <td>0.460384</td>\n      <td>0.333653</td>\n      <td>0.460384</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.292900</td>\n      <td>3.065047</td>\n      <td>0.282581</td>\n      <td>0.460801</td>\n      <td>0.340512</td>\n      <td>0.460801</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.220400</td>\n      <td>3.057290</td>\n      <td>0.277315</td>\n      <td>0.463720</td>\n      <td>0.335461</td>\n      <td>0.463720</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:56]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 09:10:53,613] Trial 2 finished with value: 0.33546085674843473 and parameters: {'learning_rate': 0.0005785687500877714, 'r': 4, 'alpha': 16, 'dropout': 0.24071697680466897, 'batch_size': 16}. Best is trial 2 with value: 0.33546085674843473.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/74789375.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:19, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>4.047500</td>\n      <td>3.431445</td>\n      <td>0.255929</td>\n      <td>0.460801</td>\n      <td>0.325942</td>\n      <td>0.460801</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.729800</td>\n      <td>3.214827</td>\n      <td>0.269512</td>\n      <td>0.429108</td>\n      <td>0.326953</td>\n      <td>0.429108</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.807000</td>\n      <td>3.175115</td>\n      <td>0.266124</td>\n      <td>0.450792</td>\n      <td>0.328590</td>\n      <td>0.450792</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.667200</td>\n      <td>3.151272</td>\n      <td>0.264948</td>\n      <td>0.431193</td>\n      <td>0.325302</td>\n      <td>0.431193</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.488400</td>\n      <td>3.137419</td>\n      <td>0.268939</td>\n      <td>0.438699</td>\n      <td>0.329309</td>\n      <td>0.438699</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:58]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 09:18:16,103] Trial 3 finished with value: 0.32930888954527415 and parameters: {'learning_rate': 0.000449627225192418, 'r': 8, 'alpha': 16, 'dropout': 0.040571844025126355, 'batch_size': 8}. Best is trial 2 with value: 0.33546085674843473.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/74789375.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 05:53, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.156600</td>\n      <td>4.552033</td>\n      <td>0.313628</td>\n      <td>0.039199</td>\n      <td>0.060948</td>\n      <td>0.039199</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.899600</td>\n      <td>3.503607</td>\n      <td>0.257764</td>\n      <td>0.420767</td>\n      <td>0.316327</td>\n      <td>0.420767</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>3.251500</td>\n      <td>3.511268</td>\n      <td>0.248838</td>\n      <td>0.475813</td>\n      <td>0.326717</td>\n      <td>0.475813</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>3.009400</td>\n      <td>3.400662</td>\n      <td>0.252535</td>\n      <td>0.464554</td>\n      <td>0.325191</td>\n      <td>0.464554</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.279100</td>\n      <td>3.375305</td>\n      <td>0.253669</td>\n      <td>0.457048</td>\n      <td>0.324281</td>\n      <td>0.457048</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 01:01]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 09:25:13,828] Trial 4 finished with value: 0.3242806797240686 and parameters: {'learning_rate': 0.00012903007255386158, 'r': 4, 'alpha': 32, 'dropout': 0.1815007570533057, 'batch_size': 4}. Best is trial 2 with value: 0.33546085674843473.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/74789375.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 07:24, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>3.918100</td>\n      <td>3.457789</td>\n      <td>0.261233</td>\n      <td>0.469975</td>\n      <td>0.326710</td>\n      <td>0.469975</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.900700</td>\n      <td>3.191489</td>\n      <td>0.266411</td>\n      <td>0.415763</td>\n      <td>0.322771</td>\n      <td>0.415763</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.471800</td>\n      <td>3.114481</td>\n      <td>0.267892</td>\n      <td>0.445788</td>\n      <td>0.329665</td>\n      <td>0.445788</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.359000</td>\n      <td>3.077873</td>\n      <td>0.283775</td>\n      <td>0.460801</td>\n      <td>0.338733</td>\n      <td>0.460801</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.301100</td>\n      <td>3.074096</td>\n      <td>0.279705</td>\n      <td>0.462469</td>\n      <td>0.337188</td>\n      <td>0.462469</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:57]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 09:33:40,777] Trial 5 finished with value: 0.33718829561183883 and parameters: {'learning_rate': 0.00043902157016477775, 'r': 4, 'alpha': 16, 'dropout': 0.08332129356555738, 'batch_size': 16}. Best is trial 5 with value: 0.33718829561183883.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/74789375.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:19, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.571300</td>\n      <td>5.368583</td>\n      <td>0.238555</td>\n      <td>0.012093</td>\n      <td>0.016007</td>\n      <td>0.012093</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.316900</td>\n      <td>5.105868</td>\n      <td>0.240999</td>\n      <td>0.015847</td>\n      <td>0.021643</td>\n      <td>0.015847</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>5.178800</td>\n      <td>4.851790</td>\n      <td>0.287849</td>\n      <td>0.026689</td>\n      <td>0.041038</td>\n      <td>0.026689</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>4.853600</td>\n      <td>4.681544</td>\n      <td>0.324010</td>\n      <td>0.035446</td>\n      <td>0.056517</td>\n      <td>0.035446</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.595400</td>\n      <td>4.619410</td>\n      <td>0.319793</td>\n      <td>0.037531</td>\n      <td>0.059343</td>\n      <td>0.037531</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:58]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 09:41:03,898] Trial 6 finished with value: 0.05934330361580167 and parameters: {'learning_rate': 3.363755225151843e-05, 'r': 4, 'alpha': 32, 'dropout': 0.04921286842875293, 'batch_size': 8}. Best is trial 5 with value: 0.33718829561183883.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/74789375.py:43: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:20, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.637000</td>\n      <td>5.492134</td>\n      <td>0.230549</td>\n      <td>0.011676</td>\n      <td>0.015135</td>\n      <td>0.011676</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.557000</td>\n      <td>5.398161</td>\n      <td>0.227478</td>\n      <td>0.012510</td>\n      <td>0.015969</td>\n      <td>0.012510</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>5.599200</td>\n      <td>5.320100</td>\n      <td>0.251188</td>\n      <td>0.013761</td>\n      <td>0.017560</td>\n      <td>0.013761</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>5.424300</td>\n      <td>5.271939</td>\n      <td>0.236172</td>\n      <td>0.013761</td>\n      <td>0.017557</td>\n      <td>0.013761</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.254900</td>\n      <td>5.254430</td>\n      <td>0.211617</td>\n      <td>0.013761</td>\n      <td>0.017526</td>\n      <td>0.013761</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:58]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 09:48:26,656] Trial 7 finished with value: 0.017526020742548522 and parameters: {'learning_rate': 1.8884826281152702e-05, 'r': 4, 'alpha': 16, 'dropout': 0.1607390896991702, 'batch_size': 8}. Best is trial 5 with value: 0.33718829561183883.\n","output_type":"stream"},{"name":"stdout","text":"Best LoRA params: {'learning_rate': 0.00043902157016477775, 'r': 4, 'alpha': 16, 'dropout': 0.08332129356555738, 'batch_size': 16}\nBest LoRA F1: 0.3372\n","output_type":"stream"}],"execution_count":13},{"id":"17f5b685-ba64-42bf-89e7-9651134257b2","cell_type":"markdown","source":"## 11. Hyperparameter Tuning - Partial Freezing","metadata":{}},{"id":"fc21193d-9cd8-4fe8-9807-ab53a1a4691a","cell_type":"code","source":"def freeze_objective(trial):\n    \"\"\"Optuna objective for partial freezing.\"\"\"\n    lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n    batch_size = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n    freeze_pct = trial.suggest_float(\"freeze_pct\", 0.25, 0.75)\n    \n    model = AutoModelForSequenceClassification.from_pretrained(\n        model_name,\n        num_labels=len(all_labels),\n        id2label=id2label,\n        label2id=label2id\n    )\n    model.resize_token_embeddings(len(tokenizer))\n    model.config.pad_token_id = tokenizer.pad_token_id\n    \n    # Freeze lower layers\n    total_layers = len([n for n, _ in model.named_parameters() if n.startswith(\"transformer.h.\")])\n    layers_to_freeze = int(total_layers * freeze_pct)\n    \n    for name, param in model.named_parameters():\n        if name.startswith(\"transformer.h.\"):\n            layer_num = int(name.split(\".\")[2])\n            if layer_num < layers_to_freeze:\n                param.requires_grad = False\n    \n    args = TrainingArguments(\n        output_dir=f\"tmp/gpt-neo-re-freeze-{trial.number}\",\n        max_steps=100,\n        per_device_train_batch_size=batch_size,\n        per_device_eval_batch_size=batch_size * 2,\n        eval_strategy=\"steps\",\n        eval_steps=20,\n        save_strategy=\"no\",\n        learning_rate=lr,\n        fp16=torch.cuda.is_available(),\n        logging_steps=20,\n        report_to=\"none\",\n    )\n    \n    trainer = Trainer(\n        model=model,\n        args=args,\n        train_dataset=train_dataset,\n        eval_dataset=dev_dataset,\n        data_collator=data_collator,\n        tokenizer=tokenizer,\n        compute_metrics=compute_metrics,\n    )\n    \n    trainer.train()\n    return trainer.evaluate()[\"eval_f1\"]\n\n# Run optimization\nstudy_freeze = optuna.create_study(direction=\"maximize\")\nstudy_freeze.optimize(freeze_objective, n_trials=8)\n\nprint(f\"Best Freeze params: {study_freeze.best_params}\")\nprint(f\"Best Freeze F1: {study_freeze.best_value:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T09:49:24.884779Z","iopub.execute_input":"2025-05-30T09:49:24.885341Z","iopub.status.idle":"2025-05-30T10:48:22.478631Z","shell.execute_reply.started":"2025-05-30T09:49:24.885318Z","shell.execute_reply":"2025-05-30T10:48:22.477917Z"}},"outputs":[{"name":"stderr","text":"[I 2025-05-30 09:49:24,891] A new study created in memory with name: no-name-b7cad64f-4c4c-454c-905c-406f87afbd79\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/2104240225.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:11, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.540300</td>\n      <td>5.379679</td>\n      <td>0.238583</td>\n      <td>0.012093</td>\n      <td>0.016013</td>\n      <td>0.012093</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.290300</td>\n      <td>5.217203</td>\n      <td>0.210785</td>\n      <td>0.013344</td>\n      <td>0.016964</td>\n      <td>0.013344</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>5.246800</td>\n      <td>5.102019</td>\n      <td>0.236881</td>\n      <td>0.016264</td>\n      <td>0.021993</td>\n      <td>0.016264</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>5.022300</td>\n      <td>5.038916</td>\n      <td>0.258887</td>\n      <td>0.017515</td>\n      <td>0.024480</td>\n      <td>0.017515</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.811500</td>\n      <td>5.016931</td>\n      <td>0.295764</td>\n      <td>0.020017</td>\n      <td>0.029295</td>\n      <td>0.020017</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:57]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 09:56:37,508] Trial 0 finished with value: 0.029295413090263005 and parameters: {'learning_rate': 3.0496021334728374e-05, 'batch_size': 8, 'freeze_pct': 0.7494639930228499}. Best is trial 0 with value: 0.029295413090263005.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/2104240225.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:11, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.623900</td>\n      <td>5.492770</td>\n      <td>0.298808</td>\n      <td>0.011676</td>\n      <td>0.015239</td>\n      <td>0.011676</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.525500</td>\n      <td>5.411210</td>\n      <td>0.226909</td>\n      <td>0.012093</td>\n      <td>0.015478</td>\n      <td>0.012093</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>5.573000</td>\n      <td>5.352933</td>\n      <td>0.239167</td>\n      <td>0.012927</td>\n      <td>0.016293</td>\n      <td>0.012927</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>5.405800</td>\n      <td>5.320802</td>\n      <td>0.250699</td>\n      <td>0.013344</td>\n      <td>0.017088</td>\n      <td>0.013344</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.241000</td>\n      <td>5.309570</td>\n      <td>0.250618</td>\n      <td>0.013344</td>\n      <td>0.017063</td>\n      <td>0.013344</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:57]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 10:03:49,947] Trial 1 finished with value: 0.017063297670036028 and parameters: {'learning_rate': 1.5047486593494805e-05, 'batch_size': 8, 'freeze_pct': 0.6134478712620085}. Best is trial 0 with value: 0.029295413090263005.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/2104240225.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 07:17, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.569200</td>\n      <td>5.395816</td>\n      <td>0.227664</td>\n      <td>0.012510</td>\n      <td>0.016021</td>\n      <td>0.012510</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.419900</td>\n      <td>5.243964</td>\n      <td>0.210081</td>\n      <td>0.012927</td>\n      <td>0.016454</td>\n      <td>0.012927</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>5.132300</td>\n      <td>5.139391</td>\n      <td>0.225589</td>\n      <td>0.014595</td>\n      <td>0.019572</td>\n      <td>0.014595</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>4.889500</td>\n      <td>5.073888</td>\n      <td>0.233123</td>\n      <td>0.015013</td>\n      <td>0.020337</td>\n      <td>0.015013</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.971400</td>\n      <td>5.051581</td>\n      <td>0.240382</td>\n      <td>0.015430</td>\n      <td>0.021133</td>\n      <td>0.015430</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:55]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 10:12:07,687] Trial 2 finished with value: 0.02113279349765483 and parameters: {'learning_rate': 2.3960600044692483e-05, 'batch_size': 16, 'freeze_pct': 0.4148388873734723}. Best is trial 0 with value: 0.029295413090263005.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/2104240225.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:11, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.458900</td>\n      <td>5.269616</td>\n      <td>0.222164</td>\n      <td>0.012927</td>\n      <td>0.016905</td>\n      <td>0.012927</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.064100</td>\n      <td>5.030199</td>\n      <td>0.278035</td>\n      <td>0.019600</td>\n      <td>0.028134</td>\n      <td>0.019600</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>4.937800</td>\n      <td>4.862018</td>\n      <td>0.288606</td>\n      <td>0.025438</td>\n      <td>0.038528</td>\n      <td>0.025438</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>4.662400</td>\n      <td>4.770736</td>\n      <td>0.288638</td>\n      <td>0.027940</td>\n      <td>0.042878</td>\n      <td>0.027940</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.413300</td>\n      <td>4.739151</td>\n      <td>0.291394</td>\n      <td>0.029191</td>\n      <td>0.045062</td>\n      <td>0.029191</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:57]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 10:19:20,454] Trial 3 finished with value: 0.04506217057743882 and parameters: {'learning_rate': 4.573948969033365e-05, 'batch_size': 8, 'freeze_pct': 0.7019650704348654}. Best is trial 3 with value: 0.04506217057743882.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/2104240225.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 06:11, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.497300</td>\n      <td>5.321597</td>\n      <td>0.250314</td>\n      <td>0.012510</td>\n      <td>0.016857</td>\n      <td>0.012510</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.170600</td>\n      <td>5.118277</td>\n      <td>0.235873</td>\n      <td>0.015847</td>\n      <td>0.021444</td>\n      <td>0.015847</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>5.082600</td>\n      <td>4.974785</td>\n      <td>0.284865</td>\n      <td>0.022519</td>\n      <td>0.033434</td>\n      <td>0.022519</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>4.830500</td>\n      <td>4.896509</td>\n      <td>0.296356</td>\n      <td>0.025021</td>\n      <td>0.037949</td>\n      <td>0.025021</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.598600</td>\n      <td>4.869318</td>\n      <td>0.292917</td>\n      <td>0.025438</td>\n      <td>0.038673</td>\n      <td>0.025438</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:57]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 10:26:33,330] Trial 4 finished with value: 0.038672877447831974 and parameters: {'learning_rate': 3.851381478939642e-05, 'batch_size': 8, 'freeze_pct': 0.2543291611845244}. Best is trial 3 with value: 0.04506217057743882.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/2104240225.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 05:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.485900</td>\n      <td>5.355409</td>\n      <td>0.238740</td>\n      <td>0.011676</td>\n      <td>0.015957</td>\n      <td>0.011676</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.261700</td>\n      <td>5.174768</td>\n      <td>0.210787</td>\n      <td>0.013761</td>\n      <td>0.018807</td>\n      <td>0.013761</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>5.071000</td>\n      <td>5.052107</td>\n      <td>0.282167</td>\n      <td>0.019183</td>\n      <td>0.027621</td>\n      <td>0.019183</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>4.843000</td>\n      <td>4.980114</td>\n      <td>0.283465</td>\n      <td>0.022102</td>\n      <td>0.032831</td>\n      <td>0.022102</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.893500</td>\n      <td>4.953307</td>\n      <td>0.300114</td>\n      <td>0.024187</td>\n      <td>0.036465</td>\n      <td>0.024187</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 00:59]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 10:33:19,603] Trial 5 finished with value: 0.03646463582388761 and parameters: {'learning_rate': 4.121408245899747e-05, 'batch_size': 4, 'freeze_pct': 0.7280325738566156}. Best is trial 3 with value: 0.04506217057743882.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/2104240225.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 07:16, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.644900</td>\n      <td>5.491927</td>\n      <td>0.254552</td>\n      <td>0.011259</td>\n      <td>0.014760</td>\n      <td>0.011259</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.616700</td>\n      <td>5.409361</td>\n      <td>0.227140</td>\n      <td>0.012510</td>\n      <td>0.015616</td>\n      <td>0.012510</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>5.431200</td>\n      <td>5.352139</td>\n      <td>0.239589</td>\n      <td>0.012927</td>\n      <td>0.016408</td>\n      <td>0.012927</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>5.243700</td>\n      <td>5.316144</td>\n      <td>0.251027</td>\n      <td>0.013344</td>\n      <td>0.017191</td>\n      <td>0.013344</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.355800</td>\n      <td>5.303869</td>\n      <td>0.251025</td>\n      <td>0.013344</td>\n      <td>0.017188</td>\n      <td>0.013344</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:55]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 10:41:35,908] Trial 6 finished with value: 0.017188254910011663 and parameters: {'learning_rate': 1.2853278607687636e-05, 'batch_size': 16, 'freeze_pct': 0.3398122150862259}. Best is trial 3 with value: 0.04506217057743882.\nSome weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n/tmp/ipykernel_35/2104240225.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [100/100 05:44, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>5.566500</td>\n      <td>5.470162</td>\n      <td>0.288075</td>\n      <td>0.010842</td>\n      <td>0.014490</td>\n      <td>0.010842</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.485900</td>\n      <td>5.371508</td>\n      <td>0.226022</td>\n      <td>0.011259</td>\n      <td>0.015115</td>\n      <td>0.011259</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>5.393800</td>\n      <td>5.304023</td>\n      <td>0.235246</td>\n      <td>0.012927</td>\n      <td>0.016918</td>\n      <td>0.012927</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>5.248600</td>\n      <td>5.264066</td>\n      <td>0.210815</td>\n      <td>0.013344</td>\n      <td>0.016978</td>\n      <td>0.013344</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>5.302300</td>\n      <td>5.249181</td>\n      <td>0.210815</td>\n      <td>0.013344</td>\n      <td>0.016978</td>\n      <td>0.013344</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 00:59]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"[I 2025-05-30 10:48:22,474] Trial 7 finished with value: 0.016978002083481606 and parameters: {'learning_rate': 2.2059395491566637e-05, 'batch_size': 4, 'freeze_pct': 0.39228928266480134}. Best is trial 3 with value: 0.04506217057743882.\n","output_type":"stream"},{"name":"stdout","text":"Best Freeze params: {'learning_rate': 4.573948969033365e-05, 'batch_size': 8, 'freeze_pct': 0.7019650704348654}\nBest Freeze F1: 0.0451\n","output_type":"stream"}],"execution_count":14},{"id":"58109943-b625-4996-ace1-29d6c70d9474","cell_type":"markdown","source":"## 12. Final Training with Best Parameters","metadata":{}},{"id":"b899c390-108c-40fa-afbb-725ebec8bdc5","cell_type":"code","source":"# Store results\nresults_summary = {\n    \"baseline\": baseline_metrics,\n    \"full_ft\": None,\n    \"lora\": None,\n    \"partial_freeze\": None\n}\n\n# Full Fine-Tuning with best params\nprint(\"\\n\" + \"=\"*50)\nprint(\"FULL FINE-TUNING WITH BEST PARAMETERS\")\nprint(\"=\"*50)\n\nbest_ft_params = study_ft.best_params\nft_model = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(all_labels),\n    id2label=id2label,\n    label2id=label2id\n)\nft_model.resize_token_embeddings(len(tokenizer))\nft_model.config.pad_token_id = tokenizer.pad_token_id\n\nft_args = TrainingArguments(\n    output_dir=\"outputs/gpt-neo-re-ft-final\",\n    max_steps=200,\n    per_device_train_batch_size=best_ft_params[\"batch_size\"],\n    per_device_eval_batch_size=best_ft_params[\"batch_size\"] * 2,\n    eval_strategy=\"steps\",\n    eval_steps=20,\n    save_strategy=\"no\",\n    learning_rate=best_ft_params[\"learning_rate\"],\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\",\n)\n\nft_trainer = Trainer(\n    model=ft_model,\n    args=ft_args,\n    train_dataset=train_dataset,\n    eval_dataset=dev_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nft_trainer.train()\nft_metrics = ft_trainer.evaluate()\nresults_summary[\"full_ft\"] = ft_metrics\nprint(f\"Full FT F1: {ft_metrics['eval_f1']:.4f}\")\n\n# LoRA with best params\nprint(\"\\n\" + \"=\"*50)\nprint(\"LoRA WITH BEST PARAMETERS\")\nprint(\"=\"*50)\n\nbest_lora_params = study_lora.best_params\nlora_config = LoraConfig(\n    task_type=TaskType.SEQ_CLS,\n    inference_mode=False,\n    r=best_lora_params[\"r\"],\n    lora_alpha=best_lora_params[\"alpha\"],\n    lora_dropout=best_lora_params[\"dropout\"],\n    target_modules=[\"c_attn\", \"c_proj\"],\n)\n\nbase_model = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(all_labels),\n    id2label=id2label,\n    label2id=label2id\n)\nbase_model.resize_token_embeddings(len(tokenizer))\nbase_model.config.pad_token_id = tokenizer.pad_token_id\n\nlora_model = get_peft_model(base_model, lora_config)\nlora_model.print_trainable_parameters()\n\nlora_args = TrainingArguments(\n    output_dir=\"outputs/gpt-neo-re-lora-final\",\n    max_steps=200,\n    per_device_train_batch_size=best_lora_params[\"batch_size\"],\n    per_device_eval_batch_size=best_lora_params[\"batch_size\"] * 2,\n    eval_strategy=\"steps\",\n    eval_steps=20,\n    save_strategy=\"no\",\n    learning_rate=best_lora_params[\"learning_rate\"],\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\",\n    \n)\n\nlora_trainer = Trainer(\n    model=lora_model,\n    args=lora_args,\n    train_dataset=train_dataset,\n    eval_dataset=dev_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nlora_trainer.train()\nlora_metrics = lora_trainer.evaluate()\nresults_summary[\"lora\"] = lora_metrics\nprint(f\"LoRA F1: {lora_metrics['eval_f1']:.4f}\")\n\n# Partial Freezing with best params\nprint(\"\\n\" + \"=\"*50)\nprint(\"PARTIAL FREEZING WITH BEST PARAMETERS\")\nprint(\"=\"*50)\n\nbest_freeze_params = study_freeze.best_params\nfreeze_model = AutoModelForSequenceClassification.from_pretrained(\n    model_name,\n    num_labels=len(all_labels),\n    id2label=id2label,\n    label2id=label2id\n)\nfreeze_model.resize_token_embeddings(len(tokenizer))\nfreeze_model.config.pad_token_id = tokenizer.pad_token_id\n\n# Apply freezing\ntotal_layers = len([n for n, _ in freeze_model.named_parameters() if n.startswith(\"transformer.h.\")])\nlayers_to_freeze = int(total_layers * best_freeze_params[\"freeze_pct\"])\n\nfor name, param in freeze_model.named_parameters():\n    if name.startswith(\"transformer.h.\"):\n        layer_num = int(name.split(\".\")[2])\n        if layer_num < layers_to_freeze:\n            param.requires_grad = False\n\n# Count trainable parameters\ntrainable_params = sum(p.numel() for p in freeze_model.parameters() if p.requires_grad)\ntotal_params = sum(p.numel() for p in freeze_model.parameters())\nprint(f\"Trainable parameters: {trainable_params:,} / {total_params:,} ({trainable_params/total_params*100:.2f}%)\")\n\nfreeze_args = TrainingArguments(\n    output_dir=\"outputs/gpt-neo-re-freeze-final\",\n    max_steps=200,\n    per_device_train_batch_size=best_freeze_params[\"batch_size\"],\n    per_device_eval_batch_size=best_freeze_params[\"batch_size\"] * 2,\n    eval_strategy=\"steps\",\n    eval_steps=20,\n    save_strategy=\"no\",\n    learning_rate=best_freeze_params[\"learning_rate\"],\n    fp16=torch.cuda.is_available(),\n    report_to=\"none\",\n)\n\nfreeze_trainer = Trainer(\n    model=freeze_model,\n    args=freeze_args,\n    train_dataset=train_dataset,\n    eval_dataset=dev_dataset,\n    data_collator=data_collator,\n    tokenizer=tokenizer,\n    compute_metrics=compute_metrics,\n)\n\nfreeze_trainer.train()\nfreeze_metrics = freeze_trainer.evaluate()\nresults_summary[\"partial_freeze\"] = freeze_metrics\nprint(f\"Partial Freeze F1: {freeze_metrics['eval_f1']:.4f}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T11:47:30.683375Z","iopub.execute_input":"2025-05-30T11:47:30.683639Z","iopub.status.idle":"2025-05-30T12:32:39.028297Z","shell.execute_reply.started":"2025-05-30T11:47:30.683620Z","shell.execute_reply":"2025-05-30T12:32:39.027695Z"}},"outputs":[{"name":"stdout","text":"\\n==================================================\nFULL FINE-TUNING WITH BEST PARAMETERS\n==================================================\nFT Final Training: Effective BS: 8, Actual BS: 4, Grad Acc: 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/1.01k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"785baac53ab24b2f90a87b3bb101a6fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/526M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"57f7e976667d4c4e80ce01ec2f38ff26"}},"metadata":{}},{"name":"stderr","text":"Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nThe new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n/tmp/ipykernel_35/815101731.py:55: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  ft_trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 13:34, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>3.975800</td>\n      <td>3.863049</td>\n      <td>0.294008</td>\n      <td>0.471643</td>\n      <td>0.327699</td>\n      <td>0.471643</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.061500</td>\n      <td>3.363433</td>\n      <td>0.266824</td>\n      <td>0.449958</td>\n      <td>0.328421</td>\n      <td>0.449958</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.858700</td>\n      <td>3.293933</td>\n      <td>0.273011</td>\n      <td>0.486239</td>\n      <td>0.333411</td>\n      <td>0.486239</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.657600</td>\n      <td>3.140432</td>\n      <td>0.269625</td>\n      <td>0.467473</td>\n      <td>0.334335</td>\n      <td>0.467473</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.700200</td>\n      <td>3.098319</td>\n      <td>0.269354</td>\n      <td>0.487490</td>\n      <td>0.331820</td>\n      <td>0.487490</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.334500</td>\n      <td>3.037919</td>\n      <td>0.276307</td>\n      <td>0.490409</td>\n      <td>0.335045</td>\n      <td>0.490409</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.384800</td>\n      <td>3.008042</td>\n      <td>0.266998</td>\n      <td>0.494579</td>\n      <td>0.335055</td>\n      <td>0.494579</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.306800</td>\n      <td>3.005566</td>\n      <td>0.279509</td>\n      <td>0.495413</td>\n      <td>0.337821</td>\n      <td>0.495413</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>2.236000</td>\n      <td>3.004538</td>\n      <td>0.263473</td>\n      <td>0.495413</td>\n      <td>0.333691</td>\n      <td>0.495413</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>2.376800</td>\n      <td>2.995371</td>\n      <td>0.266208</td>\n      <td>0.493745</td>\n      <td>0.333787</td>\n      <td>0.493745</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:58]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Full FT F1: 0.3338\n\\n==================================================\nLoRA WITH BEST PARAMETERS\n==================================================\nLoRA Final Training: Effective BS: 16, Actual BS: 4, Grad Acc: 4\n","output_type":"stream"},{"name":"stderr","text":"Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"trainable params: 242,688 || all params: 125,500,416 || trainable%: 0.1934\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/815101731.py:126: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  lora_trainer = Trainer(\nNo label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 15:41, Epoch 1/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>3.844100</td>\n      <td>3.412768</td>\n      <td>0.275179</td>\n      <td>0.436614</td>\n      <td>0.321271</td>\n      <td>0.436614</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>2.663200</td>\n      <td>3.349540</td>\n      <td>0.281909</td>\n      <td>0.386155</td>\n      <td>0.312808</td>\n      <td>0.386155</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>2.429000</td>\n      <td>3.264125</td>\n      <td>0.260054</td>\n      <td>0.389074</td>\n      <td>0.309101</td>\n      <td>0.389074</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>2.282500</td>\n      <td>3.252667</td>\n      <td>0.262789</td>\n      <td>0.359466</td>\n      <td>0.299185</td>\n      <td>0.359466</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>2.206100</td>\n      <td>3.238790</td>\n      <td>0.257891</td>\n      <td>0.381151</td>\n      <td>0.305701</td>\n      <td>0.381151</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>2.087300</td>\n      <td>3.221984</td>\n      <td>0.254221</td>\n      <td>0.444537</td>\n      <td>0.322651</td>\n      <td>0.444537</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.125900</td>\n      <td>3.232649</td>\n      <td>0.266020</td>\n      <td>0.415763</td>\n      <td>0.318382</td>\n      <td>0.415763</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>2.045900</td>\n      <td>3.223322</td>\n      <td>0.255407</td>\n      <td>0.429108</td>\n      <td>0.320032</td>\n      <td>0.429108</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>1.933200</td>\n      <td>3.207082</td>\n      <td>0.260490</td>\n      <td>0.439533</td>\n      <td>0.323131</td>\n      <td>0.439533</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>1.852700</td>\n      <td>3.203838</td>\n      <td>0.254828</td>\n      <td>0.442035</td>\n      <td>0.322595</td>\n      <td>0.442035</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [38/38 00:56]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"LoRA F1: 0.3226\n\\n==================================================\nPARTIAL FREEZING WITH BEST PARAMETERS\n==================================================\nFreeze Final Training: Effective BS: 8, Actual BS: 4, Grad Acc: 2\n","output_type":"stream"},{"name":"stderr","text":"Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Trainable parameters: 40,230,912 / 125,257,728 (32.12%)\n","output_type":"stream"},{"name":"stderr","text":"/tmp/ipykernel_35/815101731.py:204: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  freeze_trainer = Trainer(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [200/200 12:39, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>4.991100</td>\n      <td>4.989683</td>\n      <td>0.268640</td>\n      <td>0.025021</td>\n      <td>0.039922</td>\n      <td>0.025021</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>4.463500</td>\n      <td>4.721925</td>\n      <td>0.268895</td>\n      <td>0.030859</td>\n      <td>0.048935</td>\n      <td>0.030859</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>4.139300</td>\n      <td>4.514584</td>\n      <td>0.282456</td>\n      <td>0.051710</td>\n      <td>0.082877</td>\n      <td>0.051710</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>3.757300</td>\n      <td>4.355665</td>\n      <td>0.282822</td>\n      <td>0.074229</td>\n      <td>0.113807</td>\n      <td>0.074229</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>3.798700</td>\n      <td>4.235009</td>\n      <td>0.255751</td>\n      <td>0.085488</td>\n      <td>0.124951</td>\n      <td>0.085488</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>3.452300</td>\n      <td>4.145079</td>\n      <td>0.255892</td>\n      <td>0.097164</td>\n      <td>0.137765</td>\n      <td>0.097164</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>3.305100</td>\n      <td>4.075097</td>\n      <td>0.262471</td>\n      <td>0.111343</td>\n      <td>0.153407</td>\n      <td>0.111343</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>3.143700</td>\n      <td>4.031488</td>\n      <td>0.269013</td>\n      <td>0.125938</td>\n      <td>0.168283</td>\n      <td>0.125938</td>\n    </tr>\n    <tr>\n      <td>180</td>\n      <td>3.189200</td>\n      <td>4.005671</td>\n      <td>0.269662</td>\n      <td>0.133028</td>\n      <td>0.175022</td>\n      <td>0.133028</td>\n    </tr>\n    <tr>\n      <td>200</td>\n      <td>3.247800</td>\n      <td>3.997261</td>\n      <td>0.266860</td>\n      <td>0.135113</td>\n      <td>0.176455</td>\n      <td>0.135113</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='75' max='75' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [75/75 00:56]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Partial Freeze F1: 0.1765\n","output_type":"stream"}],"execution_count":20},{"id":"5b71cad9-3f48-4256-814a-9f168d371132","cell_type":"markdown","source":"## 13. Results Analysis and Visualization","metadata":{}},{"id":"b1256832-7755-42aa-9a56-4dcea8fbec67","cell_type":"code","source":"# Create comparison DataFrame\ncomparison_data = []\nfor method, metrics in results_summary.items():\n    comparison_data.append({\n        \"Method\": method.replace(\"_\", \" \").title(),\n        \"F1\": metrics[\"eval_f1\"] * 100,\n        \"Precision\": metrics[\"eval_precision\"] * 100,\n        \"Recall\": metrics[\"eval_recall\"] * 100,\n        \"Accuracy\": metrics[\"eval_accuracy\"] * 100,\n    })\n\ncomparison_df = pd.DataFrame(comparison_data)\ncomparison_df = comparison_df.round(2)\n\nprint(\"\\n\" + \"=\"*50)\nprint(\"GPT-NEO RELATION EXTRACTION RESULTS\")\nprint(\"=\"*50)\nprint(comparison_df.to_string(index=False))\n\n# Visualization\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 6))\n\n# F1 Score comparison\nmethods = comparison_df[\"Method\"].tolist()\nf1_scores = comparison_df[\"F1\"].tolist()\n\nax1.bar(methods, f1_scores, color=['skyblue', 'lightgreen', 'coral', 'gold'])\nax1.set_ylabel('F1 Score (%)')\nax1.set_title('GPT-Neo RE - F1 Score Comparison')\nax1.set_ylim(0, 100)\n\nfor i, v in enumerate(f1_scores):\n    ax1.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n\n# Accuracy comparison\naccuracies = comparison_df[\"Accuracy\"].tolist()\nax2.bar(methods, accuracies, color=['skyblue', 'lightgreen', 'coral', 'gold'])\nax2.set_ylabel('Accuracy (%)')\nax2.set_title('GPT-Neo RE - Accuracy Comparison')\nax2.set_ylim(0, 100)\n\nfor i, v in enumerate(accuracies):\n    ax2.text(i, v + 1, f'{v:.1f}%', ha='center', va='bottom')\n\nplt.tight_layout()\nplt.savefig('outputs/gpt_neo_re_comparison.png', dpi=300, bbox_inches='tight')\nplt.show()\n\n# Best performing method\nbest_method_idx = comparison_df[\"F1\"].idxmax()\nbest_method = comparison_df.iloc[best_method_idx]\nprint(f\"\\n Best performing method: {best_method['Method']}\")\nprint(f\"   F1 Score: {best_method['F1']:.2f}%\")\nprint(f\"   Accuracy: {best_method['Accuracy']:.2f}%\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T12:32:46.478006Z","iopub.execute_input":"2025-05-30T12:32:46.478597Z","iopub.status.idle":"2025-05-30T12:32:47.678807Z","shell.execute_reply.started":"2025-05-30T12:32:46.478574Z","shell.execute_reply":"2025-05-30T12:32:47.678035Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nGPT-NEO RELATION EXTRACTION RESULTS\n==================================================\n        Method    F1  Precision  Recall  Accuracy\n      Baseline 35.28      28.19   48.17     48.17\n       Full Ft 33.38      26.62   49.37     49.37\n          Lora 32.26      25.48   44.20     44.20\nPartial Freeze 17.65      26.69   13.51     13.51\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<Figure size 1500x600 with 2 Axes>","image/png":"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\n"},"metadata":{}},{"name":"stdout","text":"\n Best performing method: Baseline\n   F1 Score: 35.28%\n   Accuracy: 48.17%\n","output_type":"stream"}],"execution_count":21},{"id":"140b4fb2-bb6c-45f8-bd8a-cee8c6bc8e7e","cell_type":"markdown","source":"## 14. Hyperparameter Analysis","metadata":{}},{"id":"dd78e618-ed4e-4072-838d-97169ef447ef","cell_type":"code","source":"print(\"\\n\" + \"=\"*50)\nprint(\"HYPERPARAMETER ANALYSIS\")\nprint(\"=\"*50)\n\nprint(\"\\n Full Fine-Tuning Best Parameters:\")\nfor param, value in study_ft.best_params.items():\n    print(f\"  {param}: {value}\")\n\nprint(f\"\\n LoRA Best Parameters:\")\nfor param, value in study_lora.best_params.items():\n    print(f\"  {param}: {value}\")\n\nprint(f\"\\n Partial Freeze Best Parameters:\")\nfor param, value in study_freeze.best_params.items():\n    print(f\"  {param}: {value}\")\n\n# Save results\ncomparison_df.to_csv(\"outputs/gpt_neo_re_results.csv\", index=False)\nprint(\"\\n Results saved to outputs/gpt_neo_re_results.csv\")\n\n# Save best model\nbest_trainer = ft_trainer if best_method['Method'] == 'Full Ft' else (\n    lora_trainer if best_method['Method'] == 'Lora' else freeze_trainer\n)\nbest_trainer.save_model(\"outputs/gpt-neo-re-best-model\")\ntokenizer.save_pretrained(\"outputs/gpt-neo-re-best-model\")\nprint(\" Best model saved to outputs/gpt-neo-re-best-model/\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T12:36:02.715459Z","iopub.execute_input":"2025-05-30T12:36:02.716149Z","iopub.status.idle":"2025-05-30T12:36:03.858731Z","shell.execute_reply.started":"2025-05-30T12:36:02.716118Z","shell.execute_reply":"2025-05-30T12:36:03.857886Z"}},"outputs":[{"name":"stdout","text":"\n==================================================\nHYPERPARAMETER ANALYSIS\n==================================================\n\n Full Fine-Tuning Best Parameters:\n  learning_rate: 2.467262880195005e-05\n  batch_size: 8\n\n LoRA Best Parameters:\n  learning_rate: 0.00043902157016477775\n  r: 4\n  alpha: 16\n  dropout: 0.08332129356555738\n  batch_size: 16\n\n Partial Freeze Best Parameters:\n  learning_rate: 4.573948969033365e-05\n  batch_size: 8\n  freeze_pct: 0.7019650704348654\n\n Results saved to outputs/gpt_neo_re_results.csv\n Best model saved to outputs/gpt-neo-re-best-model/\n","output_type":"stream"}],"execution_count":24},{"id":"1c2e8c9a-1f37-43e8-8696-5ef58e4e564f","cell_type":"markdown","source":"## 15. Final Summary","metadata":{}},{"id":"fce0dc29-c608-4623-9bc5-06834793bcad","cell_type":"code","source":"# Create summary report\nsummary = {\n    \"task\": \"Relation Extraction\",\n    \"model\": \"GPT-Neo 125M\",\n    \"dataset\": \"DocIE\",\n    \"best_method\": best_method['Method'],\n    \"best_f1\": float(best_method['F1']),\n    \"best_accuracy\": float(best_method['Accuracy']),\n    \"training_examples\": len(train_dataset),\n    \"dev_examples\": len(dev_dataset),\n    \"num_classes\": len(all_labels),\n    \"hyperparameter_trials\": 8,\n    \"fine_tuning_methods\": [\"Full Fine-Tuning\", \"LoRA\", \"Partial Freezing\"]\n}\n\nwith open(\"outputs/gpt_neo_re_summary.json\", \"w\") as f:\n    json.dump(summary, f, indent=2)\n\nprint(\"\\n Summary report saved to outputs/gpt_neo_re_summary.json\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-30T12:36:16.473052Z","iopub.execute_input":"2025-05-30T12:36:16.473362Z","iopub.status.idle":"2025-05-30T12:36:16.479270Z","shell.execute_reply.started":"2025-05-30T12:36:16.473324Z","shell.execute_reply":"2025-05-30T12:36:16.478590Z"}},"outputs":[{"name":"stdout","text":"\n Summary report saved to outputs/gpt_neo_re_summary.json\n","output_type":"stream"}],"execution_count":25}]}