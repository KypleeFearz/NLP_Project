{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a3c54d-f00b-4700-b7de-d1153ba686e9",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "83bdc97a-885e-4c62-8431-a1ad552f56ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:06:11.769606Z",
     "iopub.status.busy": "2025-05-29T16:06:11.768712Z",
     "iopub.status.idle": "2025-05-29T16:06:11.773917Z",
     "shell.execute_reply": "2025-05-29T16:06:11.773363Z",
     "shell.execute_reply.started": "2025-05-29T16:06:11.769573Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset, DatasetDict, Features, Value, Sequence\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e81542-3f8d-4665-9bb5-1ab0922004be",
   "metadata": {},
   "source": [
    "## 2. Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6afc85-7f48-46a2-9b2c-54b5bde59cc1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:04:47.277768Z",
     "iopub.status.busy": "2025-05-29T16:04:47.277151Z",
     "iopub.status.idle": "2025-05-29T16:04:47.286507Z",
     "shell.execute_reply": "2025-05-29T16:04:47.285689Z",
     "shell.execute_reply.started": "2025-05-29T16:04:47.277744Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path(r\"/kaggle/input/nlp-augmentedset/aug_train\")\n",
    "DEV_DIR   = Path(r\"/kaggle/input/nlp-augmentedset/aug_dev\")\n",
    "TEST_DIR  = Path(r\"/kaggle/input/nlp-augmentedset/test\")\n",
    "\n",
    "assert TRAIN_DIR.exists(), f\"Train directory not found: {TRAIN_DIR}\"\n",
    "assert DEV_DIR.exists(), f\"Dev directory not found: {DEV_DIR}\"\n",
    "assert TEST_DIR.exists(), f\"Test directory not found: {TEST_DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50806017-9832-408e-aeb1-4a2ce5d7122e",
   "metadata": {},
   "source": [
    "## 3. Load DocIE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f274a9e-9277-4d0c-898a-6140f86fe09b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:04:50.213697Z",
     "iopub.status.busy": "2025-05-29T16:04:50.213186Z",
     "iopub.status.idle": "2025-05-29T16:04:51.595564Z",
     "shell.execute_reply": "2025-05-29T16:04:51.594679Z",
     "shell.execute_reply.started": "2025-05-29T16:04:50.213653Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents - Train: 204, Dev: 88, Test: 248\n"
     ]
    }
   ],
   "source": [
    "def load_docie_docs(folder: Path, recursive: bool = False):\n",
    "    \"\"\"Load JSON documents from DocIE dataset.\"\"\"\n",
    "    docs = []\n",
    "    pattern = \"**/*.json\" if recursive else \"*.json\"\n",
    "    for file in folder.glob(pattern):\n",
    "        data = json.loads(file.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(data, list):\n",
    "            docs.extend(data)\n",
    "        else:\n",
    "            docs.append(data)\n",
    "    return docs\n",
    "\n",
    "train_docs = load_docie_docs(TRAIN_DIR)\n",
    "dev_docs = load_docie_docs(DEV_DIR)\n",
    "test_docs = load_docie_docs(TEST_DIR, recursive=True)\n",
    "\n",
    "print(f\"Loaded documents - Train: {len(train_docs)}, Dev: {len(dev_docs)}, Test: {len(test_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70317df-dc73-4ef2-8957-9c969e64a720",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "791904ac-c9cc-4c14-bb54-b5bb699a72a2",
   "metadata": {
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document statistics - Avg tokens: 839.2, Max tokens: 2571\n",
      "\n",
      "Top 10 entity types:\n",
      "  DATE: 2588\n",
      "  MISC: 1668\n",
      "  PERSON: 968\n",
      "  ORG: 964\n",
      "  CARDINAL: 896\n",
      "  GPE: 628\n",
      "  WORK_OF_ART: 260\n",
      "  NORP: 236\n",
      "  ORDINAL: 220\n",
      "  QUANTITY: 168\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Document lengths\n",
    "lengths = [len(doc[\"doc\"].split()) for doc in train_docs]\n",
    "print(f\"Document statistics - Avg tokens: {np.mean(lengths):.1f}, Max tokens: {np.max(lengths)}\")\n",
    "\n",
    "# 4.2 Entity distribution\n",
    "entity_counter = Counter(ent[\"type\"] for doc in train_docs for ent in doc[\"entities\"])\n",
    "print(f\"\\nTop 10 entity types:\")\n",
    "for entity_type, count in entity_counter.most_common(10):\n",
    "    print(f\"  {entity_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3eb57-59b6-4eb2-ad9c-def56db4d99d",
   "metadata": {},
   "source": [
    "## 5. Setup Label Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "936ca5a2-80de-4e10-b5af-04a54b3ba0be",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:06:20.261344Z",
     "iopub.status.busy": "2025-05-29T16:06:20.260619Z",
     "iopub.status.idle": "2025-05-29T16:06:20.266172Z",
     "shell.execute_reply": "2025-05-29T16:06:20.265492Z",
     "shell.execute_reply.started": "2025-05-29T16:06:20.261292Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NER labels: 39\n"
     ]
    }
   ],
   "source": [
    "# Get all entity types from train set\n",
    "entity_types = train_docs[0][\"entity_label_set\"]\n",
    "\n",
    "# Create BIO tags\n",
    "ner_labels = [\"O\"]\n",
    "for entity_type in entity_types:\n",
    "    ner_labels.extend([f\"B-{entity_type}\", f\"I-{entity_type}\"])\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(ner_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Total NER labels: {len(ner_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120cdce5-9063-4233-ae38-0bd132d0e88b",
   "metadata": {},
   "source": [
    "## 6. Initialize GPT-Neo Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6cda9680-f415-4d3c-9004-7d1c7a3893ee",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:06:23.993052Z",
     "iopub.status.busy": "2025-05-29T16:06:23.992460Z",
     "iopub.status.idle": "2025-05-29T16:06:25.648188Z",
     "shell.execute_reply": "2025-05-29T16:06:25.647422Z",
     "shell.execute_reply.started": "2025-05-29T16:06:23.993031Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0bbd8f2dbbab4ac8a21737d8fe2543b9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/727 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4d2a725f1416473887c2db3ac8c2ae80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bfddec447d2c4cdeac8ea7fd380fb7d1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e2749bdd1414a9cb10f835c9b6d8b5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/2.11M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "678df0c306734603888398046b7cdaff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/357 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50258\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# CRITICAL: Add padding token for GPT-Neo\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e072468-b787-4673-bc0e-6d9e894161a2",
   "metadata": {},
   "source": [
    "## 7. Tokenization with Label Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "384fe02b-9f89-4380-9b19-798f6b45ea1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:06:33.494429Z",
     "iopub.status.busy": "2025-05-29T16:06:33.493764Z",
     "iopub.status.idle": "2025-05-29T16:06:33.501267Z",
     "shell.execute_reply": "2025-05-29T16:06:33.500486Z",
     "shell.execute_reply.started": "2025-05-29T16:06:33.494408Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "stride = 128\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"Tokenize documents and align NER labels with subword tokens.\"\"\"\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for doc, entities in zip(examples[\"doc\"], examples[\"entities\"]):\n",
    "        # Tokenize with overflow handling\n",
    "        tokenized = tokenizer(\n",
    "            doc,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            return_overflowing_tokens=True,\n",
    "        )\n",
    "        \n",
    "        # Process each chunk\n",
    "        for i in range(len(tokenized[\"input_ids\"])):\n",
    "            offsets = tokenized[\"offset_mapping\"][i]\n",
    "            input_ids = tokenized[\"input_ids\"][i]\n",
    "            attention_mask = tokenized[\"attention_mask\"][i]\n",
    "            \n",
    "            # Initialize with \"O\" labels\n",
    "            chunk_labels = [\"O\"] * len(offsets)\n",
    "            \n",
    "            # Map entity mentions to token labels\n",
    "            for entity in entities:\n",
    "                entity_type = entity[\"type\"]\n",
    "                for mention in entity[\"mentions\"]:\n",
    "                    start = doc.find(mention)\n",
    "                    if start < 0:\n",
    "                        continue\n",
    "                    end = start + len(mention)\n",
    "                    \n",
    "                    # Label tokens that overlap with entity mention\n",
    "                    for idx, (token_start, token_end) in enumerate(offsets):\n",
    "                        if token_start >= start and token_end <= end:\n",
    "                            prefix = \"B\" if token_start == start else \"I\"\n",
    "                            chunk_labels[idx] = f\"{prefix}-{entity_type}\"\n",
    "            \n",
    "            # Convert labels to IDs\n",
    "            label_ids = [label2id.get(label, label2id[\"O\"]) for label in chunk_labels]\n",
    "            \n",
    "            all_input_ids.append(input_ids)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "            all_labels.append(label_ids)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_mask,\n",
    "        \"labels\": all_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e88248-8a37-4b3c-a604-75d75ffa5832",
   "metadata": {},
   "source": [
    "## 8. Create Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82a92c85-bc94-4225-8b9e-18087336bc83",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:12:26.363086Z",
     "iopub.status.busy": "2025-05-29T16:12:26.362352Z",
     "iopub.status.idle": "2025-05-29T16:12:29.850210Z",
     "shell.execute_reply": "2025-05-29T16:12:29.849643Z",
     "shell.execute_reply.started": "2025-05-29T16:12:26.363062Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a8036f786aa4e458d3fd35979fc6468",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/204 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99731bab04c548ee93eebe232f6401ba",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/88 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized datasets - Train: 627, Dev: 272\n"
     ]
    }
   ],
   "source": [
    "# Convert to HF Dataset\n",
    "hf_train = Dataset.from_list(train_docs)\n",
    "hf_dev = Dataset.from_list(dev_docs)\n",
    "\n",
    "# Define output features for the .map() function\n",
    "\n",
    "output_features_for_map = Features({\n",
    "    \"input_ids\": Sequence(Value(\"int32\")),\n",
    "    \"attention_mask\": Sequence(Value(\"int8\")),\n",
    "    \"labels\": Sequence(Value(\"int64\")),\n",
    "})\n",
    "\n",
    "# Apply tokenization\n",
    "# The .map() function will automatically remove columns that are not part of the new features\n",
    "# if 'remove_columns' is not set, OR it will remove the specified ones AFTER processing.\n",
    "\n",
    "# Columns that tokenize_and_align_labels consumes from the input\n",
    "input_map_columns = [\"doc\", \"entities\"]\n",
    "# Columns to remove AFTER the map operation (original columns that are not in output_features_for_map)\n",
    "columns_to_remove_after_map = [\n",
    "    col for col in hf_train.column_names if col not in output_features_for_map\n",
    "]\n",
    "# Ensure 'doc' and 'entities' are removed as they are processed into new features\n",
    "if 'doc' not in columns_to_remove_after_map:\n",
    "    columns_to_remove_after_map.append('doc')\n",
    "if 'entities' not in columns_to_remove_after_map:\n",
    "    columns_to_remove_after_map.append('entities')\n",
    "columns_to_remove_after_map = list(set(columns_to_remove_after_map))\n",
    "\n",
    "\n",
    "hf_train_tokenized = hf_train.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=columns_to_remove_after_map,\n",
    "    features=output_features_for_map # Define the output structure\n",
    ")\n",
    "\n",
    "hf_dev_tokenized = hf_dev.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    batch_size=100,\n",
    "    remove_columns=columns_to_remove_after_map, # Adjust for dev if columns differ, usually same\n",
    "    features=output_features_for_map # Define the output structure\n",
    ")\n",
    "\n",
    "hf_train = hf_train_tokenized\n",
    "hf_dev = hf_dev_tokenized\n",
    "\n",
    "print(f\"Tokenized datasets - Train: {len(hf_train)}, Dev: {len(hf_dev)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b07321-97c9-4b3d-a800-a505d5560de7",
   "metadata": {},
   "source": [
    "## 9. Setup Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "4ba3de4c-fc8a-4ed5-bbf2-2fe72e72e42c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:12:38.759443Z",
     "iopub.status.busy": "2025-05-29T16:12:38.758824Z",
     "iopub.status.idle": "2025-05-29T16:12:38.764017Z",
     "shell.execute_reply": "2025-05-29T16:12:38.763475Z",
     "shell.execute_reply.started": "2025-05-29T16:12:38.759423Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics_entity_only(pred):\n",
    "    \"\"\"Compute metrics only on entity tokens (non-O labels).\"\"\"\n",
    "    preds = pred.predictions.argmax(-1).flatten()\n",
    "    labels = pred.label_ids.flatten()\n",
    "    \n",
    "    # Filter out non-entity labels and padding\n",
    "    mask = (labels != label2id[\"O\"]) & (labels != -100)\n",
    "    \n",
    "    if mask.sum() == 0:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels[mask], preds[mask], average=\"micro\"\n",
    "    )\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51522042-b57e-4080-9088-3ae5b394b173",
   "metadata": {},
   "source": [
    "## 10. Baseline: Full Fine-Tuning (3 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b519c69-8508-4c22-8885-e08dbe2c2b54",
   "metadata": {},
   "source": [
    "## 10.1 Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9e24584d-aab4-4432-99ed-1b36024ccb81",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:13:36.489751Z",
     "iopub.status.busy": "2025-05-29T16:13:36.489469Z",
     "iopub.status.idle": "2025-05-29T16:15:48.582467Z",
     "shell.execute_reply": "2025-05-29T16:15:48.581692Z",
     "shell.execute_reply.started": "2025-05-29T16:13:36.489733Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/758557065.py:30: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='60' max='60' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [60/60 01:59, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.457558</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.400045</td>\n",
       "      <td>0.057704</td>\n",
       "      <td>0.057704</td>\n",
       "      <td>0.057704</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.399712</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>0.004885</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Dev F1: 0.0577\n"
     ]
    }
   ],
   "source": [
    "def train_baseline():\n",
    "    \"\"\"Train GPT-Neo baseline for 3 epochs.\"\"\"\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    \n",
    "    # Resize embeddings to accommodate padding token\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"outputs/gpt-neo-ner-baseline\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        eval_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=3e-3,\n",
    "        weight_decay=0.0,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    \n",
    "    return trainer, metrics\n",
    "\n",
    "baseline_trainer, baseline_metrics = train_baseline()\n",
    "print(f\"Baseline Dev F1: {baseline_metrics['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a3a98-9e05-41d6-a790-e994286a7eee",
   "metadata": {},
   "source": [
    "## 11. Hyperparameter Tuning with 100 Steps Budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd6d1e-69cd-4f14-825a-56f8cc7f0d42",
   "metadata": {},
   "source": [
    "## 11.1 Full Fine-Tuning Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d250e230-23b0-496c-b54b-dd2170062098",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:16:29.561205Z",
     "iopub.status.busy": "2025-05-29T16:16:29.560918Z",
     "iopub.status.idle": "2025-05-29T16:39:01.014975Z",
     "shell.execute_reply": "2025-05-29T16:39:01.014344Z",
     "shell.execute_reply.started": "2025-05-29T16:16:29.561184Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3624539862.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.658414</td>\n",
       "      <td>0.025443</td>\n",
       "      <td>0.025443</td>\n",
       "      <td>0.025443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.523800</td>\n",
       "      <td>0.444792</td>\n",
       "      <td>0.058518</td>\n",
       "      <td>0.058518</td>\n",
       "      <td>0.058518</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.523800</td>\n",
       "      <td>0.384895</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>0.020558</td>\n",
       "      <td>0.020558</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.362635</td>\n",
       "      <td>0.024120</td>\n",
       "      <td>0.024120</td>\n",
       "      <td>0.024120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.573500</td>\n",
       "      <td>0.355448</td>\n",
       "      <td>0.033279</td>\n",
       "      <td>0.033279</td>\n",
       "      <td>0.033279</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:20:09,552] Trial 0 finished with value: 0.033279055566863426 and parameters: {'learning_rate': 1.5596622305412592e-05, 'batch_size': 16}. Best is trial 0 with value: 0.033279055566863426.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3624539862.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.460811</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.005699</td>\n",
       "      <td>0.005699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.063700</td>\n",
       "      <td>0.355860</td>\n",
       "      <td>0.053023</td>\n",
       "      <td>0.053023</td>\n",
       "      <td>0.053023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.063700</td>\n",
       "      <td>0.316568</td>\n",
       "      <td>0.116222</td>\n",
       "      <td>0.116222</td>\n",
       "      <td>0.116222</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>0.302295</td>\n",
       "      <td>0.156015</td>\n",
       "      <td>0.156015</td>\n",
       "      <td>0.156015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.434200</td>\n",
       "      <td>0.299728</td>\n",
       "      <td>0.175453</td>\n",
       "      <td>0.175453</td>\n",
       "      <td>0.175453</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:23:50,702] Trial 1 finished with value: 0.1754528801139833 and parameters: {'learning_rate': 3.200739245599043e-05, 'batch_size': 16}. Best is trial 1 with value: 0.1754528801139833.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3624539862.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:28, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.528760</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002341</td>\n",
       "      <td>0.002341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.183600</td>\n",
       "      <td>0.401629</td>\n",
       "      <td>0.016080</td>\n",
       "      <td>0.016080</td>\n",
       "      <td>0.016080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.183600</td>\n",
       "      <td>0.350814</td>\n",
       "      <td>0.061266</td>\n",
       "      <td>0.061266</td>\n",
       "      <td>0.061266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.533200</td>\n",
       "      <td>0.337007</td>\n",
       "      <td>0.085589</td>\n",
       "      <td>0.085589</td>\n",
       "      <td>0.085589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.533200</td>\n",
       "      <td>0.335211</td>\n",
       "      <td>0.080094</td>\n",
       "      <td>0.080094</td>\n",
       "      <td>0.080094</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:25:27,664] Trial 2 finished with value: 0.08009362914716059 and parameters: {'learning_rate': 2.6994081548659585e-05, 'batch_size': 4}. Best is trial 1 with value: 0.1754528801139833.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3624539862.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:08, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.458752</td>\n",
       "      <td>0.023611</td>\n",
       "      <td>0.023611</td>\n",
       "      <td>0.023611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.123000</td>\n",
       "      <td>0.354063</td>\n",
       "      <td>0.034907</td>\n",
       "      <td>0.034907</td>\n",
       "      <td>0.034907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.123000</td>\n",
       "      <td>0.321281</td>\n",
       "      <td>0.098412</td>\n",
       "      <td>0.098412</td>\n",
       "      <td>0.098412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.417800</td>\n",
       "      <td>0.304699</td>\n",
       "      <td>0.143395</td>\n",
       "      <td>0.143395</td>\n",
       "      <td>0.143395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.417800</td>\n",
       "      <td>0.305521</td>\n",
       "      <td>0.134541</td>\n",
       "      <td>0.134541</td>\n",
       "      <td>0.134541</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:27:44,679] Trial 3 finished with value: 0.13454101363728882 and parameters: {'learning_rate': 3.813637473006929e-05, 'batch_size': 8}. Best is trial 1 with value: 0.1754528801139833.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3624539862.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.421473</td>\n",
       "      <td>0.024323</td>\n",
       "      <td>0.024323</td>\n",
       "      <td>0.024323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.014500</td>\n",
       "      <td>0.332058</td>\n",
       "      <td>0.097191</td>\n",
       "      <td>0.097191</td>\n",
       "      <td>0.097191</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.014500</td>\n",
       "      <td>0.305778</td>\n",
       "      <td>0.130979</td>\n",
       "      <td>0.130979</td>\n",
       "      <td>0.130979</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.297594</td>\n",
       "      <td>0.157338</td>\n",
       "      <td>0.157338</td>\n",
       "      <td>0.157338</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.397800</td>\n",
       "      <td>0.294736</td>\n",
       "      <td>0.180236</td>\n",
       "      <td>0.180236</td>\n",
       "      <td>0.180236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:31:25,368] Trial 4 finished with value: 0.1802361082841441 and parameters: {'learning_rate': 3.994900876547036e-05, 'batch_size': 16}. Best is trial 4 with value: 0.1802361082841441.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3624539862.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:31, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.496029</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.003155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.122400</td>\n",
       "      <td>0.368362</td>\n",
       "      <td>0.040912</td>\n",
       "      <td>0.040912</td>\n",
       "      <td>0.040912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.122400</td>\n",
       "      <td>0.325207</td>\n",
       "      <td>0.096784</td>\n",
       "      <td>0.096784</td>\n",
       "      <td>0.096784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.309977</td>\n",
       "      <td>0.132811</td>\n",
       "      <td>0.132811</td>\n",
       "      <td>0.132811</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.452600</td>\n",
       "      <td>0.306290</td>\n",
       "      <td>0.153063</td>\n",
       "      <td>0.153063</td>\n",
       "      <td>0.153063</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:35:06,452] Trial 5 finished with value: 0.15306330144514554 and parameters: {'learning_rate': 2.805721423233097e-05, 'batch_size': 16}. Best is trial 4 with value: 0.1802361082841441.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3624539862.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:08, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.867661</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.003867</td>\n",
       "      <td>0.003867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.681900</td>\n",
       "      <td>0.525399</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.003155</td>\n",
       "      <td>0.003155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.681900</td>\n",
       "      <td>0.447331</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.010279</td>\n",
       "      <td>0.010279</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.660700</td>\n",
       "      <td>0.413107</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.015469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.660700</td>\n",
       "      <td>0.404533</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.018828</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:37:24,161] Trial 6 finished with value: 0.01882760024424995 and parameters: {'learning_rate': 1.273275420070486e-05, 'batch_size': 8}. Best is trial 4 with value: 0.1802361082841441.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3624539862.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:28, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.976506</td>\n",
       "      <td>0.005496</td>\n",
       "      <td>0.005496</td>\n",
       "      <td>0.005496</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.848200</td>\n",
       "      <td>0.550722</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>0.004885</td>\n",
       "      <td>0.004885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.848200</td>\n",
       "      <td>0.451409</td>\n",
       "      <td>0.013332</td>\n",
       "      <td>0.013332</td>\n",
       "      <td>0.013332</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.422380</td>\n",
       "      <td>0.013027</td>\n",
       "      <td>0.013027</td>\n",
       "      <td>0.013027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.715000</td>\n",
       "      <td>0.415784</td>\n",
       "      <td>0.013943</td>\n",
       "      <td>0.013943</td>\n",
       "      <td>0.013943</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:39:01,011] Trial 7 finished with value: 0.01394260126195807 and parameters: {'learning_rate': 1.3076326876907804e-05, 'batch_size': 4}. Best is trial 4 with value: 0.1802361082841441.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Full-FT params: {'learning_rate': 3.994900876547036e-05, 'batch_size': 16}\n",
      "Best Full-FT Dev F1: 0.1802\n"
     ]
    }
   ],
   "source": [
    "def ft_objective(trial):\n",
    "    \"\"\"Optuna objective for full fine-tuning.\"\"\"\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/gpt-neo-ft-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs * 2,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=40,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "study_ft = optuna.create_study(direction=\"maximize\")\n",
    "study_ft.optimize(ft_objective, n_trials=8)\n",
    "\n",
    "print(f\"Best Full-FT params: {study_ft.best_params}\")\n",
    "print(f\"Best Full-FT Dev F1: {study_ft.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5564f-cc6d-4852-baae-4dddbf451490",
   "metadata": {},
   "source": [
    "## 11.2 LoRA Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0f7a9300-b710-455a-9399-d8084d60d1a1",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:39:01.016271Z",
     "iopub.status.busy": "2025-05-29T16:39:01.016095Z",
     "iopub.status.idle": "2025-05-29T16:59:04.503101Z",
     "shell.execute_reply": "2025-05-29T16:59:04.502489Z",
     "shell.execute_reply.started": "2025-05-29T16:39:01.016258Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:39:01,020] A new study created in memory with name: no-name-798b3149-a545-4ce4-828e-aa799c05534e\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3342631922.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.570324</td>\n",
       "      <td>0.015266</td>\n",
       "      <td>0.015266</td>\n",
       "      <td>0.015266</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.323300</td>\n",
       "      <td>5.318190</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.017505</td>\n",
       "      <td>0.017505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.323300</td>\n",
       "      <td>5.129070</td>\n",
       "      <td>0.020354</td>\n",
       "      <td>0.020354</td>\n",
       "      <td>0.020354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.867400</td>\n",
       "      <td>5.012000</td>\n",
       "      <td>0.022695</td>\n",
       "      <td>0.022695</td>\n",
       "      <td>0.022695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.867400</td>\n",
       "      <td>4.971854</td>\n",
       "      <td>0.023407</td>\n",
       "      <td>0.023407</td>\n",
       "      <td>0.023407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:42:04,396] Trial 0 finished with value: 0.023407286790148585 and parameters: {'learning_rate': 1.8859282896255805e-05, 'r': 16, 'alpha': 16, 'dropout': 0.1599934449030607, 'batch_size': 16}. Best is trial 0 with value: 0.023407286790148585.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3342631922.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:47, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.382599</td>\n",
       "      <td>0.086302</td>\n",
       "      <td>0.086302</td>\n",
       "      <td>0.086302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>0.302471</td>\n",
       "      <td>0.129656</td>\n",
       "      <td>0.129656</td>\n",
       "      <td>0.129656</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.910500</td>\n",
       "      <td>0.281222</td>\n",
       "      <td>0.153674</td>\n",
       "      <td>0.153674</td>\n",
       "      <td>0.153674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.267198</td>\n",
       "      <td>0.168024</td>\n",
       "      <td>0.168024</td>\n",
       "      <td>0.168024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.360200</td>\n",
       "      <td>0.264256</td>\n",
       "      <td>0.198962</td>\n",
       "      <td>0.198962</td>\n",
       "      <td>0.198962</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:44:01,006] Trial 1 finished with value: 0.19896193771626297 and parameters: {'learning_rate': 0.0006864436426020529, 'r': 16, 'alpha': 32, 'dropout': 0.13102133790278095, 'batch_size': 8}. Best is trial 1 with value: 0.19896193771626297.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3342631922.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.107285</td>\n",
       "      <td>0.026359</td>\n",
       "      <td>0.026359</td>\n",
       "      <td>0.026359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.501400</td>\n",
       "      <td>0.644550</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.019744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.501400</td>\n",
       "      <td>0.465420</td>\n",
       "      <td>0.031752</td>\n",
       "      <td>0.031752</td>\n",
       "      <td>0.031752</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.422331</td>\n",
       "      <td>0.049155</td>\n",
       "      <td>0.049155</td>\n",
       "      <td>0.049155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.689500</td>\n",
       "      <td>0.410419</td>\n",
       "      <td>0.047731</td>\n",
       "      <td>0.047731</td>\n",
       "      <td>0.047731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:47:04,735] Trial 2 finished with value: 0.0477305108894769 and parameters: {'learning_rate': 0.00012983952880904096, 'r': 16, 'alpha': 32, 'dropout': 0.07319761422656419, 'batch_size': 16}. Best is trial 1 with value: 0.19896193771626297.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3342631922.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:16, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.709233</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.015469</td>\n",
       "      <td>0.015469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.592100</td>\n",
       "      <td>0.432424</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.042133</td>\n",
       "      <td>0.042133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.592100</td>\n",
       "      <td>0.376216</td>\n",
       "      <td>0.054651</td>\n",
       "      <td>0.054651</td>\n",
       "      <td>0.054651</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.561200</td>\n",
       "      <td>0.354376</td>\n",
       "      <td>0.068899</td>\n",
       "      <td>0.068899</td>\n",
       "      <td>0.068899</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.561200</td>\n",
       "      <td>0.350253</td>\n",
       "      <td>0.065337</td>\n",
       "      <td>0.065337</td>\n",
       "      <td>0.065337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:48:30,035] Trial 3 finished with value: 0.06533686138815388 and parameters: {'learning_rate': 0.00024850857514170053, 'r': 4, 'alpha': 32, 'dropout': 0.09113061629333172, 'batch_size': 4}. Best is trial 1 with value: 0.19896193771626297.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3342631922.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:16, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.096762</td>\n",
       "      <td>0.021372</td>\n",
       "      <td>0.021372</td>\n",
       "      <td>0.021372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.005200</td>\n",
       "      <td>0.473517</td>\n",
       "      <td>0.035620</td>\n",
       "      <td>0.035620</td>\n",
       "      <td>0.035620</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.005200</td>\n",
       "      <td>0.413774</td>\n",
       "      <td>0.043354</td>\n",
       "      <td>0.043354</td>\n",
       "      <td>0.043354</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.569100</td>\n",
       "      <td>0.390699</td>\n",
       "      <td>0.070832</td>\n",
       "      <td>0.070832</td>\n",
       "      <td>0.070832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.569100</td>\n",
       "      <td>0.381135</td>\n",
       "      <td>0.060452</td>\n",
       "      <td>0.060452</td>\n",
       "      <td>0.060452</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:49:55,321] Trial 4 finished with value: 0.060451862405862 and parameters: {'learning_rate': 0.0003046668762171177, 'r': 16, 'alpha': 16, 'dropout': 0.04972267741305342, 'batch_size': 4}. Best is trial 1 with value: 0.19896193771626297.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3342631922.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.300737</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.017708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.058400</td>\n",
       "      <td>4.820600</td>\n",
       "      <td>0.026359</td>\n",
       "      <td>0.026359</td>\n",
       "      <td>0.026359</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.058400</td>\n",
       "      <td>4.449072</td>\n",
       "      <td>0.032363</td>\n",
       "      <td>0.032363</td>\n",
       "      <td>0.032363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.188400</td>\n",
       "      <td>4.214063</td>\n",
       "      <td>0.036841</td>\n",
       "      <td>0.036841</td>\n",
       "      <td>0.036841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.188400</td>\n",
       "      <td>4.132644</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.038062</td>\n",
       "      <td>0.038062</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:52:58,057] Trial 5 finished with value: 0.03806228373702422 and parameters: {'learning_rate': 3.491787560798341e-05, 'r': 8, 'alpha': 16, 'dropout': 0.202989670831165, 'batch_size': 16}. Best is trial 1 with value: 0.19896193771626297.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3342631922.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:53, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.952615</td>\n",
       "      <td>0.018013</td>\n",
       "      <td>0.018013</td>\n",
       "      <td>0.018013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.890100</td>\n",
       "      <td>3.402635</td>\n",
       "      <td>0.023814</td>\n",
       "      <td>0.023814</td>\n",
       "      <td>0.023814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.890100</td>\n",
       "      <td>2.986992</td>\n",
       "      <td>0.025850</td>\n",
       "      <td>0.025850</td>\n",
       "      <td>0.025850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.925800</td>\n",
       "      <td>2.731315</td>\n",
       "      <td>0.025341</td>\n",
       "      <td>0.025341</td>\n",
       "      <td>0.025341</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.925800</td>\n",
       "      <td>2.644315</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>0.024934</td>\n",
       "      <td>0.024934</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:56:00,993] Trial 6 finished with value: 0.024933848972114796 and parameters: {'learning_rate': 4.0562002433105186e-05, 'r': 8, 'alpha': 16, 'dropout': 0.19795698792925626, 'batch_size': 16}. Best is trial 1 with value: 0.19896193771626297.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/3342631922.py:42: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:54, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.509303</td>\n",
       "      <td>0.030531</td>\n",
       "      <td>0.030531</td>\n",
       "      <td>0.030531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.268100</td>\n",
       "      <td>0.357551</td>\n",
       "      <td>0.078160</td>\n",
       "      <td>0.078160</td>\n",
       "      <td>0.078160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.268100</td>\n",
       "      <td>0.314594</td>\n",
       "      <td>0.098412</td>\n",
       "      <td>0.098412</td>\n",
       "      <td>0.098412</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.297823</td>\n",
       "      <td>0.127621</td>\n",
       "      <td>0.127621</td>\n",
       "      <td>0.127621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.424200</td>\n",
       "      <td>0.294599</td>\n",
       "      <td>0.136373</td>\n",
       "      <td>0.136373</td>\n",
       "      <td>0.136373</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:59:04,499] Trial 7 finished with value: 0.13637288825564828 and parameters: {'learning_rate': 0.0004178040450177898, 'r': 16, 'alpha': 16, 'dropout': 0.15023311250292662, 'batch_size': 16}. Best is trial 1 with value: 0.19896193771626297.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LoRA params: {'learning_rate': 0.0006864436426020529, 'r': 16, 'alpha': 32, 'dropout': 0.13102133790278095, 'batch_size': 8}\n",
      "Best LoRA Dev F1: 0.1990\n"
     ]
    }
   ],
   "source": [
    "def lora_objective(trial):\n",
    "    \"\"\"Optuna objective for LoRA fine-tuning.\"\"\"\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    r = trial.suggest_categorical(\"r\", [4, 8, 16])\n",
    "    alpha = trial.suggest_categorical(\"alpha\", [16, 32])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"TOKEN_CLS\",\n",
    "        inference_mode=False,\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=dropout,\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/gpt-neo-lora-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs * 2,\n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=40,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "study_lora = optuna.create_study(direction=\"maximize\")\n",
    "study_lora.optimize(lora_objective, n_trials=8)\n",
    "\n",
    "print(f\"Best LoRA params: {study_lora.best_params}\")\n",
    "print(f\"Best LoRA Dev F1: {study_lora.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79447e-efcd-416f-8f7e-8cbb513809ee",
   "metadata": {},
   "source": [
    "## 11.3 Partial Freezing Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "54ea7040-3285-454a-9a1e-718a58dff91d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T16:59:04.511106Z",
     "iopub.status.busy": "2025-05-29T16:59:04.510763Z",
     "iopub.status.idle": "2025-05-29T17:16:53.229634Z",
     "shell.execute_reply": "2025-05-29T17:16:53.229067Z",
     "shell.execute_reply.started": "2025-05-29T16:59:04.511080Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 16:59:04,515] A new study created in memory with name: no-name-327b0011-996b-4004-9d16-5e4bfe956512\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/2601435201.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.942537</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.016589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.813800</td>\n",
       "      <td>3.429366</td>\n",
       "      <td>0.019947</td>\n",
       "      <td>0.019947</td>\n",
       "      <td>0.019947</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.813800</td>\n",
       "      <td>3.068757</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>0.022491</td>\n",
       "      <td>0.022491</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.812900</td>\n",
       "      <td>2.857040</td>\n",
       "      <td>0.023611</td>\n",
       "      <td>0.023611</td>\n",
       "      <td>0.023611</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.812900</td>\n",
       "      <td>2.786121</td>\n",
       "      <td>0.024120</td>\n",
       "      <td>0.024120</td>\n",
       "      <td>0.024120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 17:02:02,239] Trial 0 finished with value: 0.024119682475066146 and parameters: {'learning_rate': 3.2799657790412474e-05, 'batch_size': 16, 'freeze_pct': 0.5823635847865559}. Best is trial 0 with value: 0.024119682475066146.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/2601435201.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.192076</td>\n",
       "      <td>0.015062</td>\n",
       "      <td>0.015062</td>\n",
       "      <td>0.015062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.075700</td>\n",
       "      <td>3.859626</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.016283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.075700</td>\n",
       "      <td>3.622548</td>\n",
       "      <td>0.018929</td>\n",
       "      <td>0.018929</td>\n",
       "      <td>0.018929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.413400</td>\n",
       "      <td>3.481192</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.019744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.413400</td>\n",
       "      <td>3.433424</td>\n",
       "      <td>0.019947</td>\n",
       "      <td>0.019947</td>\n",
       "      <td>0.019947</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 17:04:59,832] Trial 1 finished with value: 0.019947079177691836 and parameters: {'learning_rate': 2.0836059096300748e-05, 'batch_size': 16, 'freeze_pct': 0.7347109035053484}. Best is trial 0 with value: 0.024119682475066146.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/2601435201.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:44, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.245159</td>\n",
       "      <td>0.015062</td>\n",
       "      <td>0.015062</td>\n",
       "      <td>0.015062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.174000</td>\n",
       "      <td>3.947846</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.016589</td>\n",
       "      <td>0.016589</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.174000</td>\n",
       "      <td>3.744111</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.018115</td>\n",
       "      <td>0.018115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.571000</td>\n",
       "      <td>3.620102</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.018828</td>\n",
       "      <td>0.018828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.571000</td>\n",
       "      <td>3.577064</td>\n",
       "      <td>0.019031</td>\n",
       "      <td>0.019031</td>\n",
       "      <td>0.019031</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 17:06:52,949] Trial 2 finished with value: 0.01903114186851211 and parameters: {'learning_rate': 1.9911060209753957e-05, 'batch_size': 8, 'freeze_pct': 0.39967121626549673}. Best is trial 0 with value: 0.024119682475066146.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/2601435201.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:44, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.315043</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.018421</td>\n",
       "      <td>0.018421</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.415400</td>\n",
       "      <td>3.922961</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>0.019845</td>\n",
       "      <td>0.019845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.415400</td>\n",
       "      <td>3.653962</td>\n",
       "      <td>0.021372</td>\n",
       "      <td>0.021372</td>\n",
       "      <td>0.021372</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.641400</td>\n",
       "      <td>3.490267</td>\n",
       "      <td>0.022084</td>\n",
       "      <td>0.022084</td>\n",
       "      <td>0.022084</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.641400</td>\n",
       "      <td>3.433566</td>\n",
       "      <td>0.021982</td>\n",
       "      <td>0.021982</td>\n",
       "      <td>0.021982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 17:08:45,895] Trial 3 finished with value: 0.021982495420313455 and parameters: {'learning_rate': 2.5963730440346943e-05, 'batch_size': 8, 'freeze_pct': 0.40316731279059026}. Best is trial 0 with value: 0.024119682475066146.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/2601435201.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:44, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.593669</td>\n",
       "      <td>0.016996</td>\n",
       "      <td>0.016996</td>\n",
       "      <td>0.016996</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.705000</td>\n",
       "      <td>4.412306</td>\n",
       "      <td>0.017912</td>\n",
       "      <td>0.017912</td>\n",
       "      <td>0.017912</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.705000</td>\n",
       "      <td>4.287029</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.018319</td>\n",
       "      <td>0.018319</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.344400</td>\n",
       "      <td>4.210409</td>\n",
       "      <td>0.018726</td>\n",
       "      <td>0.018726</td>\n",
       "      <td>0.018726</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.344400</td>\n",
       "      <td>4.183796</td>\n",
       "      <td>0.018624</td>\n",
       "      <td>0.018624</td>\n",
       "      <td>0.018624</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 17:10:39,010] Trial 4 finished with value: 0.01862405861998779 and parameters: {'learning_rate': 1.1824810165667613e-05, 'batch_size': 8, 'freeze_pct': 0.2708083573380225}. Best is trial 0 with value: 0.024119682475066146.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/2601435201.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:48, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.271820</td>\n",
       "      <td>0.018013</td>\n",
       "      <td>0.018013</td>\n",
       "      <td>0.018013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.350200</td>\n",
       "      <td>3.853682</td>\n",
       "      <td>0.020049</td>\n",
       "      <td>0.020049</td>\n",
       "      <td>0.020049</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.350200</td>\n",
       "      <td>3.555363</td>\n",
       "      <td>0.021270</td>\n",
       "      <td>0.021270</td>\n",
       "      <td>0.021270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.508600</td>\n",
       "      <td>3.376915</td>\n",
       "      <td>0.021575</td>\n",
       "      <td>0.021575</td>\n",
       "      <td>0.021575</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.508600</td>\n",
       "      <td>3.316618</td>\n",
       "      <td>0.021982</td>\n",
       "      <td>0.021982</td>\n",
       "      <td>0.021982</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 17:13:36,807] Trial 5 finished with value: 0.021982495420313455 and parameters: {'learning_rate': 2.5932819926246227e-05, 'batch_size': 16, 'freeze_pct': 0.26458001021709354}. Best is trial 0 with value: 0.024119682475066146.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/2601435201.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:44, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.216228</td>\n",
       "      <td>0.014960</td>\n",
       "      <td>0.014960</td>\n",
       "      <td>0.014960</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.144300</td>\n",
       "      <td>3.897230</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.016283</td>\n",
       "      <td>0.016283</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.144300</td>\n",
       "      <td>3.678923</td>\n",
       "      <td>0.018929</td>\n",
       "      <td>0.018929</td>\n",
       "      <td>0.018929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.546189</td>\n",
       "      <td>0.019133</td>\n",
       "      <td>0.019133</td>\n",
       "      <td>0.019133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.500000</td>\n",
       "      <td>3.500149</td>\n",
       "      <td>0.019438</td>\n",
       "      <td>0.019438</td>\n",
       "      <td>0.019438</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 17:15:30,138] Trial 6 finished with value: 0.019438225117036433 and parameters: {'learning_rate': 2.1408496700657068e-05, 'batch_size': 8, 'freeze_pct': 0.5908479493424346}. Best is trial 0 with value: 0.024119682475066146.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/2601435201.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:14, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.405234</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.017708</td>\n",
       "      <td>0.017708</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.501200</td>\n",
       "      <td>4.086965</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.019744</td>\n",
       "      <td>0.019744</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.501200</td>\n",
       "      <td>3.857914</td>\n",
       "      <td>0.020151</td>\n",
       "      <td>0.020151</td>\n",
       "      <td>0.020151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.940600</td>\n",
       "      <td>3.722417</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.020761</td>\n",
       "      <td>0.020761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.940600</td>\n",
       "      <td>3.677211</td>\n",
       "      <td>0.020965</td>\n",
       "      <td>0.020965</td>\n",
       "      <td>0.020965</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='17' max='17' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [17/17 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-29 17:16:53,226] Trial 7 finished with value: 0.020964787299002648 and parameters: {'learning_rate': 2.3563691462538734e-05, 'batch_size': 4, 'freeze_pct': 0.38411610730206336}. Best is trial 0 with value: 0.024119682475066146.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Freeze params: {'learning_rate': 3.2799657790412474e-05, 'batch_size': 16, 'freeze_pct': 0.5823635847865559}\n",
      "Best Freeze Dev F1: 0.0241\n"
     ]
    }
   ],
   "source": [
    "def freeze_objective(trial):\n",
    "    \"\"\"Optuna objective for partial freezing.\"\"\"\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    freeze_pct = trial.suggest_float(\"freeze_pct\", 0.25, 0.75)\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # Freeze lower layers\n",
    "    total_layers = len([n for n, _ in model.named_parameters() if n.startswith(\"transformer.h.\")])\n",
    "    cutoff = int(total_layers * freeze_pct)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"transformer.h.\") and int(name.split(\".\")[2]) < cutoff:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/gpt-neo-freeze-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs * 2, \n",
    "        eval_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=40,\n",
    "        report_to=\"none\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "study_freeze = optuna.create_study(direction=\"maximize\")\n",
    "study_freeze.optimize(freeze_objective, n_trials=8)\n",
    "\n",
    "print(f\"Best Freeze params: {study_freeze.best_params}\")\n",
    "print(f\"Best Freeze Dev F1: {study_freeze.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c056b9e-e4b5-47c1-98b7-f366cf8b7632",
   "metadata": {},
   "source": [
    "## 12. Final Training with Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331592a-ed9c-4925-9514-9c510332cbbe",
   "metadata": {},
   "source": [
    "## 12.1 Full Fine-Tuning with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "72b5ec76-fd84-4efe-a661-d566878bae68",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T17:16:53.231576Z",
     "iopub.status.busy": "2025-05-29T17:16:53.231297Z",
     "iopub.status.idle": "2025-05-29T17:24:04.083163Z",
     "shell.execute_reply": "2025-05-29T17:24:04.082492Z",
     "shell.execute_reply.started": "2025-05-29T17:16:53.231559Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/718020180.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  ft_trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 07:01, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.071900</td>\n",
       "      <td>0.328616</td>\n",
       "      <td>0.082434</td>\n",
       "      <td>0.082434</td>\n",
       "      <td>0.082434</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.349300</td>\n",
       "      <td>0.290784</td>\n",
       "      <td>0.184714</td>\n",
       "      <td>0.184714</td>\n",
       "      <td>0.184714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.226800</td>\n",
       "      <td>0.308614</td>\n",
       "      <td>0.212192</td>\n",
       "      <td>0.212192</td>\n",
       "      <td>0.212192</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.155900</td>\n",
       "      <td>0.324837</td>\n",
       "      <td>0.231936</td>\n",
       "      <td>0.231936</td>\n",
       "      <td>0.231936</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.121500</td>\n",
       "      <td>0.337132</td>\n",
       "      <td>0.224710</td>\n",
       "      <td>0.224710</td>\n",
       "      <td>0.224710</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Full-FT Dev F1: 0.2247\n"
     ]
    }
   ],
   "source": [
    "best_ft_params = study_ft.best_params\n",
    "\n",
    "ft_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "ft_model.resize_token_embeddings(len(tokenizer))\n",
    "ft_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "ft_args = TrainingArguments(\n",
    "    output_dir=\"outputs/gpt-neo-ner-ft-final\",\n",
    "    per_device_train_batch_size=best_ft_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_ft_params[\"batch_size\"] * 2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=40,\n",
    "    save_strategy=\"epoch\",\n",
    "    max_steps=200,\n",
    "    learning_rate=best_ft_params[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=40,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "ft_trainer = Trainer(\n",
    "    model=ft_model,\n",
    "    args=ft_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "\n",
    "ft_trainer.train()\n",
    "ft_final_metrics = ft_trainer.evaluate()\n",
    "print(f\"Final Full-FT Dev F1: {ft_final_metrics['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad6c57-44a8-47a6-8bf2-35291d6eb234",
   "metadata": {},
   "source": [
    "## 12.2 LoRA with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "018b08bb-7601-4c15-9036-6f6e3175f854",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T17:26:02.984646Z",
     "iopub.status.busy": "2025-05-29T17:26:02.984137Z",
     "iopub.status.idle": "2025-05-29T17:29:22.533132Z",
     "shell.execute_reply": "2025-05-29T17:29:22.532435Z",
     "shell.execute_reply.started": "2025-05-29T17:26:02.984622Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/1391009243.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  lora_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 03:10, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.017500</td>\n",
       "      <td>0.314297</td>\n",
       "      <td>0.114085</td>\n",
       "      <td>0.114085</td>\n",
       "      <td>0.114085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.349000</td>\n",
       "      <td>0.262005</td>\n",
       "      <td>0.209953</td>\n",
       "      <td>0.209953</td>\n",
       "      <td>0.209953</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.291500</td>\n",
       "      <td>0.252488</td>\n",
       "      <td>0.227254</td>\n",
       "      <td>0.227254</td>\n",
       "      <td>0.227254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.264300</td>\n",
       "      <td>0.257544</td>\n",
       "      <td>0.235905</td>\n",
       "      <td>0.235905</td>\n",
       "      <td>0.235905</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.245400</td>\n",
       "      <td>0.255932</td>\n",
       "      <td>0.232241</td>\n",
       "      <td>0.232241</td>\n",
       "      <td>0.232241</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/peft/utils/save_and_load.py:260: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:05]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final LoRA Dev F1: 0.2322\n"
     ]
    }
   ],
   "source": [
    "best_lora_params = study_lora.best_params\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"TOKEN_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=best_lora_params[\"r\"],\n",
    "    lora_alpha=best_lora_params[\"alpha\"],\n",
    "    lora_dropout=best_lora_params[\"dropout\"],\n",
    ")\n",
    "\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "lora_args = TrainingArguments(\n",
    "    output_dir=\"outputs/gpt-neo-ner-lora-final\",\n",
    "    per_device_train_batch_size=best_lora_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_lora_params[\"batch_size\"] * 2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=40,\n",
    "    save_strategy=\"epoch\",\n",
    "    max_steps=200,\n",
    "    learning_rate=best_lora_params[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=40,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=lora_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "\n",
    "lora_trainer.train()\n",
    "lora_final_metrics = lora_trainer.evaluate()\n",
    "print(f\"Final LoRA Dev F1: {lora_final_metrics['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58066c-f688-45d5-a07a-3db71ee23b10",
   "metadata": {},
   "source": [
    "## 12.3 Partial Freezing with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b8e72d07-649a-483d-9ab4-9350da5260c6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T17:59:29.710064Z",
     "iopub.status.busy": "2025-05-29T17:59:29.709516Z",
     "iopub.status.idle": "2025-05-29T17:59:31.945546Z",
     "shell.execute_reply": "2025-05-29T17:59:31.944497Z",
     "shell.execute_reply.started": "2025-05-29T17:59:29.710039Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to clean up space...\n",
      "\\nCleaning Optuna temporary directories...\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-ft-0\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-lora-0\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-freeze-0\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-ft-1\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-lora-1\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-freeze-1\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-ft-2\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-lora-2\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-freeze-2\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-ft-3\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-lora-3\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-freeze-3\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-ft-4\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-lora-4\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-freeze-4\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-ft-5\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-lora-5\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-freeze-5\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-ft-6\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-lora-6\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-freeze-6\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-ft-7\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-lora-7\n",
      "Removing directory: /kaggle/working/tmp/gpt-neo-freeze-7\n",
      "\\nCleaning completed final model output directories (checkpoints)...\n",
      "Baseline training complete, metrics available. Removing its output directory.\n",
      "Removing directory: /kaggle/working/outputs/gpt-neo-ner-baseline\n",
      "Full FT final training complete, metrics available. Removing its output directory.\n",
      "Removing directory: /kaggle/working/outputs/gpt-neo-ner-ft-final\n",
      "LoRA final training complete, metrics available. Removing its output directory.\n",
      "Removing directory: /kaggle/working/outputs/gpt-neo-ner-lora-final\n",
      "\\nDisk space after cleanup attempt:\n",
      "Filesystem      Size  Used Avail Use% Mounted on\n",
      "/dev/loop1       20G  1.9G   18G  10% /kaggle/working\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    }
   ],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "def remove_dir_if_exists(dir_path_str):\n",
    "    dir_path = Path(dir_path_str)\n",
    "    if dir_path.exists() and dir_path.is_dir():\n",
    "        print(f\"Removing directory: {dir_path}\")\n",
    "        shutil.rmtree(dir_path)\n",
    "    else:\n",
    "        print(f\"Directory not found or not a directory: {dir_path}\")\n",
    "\n",
    "print(\"Attempting to clean up space...\")\n",
    "\n",
    "# --- Optuna Temporary Directories ---\n",
    "# These are generally safe to remove if the study_*.optimize() calls for FT and LoRA are complete\n",
    "# and you have stored study_ft.best_params and study_lora.best_params.\n",
    "# Be cautious with tmp/gpt-neo-freeze-* if you might want to resume the freeze HPO.\n",
    "# However, the error occurs in the FINAL freeze training, so HPO for freeze should be done.\n",
    "print(\"\\\\nCleaning Optuna temporary directories...\")\n",
    "for i in range(8): # Assuming max 8 trials based on your n_trials=8\n",
    "    remove_dir_if_exists(f\"/kaggle/working/tmp/gpt-neo-ft-{i}\")\n",
    "    remove_dir_if_exists(f\"/kaggle/working/tmp/gpt-neo-lora-{i}\")\n",
    "    remove_dir_if_exists(f\"/kaggle/working/tmp/gpt-neo-freeze-{i}\")\n",
    "\n",
    "\n",
    "# --- Checkpoint Directories for COMPLETED Final Training Runs ---\n",
    "# Only remove these if the corresponding Python variables holding metrics are populated\n",
    "# (e.g., baseline_metrics, ft_final_metrics, lora_final_metrics)\n",
    "# and you don't need the actual saved model files for later loading.\n",
    "# The summary table uses the Python variables for metrics.\n",
    "\n",
    "print(\"\\\\nCleaning completed final model output directories (checkpoints)...\")\n",
    "if 'baseline_metrics' in globals() and baseline_metrics:\n",
    "    print(\"Baseline training complete, metrics available. Removing its output directory.\")\n",
    "    remove_dir_if_exists(\"/kaggle/working/outputs/gpt-neo-ner-baseline/\")\n",
    "else:\n",
    "    print(\"Baseline metrics not found, skipping cleanup of its output directory.\")\n",
    "\n",
    "if 'ft_final_metrics' in globals() and ft_final_metrics:\n",
    "    print(\"Full FT final training complete, metrics available. Removing its output directory.\")\n",
    "    remove_dir_if_exists(\"/kaggle/working/outputs/gpt-neo-ner-ft-final/\")\n",
    "else:\n",
    "    print(\"Full FT final metrics not found, skipping cleanup of its output directory.\")\n",
    "\n",
    "if 'lora_final_metrics' in globals() and lora_final_metrics:\n",
    "    print(\"LoRA final training complete, metrics available. Removing its output directory.\")\n",
    "    remove_dir_if_exists(\"/kaggle/working/outputs/gpt-neo-ner-lora-final/\")\n",
    "else:\n",
    "    print(\"LoRA final metrics not found, skipping cleanup of its output directory.\")\n",
    "\n",
    "# DO NOT remove /kaggle/working/outputs/gpt-neo-ner-freeze-final/ yet, as that's the one currently failing.\n",
    "\n",
    "print(\"\\\\nDisk space after cleanup attempt:\")\n",
    "!df -h /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "1017ac5f-657a-4cae-94c5-da6db5376060",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T17:59:47.301297Z",
     "iopub.status.busy": "2025-05-29T17:59:47.300431Z",
     "iopub.status.idle": "2025-05-29T18:05:18.520232Z",
     "shell.execute_reply": "2025-05-29T18:05:18.519371Z",
     "shell.execute_reply.started": "2025-05-29T17:59:47.301261Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/tmp/ipykernel_35/885414885.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  freeze_trainer = Trainer(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 05:22, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.334100</td>\n",
       "      <td>4.013680</td>\n",
       "      <td>0.030328</td>\n",
       "      <td>0.030328</td>\n",
       "      <td>0.030328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.932300</td>\n",
       "      <td>2.943712</td>\n",
       "      <td>0.023306</td>\n",
       "      <td>0.023306</td>\n",
       "      <td>0.023306</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.959500</td>\n",
       "      <td>2.225080</td>\n",
       "      <td>0.018624</td>\n",
       "      <td>0.018624</td>\n",
       "      <td>0.018624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.413600</td>\n",
       "      <td>1.830544</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>0.017810</td>\n",
       "      <td>0.017810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.194600</td>\n",
       "      <td>1.707416</td>\n",
       "      <td>0.017301</td>\n",
       "      <td>0.017301</td>\n",
       "      <td>0.017301</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:04]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Freeze Dev F1: 0.0173\n"
     ]
    }
   ],
   "source": [
    "best_freeze_params = study_freeze.best_params\n",
    "\n",
    "freeze_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "freeze_model.resize_token_embeddings(len(tokenizer))\n",
    "freeze_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Apply freezing\n",
    "total_layers = len([n for n, _ in freeze_model.named_parameters() if n.startswith(\"transformer.h.\")])\n",
    "cutoff = int(total_layers * best_freeze_params[\"freeze_pct\"])\n",
    "\n",
    "for name, param in freeze_model.named_parameters():\n",
    "    if name.startswith(\"transformer.h.\") and int(name.split(\".\")[2]) < cutoff:\n",
    "        param.requires_grad = False\n",
    "\n",
    "freeze_args = TrainingArguments(\n",
    "    output_dir=\"outputs/gpt-neo-ner-freeze-final\",\n",
    "    per_device_train_batch_size=best_freeze_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_freeze_params[\"batch_size\"] * 2,\n",
    "    eval_strategy=\"steps\",\n",
    "    eval_steps=40,\n",
    "    save_strategy=\"epoch\",\n",
    "    max_steps=200,\n",
    "    learning_rate=best_freeze_params[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=40,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "freeze_trainer = Trainer(\n",
    "    model=freeze_model,\n",
    "    args=freeze_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "\n",
    "freeze_trainer.train()\n",
    "freeze_final_metrics = freeze_trainer.evaluate()\n",
    "print(f\"Final Freeze Dev F1: {freeze_final_metrics['eval_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9fddf8-327f-4451-afa2-e9e6ee3ffea7",
   "metadata": {},
   "source": [
    "## 13. Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5d494-cf56-48f6-abcb-45afd7b01c54",
   "metadata": {},
   "source": [
    "## 13.1 Compile Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "92348bd1-7c05-481c-92a5-25d735175e90",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T18:05:23.516740Z",
     "iopub.status.busy": "2025-05-29T18:05:23.516216Z",
     "iopub.status.idle": "2025-05-29T18:05:23.523276Z",
     "shell.execute_reply": "2025-05-29T18:05:23.522524Z",
     "shell.execute_reply.started": "2025-05-29T18:05:23.516716Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GPT-Neo NER RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "Baseline (3 epochs):\n",
      "  Dev F1: 0.0577\n",
      "  Trainable Parameters: Full model (~125M)\n",
      "\n",
      "Full Fine-Tuning (200 steps):\n",
      "  Dev F1: 0.2247\n",
      "  Trainable Parameters: Full model (~125M)\n",
      "  Best Hyperparameters: {'learning_rate': 3.994900876547036e-05, 'batch_size': 16}\n",
      "\n",
      "LoRA (200 steps):\n",
      "  Dev F1: 0.2322\n",
      "  Trainable Parameters: ~0.02M trainable\n",
      "  Best Hyperparameters: {'learning_rate': 0.0006864436426020529, 'r': 16, 'alpha': 32, 'dropout': 0.13102133790278095, 'batch_size': 8}\n",
      "\n",
      "Partial Freezing (200 steps):\n",
      "  Dev F1: 0.0173\n",
      "  Trainable Parameters: ~52.2M trainable\n",
      "  Best Hyperparameters: {'learning_rate': 3.2799657790412474e-05, 'batch_size': 16, 'freeze_pct': 0.5823635847865559}\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"GPT-Neo NER Results\": {\n",
    "        \"Baseline (3 epochs)\": {\n",
    "            \"Dev F1\": baseline_metrics[\"eval_f1\"],\n",
    "            \"Parameters\": \"Full model (~125M)\"\n",
    "        },\n",
    "        \"Full Fine-Tuning (200 steps)\": {\n",
    "            \"Dev F1\": ft_final_metrics[\"eval_f1\"],\n",
    "            \"Best Params\": best_ft_params,\n",
    "            \"Parameters\": \"Full model (~125M)\"\n",
    "        },\n",
    "        \"LoRA (200 steps)\": {\n",
    "            \"Dev F1\": lora_final_metrics[\"eval_f1\"],\n",
    "            \"Best Params\": best_lora_params,\n",
    "            \"Parameters\": f\"~{best_lora_params['r'] * 2 * 768 / 1e6:.2f}M trainable\"\n",
    "        },\n",
    "        \"Partial Freezing (200 steps)\": {\n",
    "            \"Dev F1\": freeze_final_metrics[\"eval_f1\"],\n",
    "            \"Best Params\": best_freeze_params,\n",
    "            \"Parameters\": f\"~{(1 - best_freeze_params['freeze_pct']) * 125:.1f}M trainable\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GPT-Neo NER RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for method, metrics in results[\"GPT-Neo NER Results\"].items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Dev F1: {metrics['Dev F1']:.4f}\")\n",
    "    print(f\"  Trainable Parameters: {metrics['Parameters']}\")\n",
    "    if \"Best Params\" in metrics:\n",
    "        print(f\"  Best Hyperparameters: {metrics['Best Params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f1cfd0-15e6-41f5-b680-4f79c43063f0",
   "metadata": {},
   "source": [
    "## 13.2 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8d242eca-b647-4239-bfe0-c15929437a88",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-05-29T18:05:29.022544Z",
     "iopub.status.busy": "2025-05-29T18:05:29.022273Z",
     "iopub.status.idle": "2025-05-29T18:05:29.027631Z",
     "shell.execute_reply": "2025-05-29T18:05:29.026837Z",
     "shell.execute_reply.started": "2025-05-29T18:05:29.022528Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to outputs/gpt_neo_ner_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"outputs/gpt_neo_ner_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to outputs/gpt_neo_ner_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4513ca-8a35-464f-8c05-c765a04ba467",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 7543282,
     "sourceId": 11992746,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
