{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f9a3c54d-f00b-4700-b7de-d1153ba686e9",
   "metadata": {},
   "source": [
    "## 1. Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "83bdc97a-885e-4c62-8431-a1ad552f56ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "from collections import Counter\n",
    "from sklearn.metrics import precision_recall_fscore_support\n",
    "\n",
    "import optuna\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e81542-3f8d-4665-9bb5-1ab0922004be",
   "metadata": {},
   "source": [
    "## 2. Data Paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab6afc85-7f48-46a2-9b2c-54b5bde59cc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_DIR = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\dataset\\train\")\n",
    "DEV_DIR = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\dataset\\dev\")\n",
    "TEST_DIR = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\dataset\\test\")\n",
    "\n",
    "assert TRAIN_DIR.exists(), f\"Train directory not found: {TRAIN_DIR}\"\n",
    "assert DEV_DIR.exists(), f\"Dev directory not found: {DEV_DIR}\"\n",
    "assert TEST_DIR.exists(), f\"Test directory not found: {TEST_DIR}\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50806017-9832-408e-aeb1-4a2ce5d7122e",
   "metadata": {},
   "source": [
    "## 3. Load DocIE Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9f274a9e-9277-4d0c-898a-6140f86fe09b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded documents - Train: 51, Dev: 23, Test: 248\n"
     ]
    }
   ],
   "source": [
    "def load_docie_docs(folder: Path, recursive: bool = False):\n",
    "    \"\"\"Load JSON documents from DocIE dataset.\"\"\"\n",
    "    docs = []\n",
    "    pattern = \"**/*.json\" if recursive else \"*.json\"\n",
    "    for file in folder.glob(pattern):\n",
    "        data = json.loads(file.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(data, list):\n",
    "            docs.extend(data)\n",
    "        else:\n",
    "            docs.append(data)\n",
    "    return docs\n",
    "\n",
    "train_docs = load_docie_docs(TRAIN_DIR)\n",
    "dev_docs = load_docie_docs(DEV_DIR)\n",
    "test_docs = load_docie_docs(TEST_DIR, recursive=True)\n",
    "\n",
    "print(f\"Loaded documents - Train: {len(train_docs)}, Dev: {len(dev_docs)}, Test: {len(test_docs)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70317df-dc73-4ef2-8957-9c969e64a720",
   "metadata": {},
   "source": [
    "## 4. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "791904ac-c9cc-4c14-bb54-b5bb699a72a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document statistics - Avg tokens: 919.1, Max tokens: 2560\n",
      "\n",
      "Top 10 entity types:\n",
      "  DATE: 647\n",
      "  MISC: 417\n",
      "  PERSON: 242\n",
      "  ORG: 241\n",
      "  CARDINAL: 224\n",
      "  GPE: 157\n",
      "  WORK_OF_ART: 65\n",
      "  NORP: 59\n",
      "  ORDINAL: 55\n",
      "  QUANTITY: 42\n"
     ]
    }
   ],
   "source": [
    "# 4.1 Document lengths\n",
    "lengths = [len(doc[\"doc\"].split()) for doc in train_docs]\n",
    "print(f\"Document statistics - Avg tokens: {np.mean(lengths):.1f}, Max tokens: {np.max(lengths)}\")\n",
    "\n",
    "# 4.2 Entity distribution\n",
    "entity_counter = Counter(ent[\"type\"] for doc in train_docs for ent in doc[\"entities\"])\n",
    "print(f\"\\nTop 10 entity types:\")\n",
    "for entity_type, count in entity_counter.most_common(10):\n",
    "    print(f\"  {entity_type}: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdf3eb57-59b6-4eb2-ad9c-def56db4d99d",
   "metadata": {},
   "source": [
    "## 5. Setup Label Mappings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "936ca5a2-80de-4e10-b5af-04a54b3ba0be",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total NER labels: 39\n"
     ]
    }
   ],
   "source": [
    "# Get all entity types from train set\n",
    "entity_types = train_docs[0][\"entity_label_set\"]\n",
    "\n",
    "# Create BIO tags\n",
    "ner_labels = [\"O\"]\n",
    "for entity_type in entity_types:\n",
    "    ner_labels.extend([f\"B-{entity_type}\", f\"I-{entity_type}\"])\n",
    "\n",
    "label2id = {label: i for i, label in enumerate(ner_labels)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "print(f\"Total NER labels: {len(ner_labels)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "120cdce5-9063-4233-ae38-0bd132d0e88b",
   "metadata": {},
   "source": [
    "## 6. Initialize GPT-Neo Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6cda9680-f415-4d3c-9004-7d1c7a3893ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenizer vocab size: 50258\n"
     ]
    }
   ],
   "source": [
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# CRITICAL: Add padding token for GPT-Neo\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "\n",
    "print(f\"Tokenizer vocab size: {len(tokenizer)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e072468-b787-4673-bc0e-6d9e894161a2",
   "metadata": {},
   "source": [
    "## 7. Tokenization with Label Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "384fe02b-9f89-4380-9b19-798f6b45ea1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "stride = 128\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    \"\"\"Tokenize documents and align NER labels with subword tokens.\"\"\"\n",
    "    all_input_ids = []\n",
    "    all_attention_mask = []\n",
    "    all_labels = []\n",
    "    \n",
    "    for doc, entities in zip(examples[\"doc\"], examples[\"entities\"]):\n",
    "        # Tokenize with overflow handling\n",
    "        tokenized = tokenizer(\n",
    "            doc,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            return_overflowing_tokens=True,\n",
    "        )\n",
    "        \n",
    "        # Process each chunk\n",
    "        for i in range(len(tokenized[\"input_ids\"])):\n",
    "            offsets = tokenized[\"offset_mapping\"][i]\n",
    "            input_ids = tokenized[\"input_ids\"][i]\n",
    "            attention_mask = tokenized[\"attention_mask\"][i]\n",
    "            \n",
    "            # Initialize with \"O\" labels\n",
    "            chunk_labels = [\"O\"] * len(offsets)\n",
    "            \n",
    "            # Map entity mentions to token labels\n",
    "            for entity in entities:\n",
    "                entity_type = entity[\"type\"]\n",
    "                for mention in entity[\"mentions\"]:\n",
    "                    start = doc.find(mention)\n",
    "                    if start < 0:\n",
    "                        continue\n",
    "                    end = start + len(mention)\n",
    "                    \n",
    "                    # Label tokens that overlap with entity mention\n",
    "                    for idx, (token_start, token_end) in enumerate(offsets):\n",
    "                        if token_start >= start and token_end <= end:\n",
    "                            prefix = \"B\" if token_start == start else \"I\"\n",
    "                            chunk_labels[idx] = f\"{prefix}-{entity_type}\"\n",
    "            \n",
    "            # Convert labels to IDs\n",
    "            label_ids = [label2id.get(label, label2id[\"O\"]) for label in chunk_labels]\n",
    "            \n",
    "            all_input_ids.append(input_ids)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "            all_labels.append(label_ids)\n",
    "    \n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_mask,\n",
    "        \"labels\": all_labels,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04e88248-8a37-4b3c-a604-75d75ffa5832",
   "metadata": {},
   "source": [
    "## 8. Create Hugging Face Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "82a92c85-bc94-4225-8b9e-18087336bc83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9959e8b5fdba4e5d97e62b5c73d23255",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "66f902425bea48bc811dd8f06f195256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokenized datasets - Train: 167, Dev: 75\n"
     ]
    }
   ],
   "source": [
    "# Convert to HF Dataset\n",
    "hf_train = Dataset.from_list(train_docs)\n",
    "hf_dev = Dataset.from_list(dev_docs)\n",
    "\n",
    "# Apply tokenization\n",
    "columns_to_remove = [\"domain\", \"title\", \"doc\", \"entities\", \"triples\", \"label_set\", \"entity_label_set\"]\n",
    "\n",
    "hf_train = hf_train.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=columns_to_remove,\n",
    ")\n",
    "\n",
    "hf_dev = hf_dev.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=columns_to_remove,\n",
    ")\n",
    "\n",
    "print(f\"Tokenized datasets - Train: {len(hf_train)}, Dev: {len(hf_dev)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b07321-97c9-4b3d-a800-a505d5560de7",
   "metadata": {},
   "source": [
    "## 9. Setup Training Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4ba3de4c-fc8a-4ed5-bbf2-2fe72e72e42c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# Metrics computation\n",
    "def compute_metrics_entity_only(pred):\n",
    "    \"\"\"Compute metrics only on entity tokens (non-O labels).\"\"\"\n",
    "    preds = pred.predictions.argmax(-1).flatten()\n",
    "    labels = pred.label_ids.flatten()\n",
    "    \n",
    "    # Filter out non-entity labels and padding\n",
    "    mask = (labels != label2id[\"O\"]) & (labels != -100)\n",
    "    \n",
    "    if mask.sum() == 0:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    \n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        labels[mask], preds[mask], average=\"micro\"\n",
    "    )\n",
    "    \n",
    "    return {\"precision\": precision, \"recall\": recall, \"f1\": f1}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51522042-b57e-4080-9088-3ae5b394b173",
   "metadata": {},
   "source": [
    "## 10. Baseline: Full Fine-Tuning (3 epochs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b519c69-8508-4c22-8885-e08dbe2c2b54",
   "metadata": {},
   "source": [
    "## 10.1 Train Baseline Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9e24584d-aab4-4432-99ed-1b36024ccb81",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\3252327352.py:29: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 08:55, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.606034</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.448494</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.435772</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline Dev F1: 0.0000\n"
     ]
    }
   ],
   "source": [
    "def train_baseline():\n",
    "    \"\"\"Train GPT-Neo baseline for 3 epochs.\"\"\"\n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    \n",
    "    # Resize embeddings to accommodate padding token\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"outputs/gpt-neo-ner-baseline\",\n",
    "        per_device_train_batch_size=16,\n",
    "        per_device_eval_batch_size=16,\n",
    "        evaluation_strategy=\"epoch\",\n",
    "        save_strategy=\"epoch\",\n",
    "        num_train_epochs=3,\n",
    "        learning_rate=3e-3,\n",
    "        weight_decay=0.0,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        save_total_limit=2,\n",
    "        load_best_model_at_end=True,\n",
    "        metric_for_best_model=\"eval_f1\",\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    metrics = trainer.evaluate()\n",
    "    \n",
    "    return trainer, metrics\n",
    "\n",
    "baseline_trainer, baseline_metrics = train_baseline()\n",
    "print(f\"Baseline Dev F1: {baseline_metrics['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd2a3a98-9e05-41d6-a790-e994286a7eee",
   "metadata": {},
   "source": [
    "## 11. Hyperparameter Tuning with 100 Steps Budget"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ccd6d1e-69cd-4f14-825a-56f8cc7f0d42",
   "metadata": {},
   "source": [
    "## 11.1 Full Fine-Tuning Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d250e230-23b0-496c-b54b-dd2170062098",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 01:16:48,897] A new study created in memory with name: no-name-fa03aecb-0a36-47ae-be4b-abde409aba82\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\2328446363.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 07:59, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.622342</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>0.004813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.328600</td>\n",
       "      <td>0.452824</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>0.029563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.328600</td>\n",
       "      <td>0.412285</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.630500</td>\n",
       "      <td>0.393966</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.033001</td>\n",
       "      <td>0.033001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.630500</td>\n",
       "      <td>0.388718</td>\n",
       "      <td>0.038157</td>\n",
       "      <td>0.038157</td>\n",
       "      <td>0.038157</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 01:25:14,051] Trial 0 finished with value: 0.03815744242007563 and parameters: {'learning_rate': 2.3360560900592628e-05, 'batch_size': 4}. Best is trial 0 with value: 0.03815744242007563.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\2328446363.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 13:52, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.714098</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.005156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.701400</td>\n",
       "      <td>0.521812</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.009282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.701400</td>\n",
       "      <td>0.458327</td>\n",
       "      <td>0.012719</td>\n",
       "      <td>0.012719</td>\n",
       "      <td>0.012719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.435911</td>\n",
       "      <td>0.020626</td>\n",
       "      <td>0.020626</td>\n",
       "      <td>0.020626</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.656000</td>\n",
       "      <td>0.424649</td>\n",
       "      <td>0.025782</td>\n",
       "      <td>0.025782</td>\n",
       "      <td>0.025782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 01:39:40,170] Trial 1 finished with value: 0.02578205568924029 and parameters: {'learning_rate': 1.57433061586488e-05, 'batch_size': 8}. Best is trial 0 with value: 0.03815744242007563.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\2328446363.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 07:49, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.973589</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.007563</td>\n",
       "      <td>0.007563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.818500</td>\n",
       "      <td>0.589955</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.011344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.818500</td>\n",
       "      <td>0.513352</td>\n",
       "      <td>0.020282</td>\n",
       "      <td>0.020282</td>\n",
       "      <td>0.020282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.484245</td>\n",
       "      <td>0.019594</td>\n",
       "      <td>0.019594</td>\n",
       "      <td>0.019594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.817100</td>\n",
       "      <td>0.476470</td>\n",
       "      <td>0.017532</td>\n",
       "      <td>0.017532</td>\n",
       "      <td>0.017532</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 01:47:55,313] Trial 2 finished with value: 0.017531797868683398 and parameters: {'learning_rate': 1.1388188337387634e-05, 'batch_size': 4}. Best is trial 0 with value: 0.03815744242007563.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\2328446363.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 25:13, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.114825</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.009282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.075800</td>\n",
       "      <td>0.645710</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>0.004813</td>\n",
       "      <td>0.004813</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.075800</td>\n",
       "      <td>0.529065</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.009969</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.802600</td>\n",
       "      <td>0.487641</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.011344</td>\n",
       "      <td>0.011344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.802600</td>\n",
       "      <td>0.478245</td>\n",
       "      <td>0.011688</td>\n",
       "      <td>0.011688</td>\n",
       "      <td>0.011688</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 02:13:50,491] Trial 3 finished with value: 0.011687865245788931 and parameters: {'learning_rate': 1.0286725513372162e-05, 'batch_size': 16}. Best is trial 0 with value: 0.03815744242007563.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\2328446363.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 06:39, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.463056</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "      <td>0.022001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.041300</td>\n",
       "      <td>0.363042</td>\n",
       "      <td>0.037814</td>\n",
       "      <td>0.037814</td>\n",
       "      <td>0.037814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.041300</td>\n",
       "      <td>0.326878</td>\n",
       "      <td>0.135785</td>\n",
       "      <td>0.135785</td>\n",
       "      <td>0.135785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.493500</td>\n",
       "      <td>0.314693</td>\n",
       "      <td>0.137848</td>\n",
       "      <td>0.137848</td>\n",
       "      <td>0.137848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.493500</td>\n",
       "      <td>0.312920</td>\n",
       "      <td>0.139911</td>\n",
       "      <td>0.139911</td>\n",
       "      <td>0.139911</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 02:20:53,216] Trial 4 finished with value: 0.13991062220694397 and parameters: {'learning_rate': 4.8286874395325485e-05, 'batch_size': 4}. Best is trial 4 with value: 0.13991062220694397.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\2328446363.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 06:36, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.217464</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.009282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.185700</td>\n",
       "      <td>0.658668</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.005156</td>\n",
       "      <td>0.005156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.185700</td>\n",
       "      <td>0.561856</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.006531</td>\n",
       "      <td>0.006531</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.853700</td>\n",
       "      <td>0.518426</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.009625</td>\n",
       "      <td>0.009625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.853700</td>\n",
       "      <td>0.509103</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.009969</td>\n",
       "      <td>0.009969</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 02:27:52,057] Trial 5 finished with value: 0.009969061533172912 and parameters: {'learning_rate': 1.0504389705952048e-05, 'batch_size': 4}. Best is trial 4 with value: 0.13991062220694397.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\2328446363.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 11:36, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.673290</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.004469</td>\n",
       "      <td>0.004469</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.587500</td>\n",
       "      <td>0.490267</td>\n",
       "      <td>0.011688</td>\n",
       "      <td>0.011688</td>\n",
       "      <td>0.011688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.587500</td>\n",
       "      <td>0.441822</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.013750</td>\n",
       "      <td>0.013750</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>0.417038</td>\n",
       "      <td>0.027157</td>\n",
       "      <td>0.027157</td>\n",
       "      <td>0.027157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.623100</td>\n",
       "      <td>0.406989</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>0.030938</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 02:39:55,699] Trial 6 finished with value: 0.030938466827088347 and parameters: {'learning_rate': 1.7932173751818086e-05, 'batch_size': 8}. Best is trial 4 with value: 0.13991062220694397.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\2328446363.py:28: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 20:55, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.615403</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>0.008938</td>\n",
       "      <td>0.008938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.354900</td>\n",
       "      <td>0.461433</td>\n",
       "      <td>0.016501</td>\n",
       "      <td>0.016501</td>\n",
       "      <td>0.016501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.354900</td>\n",
       "      <td>0.403151</td>\n",
       "      <td>0.030251</td>\n",
       "      <td>0.030251</td>\n",
       "      <td>0.030251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>0.381995</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.036439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.614000</td>\n",
       "      <td>0.376760</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.040564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 03:01:25,562] Trial 7 finished with value: 0.040563767617738056 and parameters: {'learning_rate': 1.811829484599362e-05, 'batch_size': 16}. Best is trial 4 with value: 0.13991062220694397.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Full-FT params: {'learning_rate': 4.8286874395325485e-05, 'batch_size': 4}\n",
      "Best Full-FT Dev F1: 0.1399\n"
     ]
    }
   ],
   "source": [
    "def ft_objective(trial):\n",
    "    \"\"\"Optuna objective for full fine-tuning.\"\"\"\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/gpt-neo-ft-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs * 2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=40,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "study_ft = optuna.create_study(direction=\"maximize\")\n",
    "study_ft.optimize(ft_objective, n_trials=8)\n",
    "\n",
    "print(f\"Best Full-FT params: {study_ft.best_params}\")\n",
    "print(f\"Best Full-FT Dev F1: {study_ft.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5e5564f-cc6d-4852-baae-4dddbf451490",
   "metadata": {},
   "source": [
    "## 11.2 LoRA Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0f7a9300-b710-455a-9399-d8084d60d1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 03:01:25,576] A new study created in memory with name: no-name-9f4e01c9-d70b-457c-86a1-d6ebab550cb1\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\1834648189.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:24, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.235757</td>\n",
       "      <td>0.025782</td>\n",
       "      <td>0.025782</td>\n",
       "      <td>0.025782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.831700</td>\n",
       "      <td>4.993161</td>\n",
       "      <td>0.028876</td>\n",
       "      <td>0.028876</td>\n",
       "      <td>0.028876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.831700</td>\n",
       "      <td>4.811079</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>0.029907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.377000</td>\n",
       "      <td>4.698526</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>0.031970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.377000</td>\n",
       "      <td>4.659576</td>\n",
       "      <td>0.032314</td>\n",
       "      <td>0.032314</td>\n",
       "      <td>0.032314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 03:07:12,907] Trial 0 finished with value: 0.03231350979718116 and parameters: {'learning_rate': 1.699587963095526e-05, 'r': 16, 'alpha': 32, 'dropout': 0.1543889276946275, 'batch_size': 4}. Best is trial 0 with value: 0.03231350979718116.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\1834648189.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 18:23, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.906203</td>\n",
       "      <td>0.039189</td>\n",
       "      <td>0.039189</td>\n",
       "      <td>0.039189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.576400</td>\n",
       "      <td>0.616284</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.008594</td>\n",
       "      <td>0.008594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.576400</td>\n",
       "      <td>0.504432</td>\n",
       "      <td>0.022344</td>\n",
       "      <td>0.022344</td>\n",
       "      <td>0.022344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.462681</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.766700</td>\n",
       "      <td>0.451140</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 03:26:10,381] Trial 1 finished with value: 0.036782399449982815 and parameters: {'learning_rate': 0.0001346154675646194, 'r': 16, 'alpha': 32, 'dropout': 0.10921539303642946, 'batch_size': 16}. Best is trial 1 with value: 0.036782399449982815.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\1834648189.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:24, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.331085</td>\n",
       "      <td>0.073565</td>\n",
       "      <td>0.073565</td>\n",
       "      <td>0.073565</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.104800</td>\n",
       "      <td>0.511995</td>\n",
       "      <td>0.030595</td>\n",
       "      <td>0.030595</td>\n",
       "      <td>0.030595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.104800</td>\n",
       "      <td>0.461866</td>\n",
       "      <td>0.090753</td>\n",
       "      <td>0.090753</td>\n",
       "      <td>0.090753</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.427077</td>\n",
       "      <td>0.089034</td>\n",
       "      <td>0.089034</td>\n",
       "      <td>0.089034</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.692300</td>\n",
       "      <td>0.420581</td>\n",
       "      <td>0.090409</td>\n",
       "      <td>0.090409</td>\n",
       "      <td>0.090409</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 03:31:56,903] Trial 2 finished with value: 0.0904090752836026 and parameters: {'learning_rate': 0.00025904814349906566, 'r': 8, 'alpha': 16, 'dropout': 0.20472740729579403, 'batch_size': 4}. Best is trial 2 with value: 0.0904090752836026.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\1834648189.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:46, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.263208</td>\n",
       "      <td>0.021657</td>\n",
       "      <td>0.021657</td>\n",
       "      <td>0.021657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.223900</td>\n",
       "      <td>0.551404</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.009282</td>\n",
       "      <td>0.009282</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.223900</td>\n",
       "      <td>0.458954</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.038501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.669600</td>\n",
       "      <td>0.435083</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>0.044689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.669600</td>\n",
       "      <td>0.423088</td>\n",
       "      <td>0.053627</td>\n",
       "      <td>0.053627</td>\n",
       "      <td>0.053627</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 03:42:08,857] Trial 3 finished with value: 0.053626675833619804 and parameters: {'learning_rate': 0.00017770439231661823, 'r': 16, 'alpha': 32, 'dropout': 0.2981820339412351, 'batch_size': 8}. Best is trial 2 with value: 0.0904090752836026.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\1834648189.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:44, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.478688</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.038501</td>\n",
       "      <td>0.038501</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.440900</td>\n",
       "      <td>3.285784</td>\n",
       "      <td>0.063596</td>\n",
       "      <td>0.063596</td>\n",
       "      <td>0.063596</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.440900</td>\n",
       "      <td>2.341301</td>\n",
       "      <td>0.054314</td>\n",
       "      <td>0.054314</td>\n",
       "      <td>0.054314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.360200</td>\n",
       "      <td>1.775057</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.036439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.360200</td>\n",
       "      <td>1.589349</td>\n",
       "      <td>0.032314</td>\n",
       "      <td>0.032314</td>\n",
       "      <td>0.032314</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 03:52:18,219] Trial 4 finished with value: 0.03231350979718116 and parameters: {'learning_rate': 8.513578365221181e-05, 'r': 8, 'alpha': 16, 'dropout': 0.018202468861115106, 'batch_size': 8}. Best is trial 2 with value: 0.0904090752836026.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\1834648189.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:46, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.208848</td>\n",
       "      <td>0.017188</td>\n",
       "      <td>0.017188</td>\n",
       "      <td>0.017188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.150600</td>\n",
       "      <td>4.701285</td>\n",
       "      <td>0.030595</td>\n",
       "      <td>0.030595</td>\n",
       "      <td>0.030595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.150600</td>\n",
       "      <td>4.312561</td>\n",
       "      <td>0.039189</td>\n",
       "      <td>0.039189</td>\n",
       "      <td>0.039189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.217100</td>\n",
       "      <td>4.068164</td>\n",
       "      <td>0.042970</td>\n",
       "      <td>0.042970</td>\n",
       "      <td>0.042970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.217100</td>\n",
       "      <td>3.982038</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>0.044689</td>\n",
       "      <td>0.044689</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 04:02:30,266] Trial 5 finished with value: 0.0446888965280165 and parameters: {'learning_rate': 4.014320539354151e-05, 'r': 16, 'alpha': 16, 'dropout': 0.2094956589758433, 'batch_size': 8}. Best is trial 2 with value: 0.0904090752836026.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\1834648189.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 18:10, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.223549</td>\n",
       "      <td>0.050533</td>\n",
       "      <td>0.050533</td>\n",
       "      <td>0.050533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.166900</td>\n",
       "      <td>2.808394</td>\n",
       "      <td>0.070471</td>\n",
       "      <td>0.070471</td>\n",
       "      <td>0.070471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.166900</td>\n",
       "      <td>1.737190</td>\n",
       "      <td>0.037470</td>\n",
       "      <td>0.037470</td>\n",
       "      <td>0.037470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.841500</td>\n",
       "      <td>1.170613</td>\n",
       "      <td>0.021657</td>\n",
       "      <td>0.021657</td>\n",
       "      <td>0.021657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.841500</td>\n",
       "      <td>1.006924</td>\n",
       "      <td>0.019251</td>\n",
       "      <td>0.019251</td>\n",
       "      <td>0.019251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 04:21:14,320] Trial 6 finished with value: 0.019250601581299414 and parameters: {'learning_rate': 9.58673524408832e-05, 'r': 8, 'alpha': 16, 'dropout': 0.02212344112971184, 'batch_size': 16}. Best is trial 2 with value: 0.0904090752836026.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\1834648189.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:47, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.727169</td>\n",
       "      <td>0.081128</td>\n",
       "      <td>0.081128</td>\n",
       "      <td>0.081128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.784400</td>\n",
       "      <td>2.578005</td>\n",
       "      <td>0.112410</td>\n",
       "      <td>0.112410</td>\n",
       "      <td>0.112410</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.784400</td>\n",
       "      <td>1.742580</td>\n",
       "      <td>0.098316</td>\n",
       "      <td>0.098316</td>\n",
       "      <td>0.098316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.851400</td>\n",
       "      <td>1.298755</td>\n",
       "      <td>0.071158</td>\n",
       "      <td>0.071158</td>\n",
       "      <td>0.071158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.851400</td>\n",
       "      <td>1.166322</td>\n",
       "      <td>0.063939</td>\n",
       "      <td>0.063939</td>\n",
       "      <td>0.063939</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 04:31:27,284] Trial 7 finished with value: 0.06393949810931591 and parameters: {'learning_rate': 8.404875701450256e-05, 'r': 16, 'alpha': 16, 'dropout': 0.2597506010900016, 'batch_size': 8}. Best is trial 2 with value: 0.0904090752836026.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best LoRA params: {'learning_rate': 0.00025904814349906566, 'r': 8, 'alpha': 16, 'dropout': 0.20472740729579403, 'batch_size': 4}\n",
      "Best LoRA Dev F1: 0.0904\n"
     ]
    }
   ],
   "source": [
    "def lora_objective(trial):\n",
    "    \"\"\"Optuna objective for LoRA fine-tuning.\"\"\"\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    r = trial.suggest_categorical(\"r\", [4, 8, 16])\n",
    "    alpha = trial.suggest_categorical(\"alpha\", [16, 32])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    \n",
    "    lora_config = LoraConfig(\n",
    "        task_type=\"TOKEN_CLS\",\n",
    "        inference_mode=False,\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=dropout,\n",
    "    )\n",
    "    \n",
    "    base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    base_model.resize_token_embeddings(len(tokenizer))\n",
    "    base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    model = get_peft_model(base_model, lora_config)\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/gpt-neo-lora-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs * 2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=40,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "study_lora = optuna.create_study(direction=\"maximize\")\n",
    "study_lora.optimize(lora_objective, n_trials=8)\n",
    "\n",
    "print(f\"Best LoRA params: {study_lora.best_params}\")\n",
    "print(f\"Best LoRA Dev F1: {study_lora.best_value:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c79447e-efcd-416f-8f7e-8cbb513809ee",
   "metadata": {},
   "source": [
    "## 11.3 Partial Freezing Hyperparameter Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "54ea7040-3285-454a-9a1e-718a58dff91d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 04:31:27,295] A new study created in memory with name: no-name-54b4691e-cf0d-468f-933f-69e97a77eadf\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\632449046.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:39, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.999034</td>\n",
       "      <td>0.017532</td>\n",
       "      <td>0.017532</td>\n",
       "      <td>0.017532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>4.366651</td>\n",
       "      <td>0.035064</td>\n",
       "      <td>0.035064</td>\n",
       "      <td>0.035064</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.900000</td>\n",
       "      <td>3.917424</td>\n",
       "      <td>0.046408</td>\n",
       "      <td>0.046408</td>\n",
       "      <td>0.046408</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.636100</td>\n",
       "      <td>3.649133</td>\n",
       "      <td>0.051220</td>\n",
       "      <td>0.051220</td>\n",
       "      <td>0.051220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.636100</td>\n",
       "      <td>3.556528</td>\n",
       "      <td>0.052252</td>\n",
       "      <td>0.052252</td>\n",
       "      <td>0.052252</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 04:41:33,218] Trial 0 finished with value: 0.052251632863526985 and parameters: {'learning_rate': 4.68643894587759e-05, 'batch_size': 8, 'freeze_pct': 0.49605228016306246}. Best is trial 0 with value: 0.052251632863526985.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\632449046.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 17:35, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.276584</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.026470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.694200</td>\n",
       "      <td>2.867229</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>0.029907</td>\n",
       "      <td>0.029907</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.694200</td>\n",
       "      <td>2.589751</td>\n",
       "      <td>0.034032</td>\n",
       "      <td>0.034032</td>\n",
       "      <td>0.034032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.841800</td>\n",
       "      <td>2.428768</td>\n",
       "      <td>0.035751</td>\n",
       "      <td>0.035751</td>\n",
       "      <td>0.035751</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.841800</td>\n",
       "      <td>2.374424</td>\n",
       "      <td>0.035407</td>\n",
       "      <td>0.035407</td>\n",
       "      <td>0.035407</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 04:59:41,298] Trial 1 finished with value: 0.03540735647989 and parameters: {'learning_rate': 3.137799220931297e-05, 'batch_size': 16, 'freeze_pct': 0.5162507103405969}. Best is trial 0 with value: 0.052251632863526985.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\632449046.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 17:35, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.811994</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>0.031970</td>\n",
       "      <td>0.031970</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.333000</td>\n",
       "      <td>4.270221</td>\n",
       "      <td>0.034720</td>\n",
       "      <td>0.034720</td>\n",
       "      <td>0.034720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.333000</td>\n",
       "      <td>3.886184</td>\n",
       "      <td>0.037470</td>\n",
       "      <td>0.037470</td>\n",
       "      <td>0.037470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.255900</td>\n",
       "      <td>3.655025</td>\n",
       "      <td>0.039876</td>\n",
       "      <td>0.039876</td>\n",
       "      <td>0.039876</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.255900</td>\n",
       "      <td>3.575303</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.040564</td>\n",
       "      <td>0.040564</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 05:17:49,246] Trial 2 finished with value: 0.040563767617738056 and parameters: {'learning_rate': 3.615326922303352e-05, 'batch_size': 16, 'freeze_pct': 0.6016283413192749}. Best is trial 0 with value: 0.052251632863526985.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\632449046.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 17:37, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>5.255941</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.026470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.809300</td>\n",
       "      <td>5.051569</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>0.029563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.809300</td>\n",
       "      <td>4.906227</td>\n",
       "      <td>0.030251</td>\n",
       "      <td>0.030251</td>\n",
       "      <td>0.030251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.390100</td>\n",
       "      <td>4.818561</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>0.030938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.390100</td>\n",
       "      <td>4.788240</td>\n",
       "      <td>0.031626</td>\n",
       "      <td>0.031626</td>\n",
       "      <td>0.031626</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 05:35:59,030] Trial 3 finished with value: 0.031625988312134756 and parameters: {'learning_rate': 1.3478567737864302e-05, 'batch_size': 16, 'freeze_pct': 0.4388738585587837}. Best is trial 0 with value: 0.052251632863526985.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\632449046.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:36, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.943456</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>0.029563</td>\n",
       "      <td>0.029563</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.484700</td>\n",
       "      <td>4.486163</td>\n",
       "      <td>0.033689</td>\n",
       "      <td>0.033689</td>\n",
       "      <td>0.033689</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.484700</td>\n",
       "      <td>4.161949</td>\n",
       "      <td>0.036095</td>\n",
       "      <td>0.036095</td>\n",
       "      <td>0.036095</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.617000</td>\n",
       "      <td>3.969516</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.617000</td>\n",
       "      <td>3.903218</td>\n",
       "      <td>0.037126</td>\n",
       "      <td>0.037126</td>\n",
       "      <td>0.037126</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 05:46:00,809] Trial 4 finished with value: 0.037126160192506016 and parameters: {'learning_rate': 3.306793054978061e-05, 'batch_size': 8, 'freeze_pct': 0.7400036459277188}. Best is trial 0 with value: 0.052251632863526985.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\632449046.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 17:31, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.062951</td>\n",
       "      <td>0.027157</td>\n",
       "      <td>0.027157</td>\n",
       "      <td>0.027157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.462100</td>\n",
       "      <td>2.516263</td>\n",
       "      <td>0.034376</td>\n",
       "      <td>0.034376</td>\n",
       "      <td>0.034376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.462100</td>\n",
       "      <td>2.160491</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.036439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.347900</td>\n",
       "      <td>1.961669</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.347900</td>\n",
       "      <td>1.896145</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:11]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 06:04:03,772] Trial 5 finished with value: 0.036782399449982815 and parameters: {'learning_rate': 4.392241100313621e-05, 'batch_size': 16, 'freeze_pct': 0.604964613507464}. Best is trial 0 with value: 0.052251632863526985.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\632449046.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:37, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.882679</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>0.030938</td>\n",
       "      <td>0.030938</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.422800</td>\n",
       "      <td>4.378869</td>\n",
       "      <td>0.034720</td>\n",
       "      <td>0.034720</td>\n",
       "      <td>0.034720</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.422800</td>\n",
       "      <td>4.022056</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "      <td>0.036782</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.466900</td>\n",
       "      <td>3.810516</td>\n",
       "      <td>0.037814</td>\n",
       "      <td>0.037814</td>\n",
       "      <td>0.037814</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.466900</td>\n",
       "      <td>3.737684</td>\n",
       "      <td>0.038845</td>\n",
       "      <td>0.038845</td>\n",
       "      <td>0.038845</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 06:14:07,152] Trial 6 finished with value: 0.038844963905122036 and parameters: {'learning_rate': 3.6501860444839506e-05, 'batch_size': 8, 'freeze_pct': 0.5958103899935909}. Best is trial 0 with value: 0.052251632863526985.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\632449046.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:22, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.466835</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>0.022688</td>\n",
       "      <td>0.022688</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.916900</td>\n",
       "      <td>3.182568</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.026470</td>\n",
       "      <td>0.026470</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.916900</td>\n",
       "      <td>2.987263</td>\n",
       "      <td>0.028532</td>\n",
       "      <td>0.028532</td>\n",
       "      <td>0.028532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.358900</td>\n",
       "      <td>2.873738</td>\n",
       "      <td>0.029220</td>\n",
       "      <td>0.029220</td>\n",
       "      <td>0.029220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.358900</td>\n",
       "      <td>2.835142</td>\n",
       "      <td>0.030251</td>\n",
       "      <td>0.030251</td>\n",
       "      <td>0.030251</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-28 06:19:51,224] Trial 7 finished with value: 0.030250945342041938 and parameters: {'learning_rate': 2.5752130433873785e-05, 'batch_size': 4, 'freeze_pct': 0.6114340820059213}. Best is trial 0 with value: 0.052251632863526985.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Freeze params: {'learning_rate': 4.68643894587759e-05, 'batch_size': 8, 'freeze_pct': 0.49605228016306246}\n",
      "Best Freeze Dev F1: 0.0523\n"
     ]
    }
   ],
   "source": [
    "def freeze_objective(trial):\n",
    "    \"\"\"Optuna objective for partial freezing.\"\"\"\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    freeze_pct = trial.suggest_float(\"freeze_pct\", 0.25, 0.75)\n",
    "    \n",
    "    model = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    \n",
    "    # Freeze lower layers\n",
    "    total_layers = len([n for n, _ in model.named_parameters() if n.startswith(\"transformer.h.\")])\n",
    "    cutoff = int(total_layers * freeze_pct)\n",
    "    \n",
    "    for name, param in model.named_parameters():\n",
    "        if name.startswith(\"transformer.h.\") and int(name.split(\".\")[2]) < cutoff:\n",
    "            param.requires_grad = False\n",
    "    \n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/gpt-neo-freeze-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs * 2, \n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "        logging_steps=40,\n",
    "    )\n",
    "    \n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    \n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "study_freeze = optuna.create_study(direction=\"maximize\")\n",
    "study_freeze.optimize(freeze_objective, n_trials=8)\n",
    "\n",
    "print(f\"Best Freeze params: {study_freeze.best_params}\")\n",
    "print(f\"Best Freeze Dev F1: {study_freeze.best_value:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c056b9e-e4b5-47c1-98b7-f366cf8b7632",
   "metadata": {},
   "source": [
    "## 12. Final Training with Optimal Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c331592a-ed9c-4925-9514-9c510332cbbe",
   "metadata": {},
   "source": [
    "## 12.1 Full Fine-Tuning with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "72b5ec76-fd84-4efe-a661-d566878bae68",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\2810298427.py:25: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  ft_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 11:51, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.013200</td>\n",
       "      <td>0.363463</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.043314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.479900</td>\n",
       "      <td>0.301928</td>\n",
       "      <td>0.186662</td>\n",
       "      <td>0.186662</td>\n",
       "      <td>0.186662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.381200</td>\n",
       "      <td>0.295022</td>\n",
       "      <td>0.201100</td>\n",
       "      <td>0.201100</td>\n",
       "      <td>0.201100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.305000</td>\n",
       "      <td>0.292476</td>\n",
       "      <td>0.215194</td>\n",
       "      <td>0.215194</td>\n",
       "      <td>0.215194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.295220</td>\n",
       "      <td>0.214163</td>\n",
       "      <td>0.214163</td>\n",
       "      <td>0.214163</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Full-FT Dev F1: 0.2142\n"
     ]
    }
   ],
   "source": [
    "best_ft_params = study_ft.best_params\n",
    "\n",
    "ft_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "ft_model.resize_token_embeddings(len(tokenizer))\n",
    "ft_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "ft_args = TrainingArguments(\n",
    "    output_dir=\"outputs/gpt-neo-ner-ft-final\",\n",
    "    per_device_train_batch_size=best_ft_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_ft_params[\"batch_size\"] * 2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=40,\n",
    "    save_strategy=\"epoch\",\n",
    "    max_steps=200,\n",
    "    learning_rate=best_ft_params[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=40,\n",
    ")\n",
    "\n",
    "ft_trainer = Trainer(\n",
    "    model=ft_model,\n",
    "    args=ft_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "\n",
    "ft_trainer.train()\n",
    "ft_final_metrics = ft_trainer.evaluate()\n",
    "print(f\"Final Full-FT Dev F1: {ft_final_metrics['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ad6c57-44a8-47a6-8bf2-35291d6eb234",
   "metadata": {},
   "source": [
    "## 12.2 LoRA with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "018b08bb-7601-4c15-9036-6f6e3175f854",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\2524108751.py:35: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  lora_trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 09:11, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.464800</td>\n",
       "      <td>0.468565</td>\n",
       "      <td>0.072877</td>\n",
       "      <td>0.072877</td>\n",
       "      <td>0.072877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.611600</td>\n",
       "      <td>0.363007</td>\n",
       "      <td>0.103128</td>\n",
       "      <td>0.103128</td>\n",
       "      <td>0.103128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.493600</td>\n",
       "      <td>0.337800</td>\n",
       "      <td>0.094878</td>\n",
       "      <td>0.094878</td>\n",
       "      <td>0.094878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.433600</td>\n",
       "      <td>0.325761</td>\n",
       "      <td>0.133035</td>\n",
       "      <td>0.133035</td>\n",
       "      <td>0.133035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.448700</td>\n",
       "      <td>0.322332</td>\n",
       "      <td>0.120660</td>\n",
       "      <td>0.120660</td>\n",
       "      <td>0.120660</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:241: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:241: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:241: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:241: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\peft\\utils\\save_and_load.py:241: UserWarning: Setting `save_embedding_layers` to `True` as the embedding layer has been resized during finetuning.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final LoRA Dev F1: 0.1207\n"
     ]
    }
   ],
   "source": [
    "best_lora_params = study_lora.best_params\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    task_type=\"TOKEN_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=best_lora_params[\"r\"],\n",
    "    lora_alpha=best_lora_params[\"alpha\"],\n",
    "    lora_dropout=best_lora_params[\"dropout\"],\n",
    ")\n",
    "\n",
    "base_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "base_model.resize_token_embeddings(len(tokenizer))\n",
    "base_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "lora_model = get_peft_model(base_model, lora_config)\n",
    "\n",
    "lora_args = TrainingArguments(\n",
    "    output_dir=\"outputs/gpt-neo-ner-lora-final\",\n",
    "    per_device_train_batch_size=best_lora_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_lora_params[\"batch_size\"] * 2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=40,\n",
    "    save_strategy=\"epoch\",\n",
    "    max_steps=200,\n",
    "    learning_rate=best_lora_params[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=40,\n",
    ")\n",
    "\n",
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=lora_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "\n",
    "lora_trainer.train()\n",
    "lora_final_metrics = lora_trainer.evaluate()\n",
    "print(f\"Final LoRA Dev F1: {lora_final_metrics['eval_f1']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc58066c-f688-45d5-a07a-3db71ee23b10",
   "metadata": {},
   "source": [
    "## 12.3 Partial Freezing with Best Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1017ac5f-657a-4cae-94c5-da6db5376060",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_25460\\1095697538.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  freeze_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 18:01, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.374600</td>\n",
       "      <td>4.242479</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.036439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.605300</td>\n",
       "      <td>2.904651</td>\n",
       "      <td>0.050189</td>\n",
       "      <td>0.050189</td>\n",
       "      <td>0.050189</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.373500</td>\n",
       "      <td>2.022399</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.043314</td>\n",
       "      <td>0.043314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>1.682900</td>\n",
       "      <td>1.572234</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.036439</td>\n",
       "      <td>0.036439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>1.403900</td>\n",
       "      <td>1.439071</td>\n",
       "      <td>0.036095</td>\n",
       "      <td>0.036095</td>\n",
       "      <td>0.036095</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:14]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Freeze Dev F1: 0.0361\n"
     ]
    }
   ],
   "source": [
    "best_freeze_params = study_freeze.best_params\n",
    "\n",
    "freeze_model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "freeze_model.resize_token_embeddings(len(tokenizer))\n",
    "freeze_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Apply freezing\n",
    "total_layers = len([n for n, _ in freeze_model.named_parameters() if n.startswith(\"transformer.h.\")])\n",
    "cutoff = int(total_layers * best_freeze_params[\"freeze_pct\"])\n",
    "\n",
    "for name, param in freeze_model.named_parameters():\n",
    "    if name.startswith(\"transformer.h.\") and int(name.split(\".\")[2]) < cutoff:\n",
    "        param.requires_grad = False\n",
    "\n",
    "freeze_args = TrainingArguments(\n",
    "    output_dir=\"outputs/gpt-neo-ner-freeze-final\",\n",
    "    per_device_train_batch_size=best_freeze_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_freeze_params[\"batch_size\"] * 2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=40,\n",
    "    save_strategy=\"epoch\",\n",
    "    max_steps=200,\n",
    "    learning_rate=best_freeze_params[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    logging_steps=40,\n",
    ")\n",
    "\n",
    "freeze_trainer = Trainer(\n",
    "    model=freeze_model,\n",
    "    args=freeze_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "\n",
    "freeze_trainer.train()\n",
    "freeze_final_metrics = freeze_trainer.evaluate()\n",
    "print(f\"Final Freeze Dev F1: {freeze_final_metrics['eval_f1']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e9fddf8-327f-4451-afa2-e9e6ee3ffea7",
   "metadata": {},
   "source": [
    "## 13. Results Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3f5d494-cf56-48f6-abcb-45afd7b01c54",
   "metadata": {},
   "source": [
    "## 13.1 Compile Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "92348bd1-7c05-481c-92a5-25d735175e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "GPT-Neo NER RESULTS SUMMARY\n",
      "==================================================\n",
      "\n",
      "Baseline (3 epochs):\n",
      "  Dev F1: 0.0000\n",
      "  Trainable Parameters: Full model (~125M)\n",
      "\n",
      "Full Fine-Tuning (200 steps):\n",
      "  Dev F1: 0.2142\n",
      "  Trainable Parameters: Full model (~125M)\n",
      "  Best Hyperparameters: {'learning_rate': 4.8286874395325485e-05, 'batch_size': 4}\n",
      "\n",
      "LoRA (200 steps):\n",
      "  Dev F1: 0.1207\n",
      "  Trainable Parameters: ~0.01M trainable\n",
      "  Best Hyperparameters: {'learning_rate': 0.00025904814349906566, 'r': 8, 'alpha': 16, 'dropout': 0.20472740729579403, 'batch_size': 4}\n",
      "\n",
      "Partial Freezing (200 steps):\n",
      "  Dev F1: 0.0361\n",
      "  Trainable Parameters: ~63.0M trainable\n",
      "  Best Hyperparameters: {'learning_rate': 4.68643894587759e-05, 'batch_size': 8, 'freeze_pct': 0.49605228016306246}\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"GPT-Neo NER Results\": {\n",
    "        \"Baseline (3 epochs)\": {\n",
    "            \"Dev F1\": baseline_metrics[\"eval_f1\"],\n",
    "            \"Parameters\": \"Full model (~125M)\"\n",
    "        },\n",
    "        \"Full Fine-Tuning (200 steps)\": {\n",
    "            \"Dev F1\": ft_final_metrics[\"eval_f1\"],\n",
    "            \"Best Params\": best_ft_params,\n",
    "            \"Parameters\": \"Full model (~125M)\"\n",
    "        },\n",
    "        \"LoRA (200 steps)\": {\n",
    "            \"Dev F1\": lora_final_metrics[\"eval_f1\"],\n",
    "            \"Best Params\": best_lora_params,\n",
    "            \"Parameters\": f\"~{best_lora_params['r'] * 2 * 768 / 1e6:.2f}M trainable\"\n",
    "        },\n",
    "        \"Partial Freezing (200 steps)\": {\n",
    "            \"Dev F1\": freeze_final_metrics[\"eval_f1\"],\n",
    "            \"Best Params\": best_freeze_params,\n",
    "            \"Parameters\": f\"~{(1 - best_freeze_params['freeze_pct']) * 125:.1f}M trainable\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"GPT-Neo NER RESULTS SUMMARY\")\n",
    "print(\"=\"*50)\n",
    "for method, metrics in results[\"GPT-Neo NER Results\"].items():\n",
    "    print(f\"\\n{method}:\")\n",
    "    print(f\"  Dev F1: {metrics['Dev F1']:.4f}\")\n",
    "    print(f\"  Trainable Parameters: {metrics['Parameters']}\")\n",
    "    if \"Best Params\" in metrics:\n",
    "        print(f\"  Best Hyperparameters: {metrics['Best Params']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07f1cfd0-15e6-41f5-b680-4f79c43063f0",
   "metadata": {},
   "source": [
    "## 13.2 Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8d242eca-b647-4239-bfe0-c15929437a88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to outputs/gpt_neo_ner_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "with open(\"outputs/gpt_neo_ner_results.json\", \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(\"\\nResults saved to outputs/gpt_neo_ner_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c4513ca-8a35-464f-8c05-c765a04ba467",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
