{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2590ddfd-ec36-42d2-9010-9c993945d6b1",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "12868581-f2a3-471e-a477-de9c6e07c837",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27ec4669-8814-4189-b2ee-3761919f6d8d",
   "metadata": {},
   "source": [
    "## 2. Pfade & Daten laden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "998caec5-13fa-4b5b-ae2e-93ff29c34a2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deine lokalen Windows-Pfade\n",
    "TRAIN_DIR = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\dataset\\train\")\n",
    "DEV_DIR   = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\dataset\\dev\")\n",
    "TEST_DIR  = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\dataset\\test\")\n",
    "\n",
    "assert TRAIN_DIR.exists(), f\"Train-Ordner nicht gefunden: {TRAIN_DIR}\"\n",
    "assert DEV_DIR.exists(),   f\"Dev-Ordner nicht gefunden:   {DEV_DIR}\"\n",
    "assert TEST_DIR.exists(),  f\"Test-Ordner nicht gefunden:  {TEST_DIR}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "b8ba6cb8-e21c-45c0-9b47-48121edbcc12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 51 â”‚ Dev: 23 â”‚ Test: 248\n"
     ]
    }
   ],
   "source": [
    "def load_docie_docs(folder: Path, recursive: bool = False):\n",
    "    docs = []\n",
    "    pattern = \"**/*.json\" if recursive else \"*.json\"\n",
    "    for file in folder.glob(pattern):\n",
    "        data = json.loads(file.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(data, list):\n",
    "            docs.extend(data)\n",
    "        else:\n",
    "            docs.append(data)\n",
    "    return docs\n",
    "\n",
    "train_docs = load_docie_docs(TRAIN_DIR)\n",
    "dev_docs   = load_docie_docs(DEV_DIR)\n",
    "test_docs  = load_docie_docs(TEST_DIR, recursive=True)\n",
    "\n",
    "print(\"Train:\", len(train_docs), \"â”‚ Dev:\", len(dev_docs), \"â”‚ Test:\", len(test_docs))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16153b0b-5e1e-4727-8484-3001dc83e964",
   "metadata": {},
   "source": [
    "## 3. Label-Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "e02c0868-b9ac-4f3a-9c1f-2f40797a829a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl NER-Labels: 39\n"
     ]
    }
   ],
   "source": [
    "# Entity-Typen & B-I Labels\n",
    "entity_types = train_docs[0][\"entity_label_set\"]\n",
    "ner_labels = [\"O\"] + [f\"{p}-{t}\" for t in entity_types for p in (\"B\",\"I\")]\n",
    "label2id   = {l:i for i,l in enumerate(ner_labels)}\n",
    "id2label   = {i:l for l,i in label2id.items()}\n",
    "print(\"Anzahl NER-Labels:\", len(ner_labels))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55a3feb-302e-439f-93ce-40596a41ac39",
   "metadata": {},
   "source": [
    "## 4. Tokenizer and dataset objects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "2f756623-cdd2-42c4-93a1-aa434c7bc6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "\n",
    "# HF-Datasets\n",
    "hf_train = Dataset.from_list(train_docs)\n",
    "hf_dev   = Dataset.from_list(dev_docs)\n",
    "\n",
    "max_length, stride = 512, 128\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a1193ca-1b19-4cb7-b926-6c3feb3ab276",
   "metadata": {},
   "source": [
    "## 5. Tokenize & Align Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e80b88d3-3b65-4b7c-8f3b-ffbc1d718d39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_and_align_labels(examples):\n",
    "    all_input_ids, all_attention_mask, all_labels = [], [], []\n",
    "    for doc, entities in zip(examples[\"doc\"], examples[\"entities\"]):\n",
    "        tok = tokenizer(\n",
    "            doc, return_offsets_mapping=True,\n",
    "            truncation=True, max_length=max_length,\n",
    "            stride=stride, return_overflowing_tokens=True\n",
    "        )\n",
    "        for i in range(len(tok[\"input_ids\"])):\n",
    "            offsets = tok[\"offset_mapping\"][i]\n",
    "            labels  = [\"O\"] * len(offsets)\n",
    "            # Mentions einzeichnenâ€¦\n",
    "            for ent in entities:\n",
    "                for mention in ent[\"mentions\"]:\n",
    "                    start = doc.find(mention)\n",
    "                    end   = start + len(mention)\n",
    "                    for idx,(o_start,o_end) in enumerate(offsets):\n",
    "                        if o_start>=start and o_end<=end:\n",
    "                            labels[idx] = (\"B\" if o_start==start else \"I\") + f\"-{ent['type']}\"\n",
    "            all_input_ids.append(tok[\"input_ids\"][i])\n",
    "            all_attention_mask.append(tok[\"attention_mask\"][i])\n",
    "            all_labels.append([label2id.get(l,0) for l in labels])\n",
    "    return {\"input_ids\": all_input_ids,\n",
    "            \"attention_mask\": all_attention_mask,\n",
    "            \"labels\": all_labels}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6645d9-ccea-4fba-9965-49df02d30dd8",
   "metadata": {},
   "source": [
    "## 6. Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "6d4da568-3e78-496c-86d8-cb2e039ab44d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c88b5ded226a4387b69b713f8be616f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e9b8974b5c64ca6b483f8712309f32b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cols_to_remove = [\"domain\",\"title\",\"doc\",\"entities\",\"triples\",\"label_set\",\"entity_label_set\"]\n",
    "hf_train = hf_train.map(tokenize_and_align_labels, batched=True, remove_columns=cols_to_remove)\n",
    "hf_dev   = hf_dev.map(  tokenize_and_align_labels, batched=True, remove_columns=cols_to_remove)\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d8d648-223d-4389-a510-119ec6a233f5",
   "metadata": {},
   "source": [
    "## 7. Eval-Metrik"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "66023ffb-191a-40d5-8371-b56e136cdaf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "\n",
    "def compute_metrics_entity_only(pred):\n",
    "    preds  = pred.predictions.argmax(-1).flatten()\n",
    "    labels = pred.label_ids.flatten()\n",
    "    # Nur echte Entity-Token (kein O, kein -100)\n",
    "    mask = (labels != -100) & (labels != label2id[\"O\"])\n",
    "    if mask.sum() == 0:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0, \"accuracy\": 0.0}\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        labels[mask], preds[mask], average=\"micro\"\n",
    "    )\n",
    "    acc = accuracy_score(labels[mask], preds[mask])\n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f1, \"accuracy\": acc}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97880f47-0ac8-436a-bd33-548b4a0f08f4",
   "metadata": {},
   "source": [
    "## 8. Full-Fine-Tuning (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7102fe06-4936-43a7-a5ea-4844c5e79427",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_131988\\431312413.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 27:46, Epoch 13/14]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>0.768800</td>\n",
       "      <td>0.529565</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000251</td>\n",
       "      <td>0.000251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.609000</td>\n",
       "      <td>0.448234</td>\n",
       "      <td>0.167671</td>\n",
       "      <td>0.167671</td>\n",
       "      <td>0.167671</td>\n",
       "      <td>0.167671</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>0.440300</td>\n",
       "      <td>0.409679</td>\n",
       "      <td>0.241717</td>\n",
       "      <td>0.241717</td>\n",
       "      <td>0.241717</td>\n",
       "      <td>0.241717</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.391100</td>\n",
       "      <td>0.426748</td>\n",
       "      <td>0.287400</td>\n",
       "      <td>0.287400</td>\n",
       "      <td>0.287400</td>\n",
       "      <td>0.287400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.345000</td>\n",
       "      <td>0.426722</td>\n",
       "      <td>0.310994</td>\n",
       "      <td>0.310994</td>\n",
       "      <td>0.310994</td>\n",
       "      <td>0.310994</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Full-FT Dev-F1: 0.31099397590361444 Accuracy: 0.31099397590361444\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('outputs/bert-ner-full-ft-opt\\\\tokenizer_config.json',\n",
       " 'outputs/bert-ner-full-ft-opt\\\\special_tokens_map.json',\n",
       " 'outputs/bert-ner-full-ft-opt\\\\vocab.txt',\n",
       " 'outputs/bert-ner-full-ft-opt\\\\added_tokens.json',\n",
       " 'outputs/bert-ner-full-ft-opt\\\\tokenizer.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8.1: Hyperparams (aus 17.1)\n",
    "best_ft = {\"learning_rate\":4.3586e-05,\"batch_size\":16}\n",
    "\n",
    "# 8.2: TrainingArguments\n",
    "ft_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bert-ner-full-ft-opt\",\n",
    "    per_device_train_batch_size=best_ft[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_ft[\"batch_size\"]*2,\n",
    "    evaluation_strategy=\"steps\", eval_steps=30,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    max_steps=150,\n",
    "    learning_rate=best_ft[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# 8.3: Model & Trainer\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id,\n",
    ")\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=ft_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "\n",
    "# 8.4: Train, Eval & Save\n",
    "trainer.train()\n",
    "metrics = trainer.evaluate()\n",
    "print(\"âœ… Full-FT Dev-F1:\", metrics[\"eval_f1\"], \"Accuracy:\", metrics[\"eval_accuracy\"])\n",
    "trainer.save_model(\"outputs/bert-ner-full-ft-opt\")\n",
    "tokenizer.save_pretrained(\"outputs/bert-ner-full-ft-opt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9abacaa5-79a7-4adb-bf66-0ff57df60d50",
   "metadata": {},
   "source": [
    "## 9. LoRA-Fine-Tuning (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "f4ca005a-2903-420e-b54d-9585e1d00e54",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "LoraConfig.__init__() got an unexpected keyword argument 'learning_rate'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 6\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# 9.2: Adapter konfigurieren\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpeft\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m LoraConfig, get_peft_model\n\u001b[1;32m----> 6\u001b[0m lora_conf \u001b[38;5;241m=\u001b[39m LoraConfig(task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTOKEN_CLS\u001b[39m\u001b[38;5;124m\"\u001b[39m, inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mbest_lora)\n\u001b[0;32m      7\u001b[0m base \u001b[38;5;241m=\u001b[39m AutoModelForTokenClassification\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name,\n\u001b[0;32m      8\u001b[0m        num_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(ner_labels), id2label\u001b[38;5;241m=\u001b[39mid2label, label2id\u001b[38;5;241m=\u001b[39mlabel2id)\n\u001b[0;32m      9\u001b[0m lora_model \u001b[38;5;241m=\u001b[39m get_peft_model(base, lora_conf)\n",
      "\u001b[1;31mTypeError\u001b[0m: LoraConfig.__init__() got an unexpected keyword argument 'learning_rate'"
     ]
    }
   ],
   "source": [
    "# 9.1: Beste LoRA-Params\n",
    "best_lora = { \"learning_rate\":2.22e-05, \"r\":16, \"alpha\":16, \"dropout\":0.0158, \"batch_size\":8 }\n",
    "\n",
    "# 9.2: Adapter konfigurieren\n",
    "from peft import LoraConfig, get_peft_model\n",
    "lora_conf = LoraConfig(task_type=\"TOKEN_CLS\", inference_mode=False, **best_lora)\n",
    "base = AutoModelForTokenClassification.from_pretrained(model_name,\n",
    "       num_labels=len(ner_labels), id2label=id2label, label2id=label2id)\n",
    "lora_model = get_peft_model(base, lora_conf)\n",
    "\n",
    "# 9.3: Trainer\n",
    "lora_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bert-ner-lora-final\",\n",
    "    per_device_train_batch_size=best_lora[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_lora[\"batch_size\"]*2,\n",
    "    evaluation_strategy=\"steps\", eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=100,\n",
    "    learning_rate=best_lora[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_only_model=True,\n",
    ")\n",
    "lora_trainer = Trainer(\n",
    "    model=lora_model,\n",
    "    args=lora_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "lora_trainer.train()\n",
    "print(\"âœ… LoRA Dev-F1:\", lora_trainer.evaluate()[\"eval_f1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13477180-e83a-4850-b707-753a508f8d7a",
   "metadata": {},
   "source": [
    "## 10. Partial-Freeze (BERT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "651c16e0-9807-4c63-876c-9c157d5a46c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_131988\\2391252804.py:26: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  freeze_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 16:30, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.489900</td>\n",
       "      <td>3.430923</td>\n",
       "      <td>0.018323</td>\n",
       "      <td>0.018323</td>\n",
       "      <td>0.018323</td>\n",
       "      <td>0.018323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.407800</td>\n",
       "      <td>3.342619</td>\n",
       "      <td>0.024598</td>\n",
       "      <td>0.024598</td>\n",
       "      <td>0.024598</td>\n",
       "      <td>0.024598</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.330000</td>\n",
       "      <td>3.264952</td>\n",
       "      <td>0.028112</td>\n",
       "      <td>0.028112</td>\n",
       "      <td>0.028112</td>\n",
       "      <td>0.028112</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.248000</td>\n",
       "      <td>3.197932</td>\n",
       "      <td>0.028865</td>\n",
       "      <td>0.028865</td>\n",
       "      <td>0.028865</td>\n",
       "      <td>0.028865</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.198500</td>\n",
       "      <td>3.140686</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>0.030120</td>\n",
       "      <td>0.030120</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.149300</td>\n",
       "      <td>3.093532</td>\n",
       "      <td>0.032380</td>\n",
       "      <td>0.032380</td>\n",
       "      <td>0.032380</td>\n",
       "      <td>0.032380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.099800</td>\n",
       "      <td>3.056425</td>\n",
       "      <td>0.033384</td>\n",
       "      <td>0.033384</td>\n",
       "      <td>0.033384</td>\n",
       "      <td>0.033384</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.092400</td>\n",
       "      <td>3.029610</td>\n",
       "      <td>0.033133</td>\n",
       "      <td>0.033133</td>\n",
       "      <td>0.033133</td>\n",
       "      <td>0.033133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.052900</td>\n",
       "      <td>3.013155</td>\n",
       "      <td>0.031878</td>\n",
       "      <td>0.031878</td>\n",
       "      <td>0.031878</td>\n",
       "      <td>0.031878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.043600</td>\n",
       "      <td>3.007274</td>\n",
       "      <td>0.031878</td>\n",
       "      <td>0.031878</td>\n",
       "      <td>0.031878</td>\n",
       "      <td>0.031878</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:06]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Freeze Dev-F1: 0.03187751004016064\n"
     ]
    }
   ],
   "source": [
    "# 10.1: Beste Freeze-Params\n",
    "best_freeze = {\"freeze_pct\":0.5,\"learning_rate\":3.23e-05,\"batch_size\":16}\n",
    "# 10.2: Modell laden + einfrieren\n",
    "freeze_model = AutoModelForTokenClassification.from_pretrained(\n",
    "   model_name, num_labels=len(ner_labels), id2label=id2label, label2id=label2id\n",
    ")\n",
    "total_layers = len([n for n,_ in freeze_model.named_parameters() \n",
    "                    if n.startswith(\"bert.encoder.layer.\")])//2\n",
    "cutoff = int(total_layers * best_freeze[\"freeze_pct\"])\n",
    "for name,param in freeze_model.named_parameters():\n",
    "    if name.startswith(\"bert.encoder.layer.\") and int(name.split(\".\")[3])<cutoff:\n",
    "        param.requires_grad=False\n",
    "\n",
    "# 10.3: Trainer\n",
    "freeze_args = TrainingArguments(\n",
    "    output_dir=\"outputs/bert-ner-freeze-final\",\n",
    "    per_device_train_batch_size=best_freeze[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_freeze[\"batch_size\"]*2,\n",
    "    evaluation_strategy=\"steps\", eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=100,\n",
    "    learning_rate=best_freeze[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "freeze_trainer = Trainer(\n",
    "    model=freeze_model,\n",
    "    args=freeze_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "freeze_trainer.train()\n",
    "print(\"âœ… Freeze Dev-F1:\", freeze_trainer.evaluate()[\"eval_f1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acd01414-1531-4ec9-b71a-80a1979ab622",
   "metadata": {},
   "source": [
    "| Model | Method         | Dev-F1\\_EI | Dev-F1\\_EC |\r\n",
    "| ----- | -------------- | ---------- | ---------- |\r\n",
    "| BERT  | Zero-Shot      | 0.0271     | x.xxx      |\r\n",
    "| BERT  | Full-FT (opt)  | 0.3753     | x.xxx      |\r\n",
    "| BERT  | LoRA (opt)     | 0.0271     | x.xxx      |\r\n",
    "| BERT  | Partial-Freeze | 0.0630     | x.xxx      |\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5fce684-a195-42ae-be26-a88e2556545c",
   "metadata": {},
   "source": [
    "## 11. Test-Set Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3f630b6e-3815-4785-90ea-e992a42a8c6e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Test-Predictions gespeichert (insgesamt 248 Dokumente).\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer\n",
    "import json\n",
    "\n",
    "# Tokenizer & Pipeline\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "ner_pipe  = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"outputs/bert-ner-full-ft-opt\",\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0\n",
    ")\n",
    "\n",
    "# Inferenz auf test_docs und Umwandlung der Scores\n",
    "ner_preds = []\n",
    "for d in test_docs:\n",
    "    ents = ner_pipe(d[\"document\"])\n",
    "    # Cast aller score-Felder zu Python float\n",
    "    for ent in ents:\n",
    "        ent[\"score\"] = float(ent[\"score\"])\n",
    "    ner_preds.append({\n",
    "        \"id\": d[\"id\"],\n",
    "        \"entities\": ents\n",
    "    })\n",
    "\n",
    "# Speichern der Predictions\n",
    "with open(\"ner_test_full_ft_bert.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(ner_preds, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"âœ… Test-Predictions gespeichert (insgesamt {len(ner_preds)} Dokumente).\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1908f4a6-79d8-4ba1-8452-2c008873bc8a",
   "metadata": {},
   "source": [
    "## 12. import the test file and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3b2fe69a-bae6-497c-8684-f55bcdbbd5ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Alles vorbereitet:\n",
      "input/res: ['results.json']\n",
      "input/ref: ['reference.json']\n"
     ]
    }
   ],
   "source": [
    "import os, json\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# 1. Ordnerstruktur anlegen\n",
    "os.makedirs(\"input/res\", exist_ok=True)\n",
    "os.makedirs(\"input/ref\", exist_ok=True)\n",
    "os.makedirs(\"output\",   exist_ok=True)\n",
    "\n",
    "# 2. Tokenizer & Pipeline laden\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "ner_pipe  = pipeline(\n",
    "    \"ner\",\n",
    "    model=\"outputs/bert-ner-full-ft-opt\",\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0  # 0 fÃ¼r GPU, -1 fÃ¼r CPU\n",
    ")\n",
    "\n",
    "# 3. Vorhersagen auf dev_docs\n",
    "dev_preds = {}\n",
    "for idx, doc in enumerate(dev_docs):\n",
    "    ents = ner_pipe(doc[\"doc\"])\n",
    "    for ent in ents:\n",
    "        ent[\"score\"] = float(ent[\"score\"])  # JSON-kompatibel machen\n",
    "    dev_preds[str(idx)] = {\n",
    "        \"entities\": ents,\n",
    "        \"triples\":  []  # Falls du spÃ¤ter RE predizierst, kannst du das fÃ¼llen\n",
    "    }\n",
    "\n",
    "# 4. Gold-Daten (reference) vorbereiten\n",
    "gt_dev = {}\n",
    "for idx, doc in enumerate(dev_docs):\n",
    "    gt_dev[str(idx)] = {\n",
    "        \"entities\": doc[\"entities\"],\n",
    "        \"triples\":  doc[\"triples\"]\n",
    "    }\n",
    "\n",
    "# 5. Abspeichern\n",
    "with open(\"input/res/results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dev_preds, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "with open(\"input/ref/reference.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(gt_dev, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"âœ… Alles vorbereitet:\")\n",
    "print(\"input/res:\", os.listdir(\"input/res\"))\n",
    "print(\"input/ref:\", os.listdir(\"input/ref\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cf5f8e3f-9c9f-4a58-9a47-b71229c394fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1_EI (Entity Identification): 0.0000\n",
      "F1_EC (Entity Classification): 0.0000\n"
     ]
    }
   ],
   "source": [
    "!python evaluate_ner.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "425aa4c5-e3ef-4d29-a8d5-3278e2b29da4",
   "metadata": {},
   "source": [
    "## look at resluts.json in the input file for the inference, show screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b23e4a16-b0ed-4490-8bb8-e00cf08ed161",
   "metadata": {},
   "source": [
    "## 13: RE-Daten aus JSON zu train/dev/test Examples aufbereiten"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "b8490c07-ed21-424b-a593-afb28b18c660",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['split', 'sentence', 'head', 'tail', 'label'],\n",
      "        num_rows: 1222\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['split', 'sentence', 'head', 'tail', 'label'],\n",
      "        num_rows: 606\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: [],\n",
      "        num_rows: 0\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "examples = []\n",
    "\n",
    "# Helferfunktion zur Erstellung von positiven und negativen RE-Beispielen\n",
    "def extract_examples(docs, split):\n",
    "    local_examples = []\n",
    "    for doc in docs:\n",
    "        if not doc.get(\"triples\") or not doc.get(\"entities\"):\n",
    "            continue\n",
    "\n",
    "        text = doc.get(\"doc\") or doc.get(\"document\")\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        ents = [e[\"mentions\"][0] for e in doc[\"entities\"] if e.get(\"mentions\")]\n",
    "        true_pairs = {(t[\"head\"], t[\"tail\"]) for t in doc[\"triples\"]}\n",
    "\n",
    "        for triple in doc[\"triples\"]:\n",
    "            # Positives Beispiel\n",
    "            local_examples.append({\n",
    "                \"split\":    split,\n",
    "                \"sentence\": text,\n",
    "                \"head\":     triple[\"head\"],\n",
    "                \"tail\":     triple[\"tail\"],\n",
    "                \"label\":    triple[\"relation\"],\n",
    "            })\n",
    "\n",
    "            # Negativbeispiel: zufÃ¤llige Kombination, die kein Gold-Paar ist\n",
    "            while True:\n",
    "                h, t = random.sample(ents, 2)\n",
    "                if (h, t) not in true_pairs:\n",
    "                    local_examples.append({\n",
    "                        \"split\":    split,\n",
    "                        \"sentence\": text,\n",
    "                        \"head\":     h,\n",
    "                        \"tail\":     t,\n",
    "                        \"label\":    \"no_relation\",\n",
    "                    })\n",
    "                    break\n",
    "    return local_examples\n",
    "\n",
    "# Jetzt mit bestehenden Variablen arbeiten\n",
    "examples.extend(extract_examples(train_docs, \"train\"))\n",
    "examples.extend(extract_examples(dev_docs, \"dev\"))\n",
    "examples.extend(extract_examples(test_docs, \"test\"))\n",
    "\n",
    "# In HuggingFace DatasetDict umwandeln\n",
    "train_ex = [e for e in examples if e[\"split\"] == \"train\"]\n",
    "dev_ex   = [e for e in examples if e[\"split\"] == \"dev\"]\n",
    "test_ex  = [e for e in examples if e[\"split\"] == \"test\"]\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_ex),\n",
    "    \"dev\":   Dataset.from_list(dev_ex),\n",
    "    \"test\":  Dataset.from_list(test_ex),\n",
    "})\n",
    "\n",
    "# Optional: Preview\n",
    "print(ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "c3805d40-949b-43e5-84a7-e35173a7bc0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_re_ds = ds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8dcf32f5-2c27-41fc-9389-cbc1995f14ae",
   "metadata": {},
   "source": [
    "## 14: Tokenisierung & Label-Mapping fÃ¼r RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "a1dd0254-5c48-4317-a0d8-2a7997af50b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "51dad0d20c554fd9908bc2ed40770a8c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3f4b48cf89040d2a988ee864edaa276",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Kapitel 14 (angepasst): Tokenisierung & Label-Mapping fÃ¼r RE ===\n",
    "\n",
    "from transformers import AutoTokenizer\n",
    "import torch\n",
    "\n",
    "\n",
    "\n",
    "# Labels aus Originaldaten extrahieren\n",
    "original_train_labels = [ex[\"label\"] for ex in raw_re_ds[\"train\"]]\n",
    "original_dev_labels = [ex[\"label\"] for ex in raw_re_ds[\"dev\"]]\n",
    "all_labels = sorted(set(original_train_labels + original_dev_labels))\n",
    "\n",
    "# Label-Mapping\n",
    "label2id_re = {lab: i for i, lab in enumerate(all_labels)}\n",
    "id2label_re = {i: lab for lab, i in label2id_re.items()}\n",
    "\n",
    "\n",
    "# 14.2: Tokenizer laden\n",
    "tokenizer_re = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "max_length_re = 128\n",
    "\n",
    "# 14.3: Tokenisierungsfunktion\n",
    "def tokenize_re(example):\n",
    "    encoded = tokenizer_re(\n",
    "        example[\"head\"],\n",
    "        example[\"tail\"] + \" \" + example[\"sentence\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length_re,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    encoded[\"labels\"] = label2id_re[example[\"label\"]]\n",
    "    return encoded\n",
    "\n",
    "# 14.4: Nur auf Splits anwenden, die NICHT leer sind und noch die originalen Spalten enthalten\n",
    "for split in ds:\n",
    "    if len(ds[split]) > 0 and \"label\" in ds[split].column_names:\n",
    "        ds[split] = ds[split].map(tokenize_re, batched=False, remove_columns=ds[split].column_names)\n",
    "\n",
    "# 14.5: PyTorch-Format setzen (fÃ¼r bereits gemappte Splits)\n",
    "for split in ds:\n",
    "    if len(ds[split]) > 0:\n",
    "        ds[split].set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd4dfc40-6240-4d07-9085-6d0455676dfe",
   "metadata": {},
   "source": [
    "## 15. BERT RE Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "5bb0e0bf-6d50-4baa-aba2-c02e7a2dc01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_131988\\1694536596.py:36: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_re = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='459' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [459/459 12:12, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.412700</td>\n",
       "      <td>2.501379</td>\n",
       "      <td>0.021458</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.139700</td>\n",
       "      <td>2.415065</td>\n",
       "      <td>0.033357</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.759900</td>\n",
       "      <td>2.419560</td>\n",
       "      <td>0.033919</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”– RE Baseline Dev-F1: 0.0339194749216301\n"
     ]
    }
   ],
   "source": [
    "# === Kapitel 15: RE â€“ Baseline-Training mit BERT (Full Fine-Tuning) ===\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 15.1: Modell laden mit passender Labelanzahl\n",
    "model_re = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label2id_re),\n",
    "    id2label=id2label_re,\n",
    "    label2id=label2id_re\n",
    ")\n",
    "\n",
    "# 15.2: Trainingsargumente definieren\n",
    "training_args_re = TrainingArguments(\n",
    "    output_dir=\"outputs/bert-re-baseline\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available()  # nur aktiv, wenn CUDA verfÃ¼gbar\n",
    ")\n",
    "\n",
    "# 15.3: F1-Metrik definieren\n",
    "def compute_metrics_re(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    return {\"eval_f1\": f1_score(p.label_ids, preds, average=\"macro\")}\n",
    "\n",
    "# 15.4: Trainer initialisieren\n",
    "trainer_re = Trainer(\n",
    "    model=model_re,\n",
    "    args=training_args_re,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"dev\"],\n",
    "    tokenizer=tokenizer_re,\n",
    "    compute_metrics=compute_metrics_re\n",
    ")\n",
    "\n",
    "# 15.5: Training starten\n",
    "trainer_re.train()\n",
    "\n",
    "# 15.6: Evaluieren\n",
    "metrics_re = trainer_re.evaluate()\n",
    "print(\"ðŸ”– RE Baseline Dev-F1:\", metrics_re[\"eval_f1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec27f978-1540-497e-bdee-bcdd4627f367",
   "metadata": {},
   "source": [
    "## 16: FUll fine tuning Bert RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "751e30d0-e251-434a-9e15-2cc0bbaf59cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_131988\\3554616472.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_re_ft = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:57, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.001500</td>\n",
       "      <td>3.485520</td>\n",
       "      <td>0.020168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.045400</td>\n",
       "      <td>2.861245</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.019500</td>\n",
       "      <td>2.796352</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.739400</td>\n",
       "      <td>2.719007</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.601900</td>\n",
       "      <td>2.673312</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.541600</td>\n",
       "      <td>2.631195</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.490100</td>\n",
       "      <td>2.616581</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.394900</td>\n",
       "      <td>2.564026</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.201300</td>\n",
       "      <td>2.555000</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.000500</td>\n",
       "      <td>2.549991</td>\n",
       "      <td>0.021505</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 1. Modell mit Label-Mappings\n",
    "model_re_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label2id_re),\n",
    "    id2label=id2label_re,\n",
    "    label2id=label2id_re\n",
    ")\n",
    "\n",
    "# 2. Beste Hyperparameter (aus deinem Tuning)\n",
    "best_ft = {\n",
    "    \"learning_rate\": 4.3575643120387554e-05,\n",
    "    \"batch_size\": 16\n",
    "}\n",
    "\n",
    "# 3. Trainingsargumente (â†’ mit Speicherpfad)\n",
    "training_args_ft = TrainingArguments(\n",
    "    output_dir=\"outputs/re-bert-final-ft\",  # << output name geÃ¤ndert\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",  # speichert am Ende jeder Epoche\n",
    "    save_total_limit=1,     # nur letztes Modell behalten\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=best_ft[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_ft[\"batch_size\"] * 2,\n",
    "    learning_rate=best_ft[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# 4. F1-Metrik\n",
    "def compute_metrics_ft(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    return {\"eval_f1\": f1_score(p.label_ids, preds, average=\"macro\")}\n",
    "\n",
    "# 5. Trainer\n",
    "trainer_re_ft = Trainer(\n",
    "    model=model_re_ft,\n",
    "    args=training_args_ft,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"dev\"],\n",
    "    tokenizer=tokenizer_re,\n",
    "    compute_metrics=compute_metrics_ft\n",
    ")\n",
    "\n",
    "# 6. Training starten\n",
    "trainer_re_ft.train()\n",
    "\n",
    "# 7. Modell speichern (optional manuell â€“ falls nicht automatisch)\n",
    "trainer_re_ft.save_model(\"outputs/re-bert-final-ft\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c449a0-faad-4d60-95b6-3beb942ccee7",
   "metadata": {},
   "source": [
    "## What Does a Micro-F1 Score of 0.5 Really Mean in RE?\n",
    "In our relation extraction task, the \"no_relation\" label is assigned to randomly paired entity mentions that do not have a meaningful relationship. Since these negative examples are added for every positive example (1:1), the \"no_relation\" class becomes the most frequent class in the dataset.\n",
    "\n",
    "This introduces a critical imbalance:\n",
    "A model that simply predicts \"no_relation\" for every input can still achieve a micro-F1 score of ~0.5, even though it fails to capture any true relationships.\n",
    "\n",
    "Therefore:\n",
    "\n",
    "A micro-F1 of 0.5 does not indicate meaningful learning.\n",
    "\n",
    "The macro-F1 score, which treats all classes equally, remains very low (e.g., ~0.02), reflecting the model's poor performance on actual relation classes.\n",
    "\n",
    "In summary, micro-F1 in this context can be misleading and should always be interpreted alongside macro-F1 and qualitative examples."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "699e880d-7b4e-44aa-964e-3f782057b1ef",
   "metadata": {},
   "source": [
    "## ðŸ” Why F1 Scores Are Higher in NER Than in RE (Relation Extraction)\r\n",
    "\r\n",
    "It is completely normal for your model to achieve **higher F1 scores in Named Entity Recognition (NER)** than in **Relation Extraction (RE)**. Hereâ€™s why:\r\n",
    "\r\n",
    "| Aspect                  | NER                                                  | RE                                                       |\r\n",
    "|-------------------------|------------------------------------------------------|-----------------------------------------------------------|\r\n",
    "| **Task**                | Detect entities in a sentence                        | Identify semantic relationships between two entities      |\r\n",
    "| **Complexity**          | Easier â€“ Local information is sufficient             | Harder â€“ Requires understanding of sentence-level context |\r\n",
    "| **Negative Examples**   | Rare or implicit                                     | Abundant due to many \"no_relation\" pairs                  |\r\n",
    "| **Model Behavior**      | Learns boundary/entity types well                    | Struggles to semantically distinguish true relations      |\r\n",
    "| **Typical F1**          | ~0.3â€“0.8 depending on model/data                     | Often very low (<0.2) without optimization                |\r\n",
    "| **Baseline**            | Random is near 0                                     | Random can reach â‰ˆ 0.5 if \"no_relation\" is dominant       |\r\n",
    "\r\n",
    "### ðŸ” Summary:\r\n",
    "NER is generally a simpler task for pre-trained models like BERT. In RE, the model must understand **complex dependencies** between multiple parts of the sentence and distinguish fine-grained relations from the dominant \"no_relation\" class.\r\n",
    "\r\n",
    "Hence, even if your model achieves **F1 = 0.5 in NER** but only **F1 = 0.03 in RE**, that is not unusual â€” it simply reflects the **increased difficulty** of the RE task.\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fcb664f-eab7-47f6-80e4-9c109206c0e6",
   "metadata": {},
   "source": [
    "### ðŸ”§ Hyperparameter Tuning (Method)\n",
    "\n",
    "We used Optuna to tune the most important hyperparameters for each fine-tuning strategy:\n",
    "\n",
    "- Full Fine-Tuning\n",
    "- LoRA\n",
    "- Partial Freezing\n",
    "\n",
    "The tuning was performed independently for:\n",
    "- Named Entity Recognition (NER)\n",
    "- Relation Extraction (RE)\n",
    "\n",
    "The same optimization loop was used for all experiments.\n",
    "\n",
    "#### Example: Tuning Loop (used across all models)\n",
    "\n",
    "```python\n",
    "def objective(trial):\n",
    "    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 5e-4, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    # ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "65e5f973-bbb2-4886-80ab-254bbc71422f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Predictions saved to re_bert_results.json\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# Load tokenizer and model\n",
    "tokenizer_re = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\"outputs/re-bert-final-ft\")\n",
    "model.eval()\n",
    "\n",
    "# Load input samples (adjust path if needed)\n",
    "with open(\"re_bert_input_samples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_inputs = json.load(f)\n",
    "\n",
    "# Predict relations\n",
    "results = []\n",
    "for ex in test_inputs:\n",
    "    encoded = tokenizer_re(\n",
    "        ex[\"head\"],\n",
    "        ex[\"tail\"] + \" \" + ex[\"sentence\"],\n",
    "        return_tensors=\"pt\",\n",
    "        truncation=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "\n",
    "    with torch.no_grad():\n",
    "        output = model(**encoded)\n",
    "        probs = torch.nn.functional.softmax(output.logits, dim=-1).squeeze()\n",
    "        pred_id = probs.argmax().item()\n",
    "        pred_label = model.config.id2label[pred_id]\n",
    "        score = probs[pred_id].item()\n",
    "\n",
    "    results.append({\n",
    "        \"head\": ex[\"head\"],\n",
    "        \"tail\": ex[\"tail\"],\n",
    "        \"relation\": pred_label,\n",
    "        \"score\": round(score, 4)\n",
    "    })\n",
    "\n",
    "# Save results for visualization or evaluation\n",
    "with open(\"re_bert_results.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(\"âœ… Predictions saved to re_bert_results.json\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "e66a25e0-3614-4594-b34f-396c559cfbab",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"re_bert_input_samples.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    test_inputs = json.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2055d9b1-fd23-4375-bc29-45dac463a2cc",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
