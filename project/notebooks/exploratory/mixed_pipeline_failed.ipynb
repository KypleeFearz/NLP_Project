{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b60645b5-32c1-421f-8faa-f73eafa78915",
   "metadata": {},
   "source": [
    "## 2. Importe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6d78889-f932-4931-aaf8-e8718cf40cd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import re\n",
    "import random\n",
    "import json\n",
    "from pathlib import Path\n",
    "from datasets import Dataset\n",
    "\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForCausalLM,\n",
    "    Trainer,\n",
    "    TrainingArguments,\n",
    "    DataCollatorForTokenClassification,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import get_peft_config, get_peft_model, LoraConfig\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.metrics import precision_recall_fscore_support, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac39e70d-17cb-4141-9798-951b1d309bc1",
   "metadata": {},
   "source": [
    "## 3 paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "af297301-9a38-4b85-b7cc-37c3b0ca96a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deine lokalen Windows-Pfade\n",
    "TRAIN_DIR = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\dataset\\train\")\n",
    "DEV_DIR   = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\dataset\\dev\")\n",
    "TEST_DIR  = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\dataset\\test\")\n",
    "\n",
    "assert TRAIN_DIR.exists(), f\"Train-Ordner nicht gefunden: {TRAIN_DIR}\"\n",
    "assert DEV_DIR.exists(),   f\"Dev-Ordner nicht gefunden:   {DEV_DIR}\"\n",
    "assert TEST_DIR.exists(),  f\"Test-Ordner nicht gefunden:  {TEST_DIR}\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb67a4d-236a-4f56-841e-1ba9c947a800",
   "metadata": {},
   "source": [
    "## 4 load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19153ad9-d2ca-4b34-8a55-f5323b690a98",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train: 51 │ Dev: 23 │ Test: 248\n"
     ]
    }
   ],
   "source": [
    "def load_docie_docs(folder: Path, recursive: bool = False):\n",
    "    docs = []\n",
    "    pattern = \"**/*.json\" if recursive else \"*.json\"\n",
    "    for file in folder.glob(pattern):\n",
    "        data = json.loads(file.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(data, list):\n",
    "            docs.extend(data)\n",
    "        else:\n",
    "            docs.append(data)\n",
    "    return docs\n",
    "\n",
    "train_docs = load_docie_docs(TRAIN_DIR)\n",
    "dev_docs   = load_docie_docs(DEV_DIR)\n",
    "test_docs  = load_docie_docs(TEST_DIR, recursive=True)\n",
    "\n",
    "print(\"Train:\", len(train_docs), \"│ Dev:\", len(dev_docs), \"│ Test:\", len(test_docs))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "75cc32a9-8a43-41ad-9d9b-58e4c3e745c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['domain', 'document', 'RE_label_set', 'NER_label_set', 'id'])\n"
     ]
    }
   ],
   "source": [
    "# -> new cell right after you do test_docs = load_docie_docs(...)\n",
    "print(test_docs[0].keys())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2beb7d-80e7-4bd9-a6ef-2ecfa132e38f",
   "metadata": {},
   "source": [
    "## 5. Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2358178f-cb9f-4234-87f4-c0eb066acd4b",
   "metadata": {},
   "source": [
    "## 5.1 doc length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e5267a66-c8af-4513-9f27-a05a6117f0b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Avg Tokens: 919.0784313725491 Max Tokens: 2560\n"
     ]
    }
   ],
   "source": [
    "lengths = [len(doc[\"doc\"].split()) for doc in train_docs]\n",
    "print(\"Avg Tokens:\", np.mean(lengths), \"Max Tokens:\", np.max(lengths))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a50daac9-a531-46f0-a191-f7fdba046759",
   "metadata": {},
   "source": [
    "## 5.2 Entity split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b285a2e1-4d7c-445d-9aa7-320e354f0702",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity-Typen: [('DATE', 647), ('MISC', 417), ('PERSON', 242), ('ORG', 241), ('CARDINAL', 224), ('GPE', 157), ('WORK_OF_ART', 65), ('NORP', 59), ('ORDINAL', 55), ('QUANTITY', 42), ('EVENT', 35), ('PRODUCT', 30), ('FAC', 30), ('MONEY', 29), ('PERCENT', 28), ('LOC', 24), ('LANGUAGE', 10), ('LAW', 9), ('TIME', 8)]\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "ctr = Counter(ent[\"type\"] for doc in train_docs for ent in doc[\"entities\"])\n",
    "print(\"Entity-Typen:\", ctr.most_common())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c8c0f7c-6f4a-491d-9fed-a7dd296fce28",
   "metadata": {},
   "source": [
    "## 5.3 Relation split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c934f612-6bdb-426b-b209-65107ebf7dea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Relation-Typen: [('HasPart', 82), ('HasEffect', 67), ('DiplomaticRelation', 45), ('LocatedIn', 44), ('InterestedIn', 38), ('OwnerOf', 32), ('NominatedFor', 25), ('SaidToBeTheSameAs', 25), ('PartOf', 18), ('Creator', 17), ('Founded', 13), ('Country', 13), ('DifferentFrom', 11), ('SignificantEvent', 11), ('PrimeFactor', 11), ('InfluencedBy', 10), ('Follows', 10), ('UsedBy', 9), ('InspiredBy', 9), ('Uses', 8), ('FollowedBy', 8), ('SharesBorderWith', 8), ('AdjacentStation', 7), ('HasWorksInTheCollection', 6), ('PositionHeld', 6), ('OfficialLanguage', 5), ('Studies', 4), ('WorkLocation', 4), ('PracticedBy', 4), ('AcademicDegree', 3), ('Author', 3), ('CountryOfCitizenship', 3), ('EducatedAt', 3), ('LanguageUsed', 3), ('IssuedBy', 3), ('Affiliation', 2), ('MemberOf', 2), ('ApprovedBy', 2), ('Continent', 2), ('OwnedBy', 2), ('Location', 2), ('LanguageOfWorkOrName', 2), ('NativeLanguage', 2), ('OriginalLanguageOfFilmOrTvShow', 2), ('Employer', 2), ('AppliesToPeople', 1), ('HasQuality', 1), ('PresentedIn', 1), ('BasedOn', 1), ('RegulatedBy', 1), ('ContainsAdministrativeTerritorialEntity', 1), ('Promoted', 1), ('NamedAfter', 1), ('Replaces', 1), ('InOppositionTo', 1), ('Partner', 1), ('PhysicallyInteractsWith', 1), ('ContainsTheAdministrativeTerritorialEntity', 1), ('ParentOrganization', 1), ('PartyChiefRepresentative', 1), ('Capital', 1), ('TwinnedAdministrativeBody', 1), ('Director', 1), ('PlaceOfBirth', 1), ('OperatingSystem', 1), ('CitesWork', 1), ('PublishedIn', 1), ('Developer', 1)]\n"
     ]
    }
   ],
   "source": [
    "ctr_rel = Counter(t[\"relation\"] for doc in train_docs for t in doc[\"triples\"])\n",
    "print(\"Relation-Typen:\", ctr_rel.most_common())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ff2526-f81c-4111-b768-8513237a9e4a",
   "metadata": {},
   "source": [
    "## 6. Label-Mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "fb81396e-2226-444c-821d-f0aa09a52491",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl NER-Labels: 39\n",
      "label2id['O'] = 0\n"
     ]
    }
   ],
   "source": [
    "entity_types = train_docs[0][\"entity_label_set\"]\n",
    "\n",
    "ner_labels = [\"O\"]\n",
    "for t in entity_types:\n",
    "    ner_labels += [f\"B-{t}\", f\"I-{t}\"]\n",
    "\n",
    "label2id = {lab: i for i, lab in enumerate(ner_labels)}\n",
    "id2label = {i: lab for lab, i in label2id.items()}\n",
    "\n",
    "print(\"Anzahl NER-Labels:\", len(ner_labels))\n",
    "print(\"label2id['O'] =\", label2id[\"O\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98c0b43-0ccb-4737-bddd-25b7a32acb06",
   "metadata": {},
   "source": [
    "## 7. load tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8f19c049-49cb-4402-baac-fa11555a0c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb82ed83-d082-497c-b8d7-41e484f7431b",
   "metadata": {},
   "source": [
    "## 8. HF-Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3f285386-225b-4b23-864b-729358db6879",
   "metadata": {},
   "outputs": [],
   "source": [
    "hf_train = Dataset.from_list(train_docs)\n",
    "hf_dev   = Dataset.from_list(dev_docs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6441289b-3bf8-4274-948f-656195c30236",
   "metadata": {},
   "source": [
    "## 9. encode_with_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "025d1767-1b55-4ec3-8e98-84be7411e884",
   "metadata": {},
   "outputs": [],
   "source": [
    "max_length = 512\n",
    "stride     = 128\n",
    "\n",
    "# 9.2 Funktion für batched map\n",
    "def tokenize_and_align_labels(examples):\n",
    "    all_input_ids      = []\n",
    "    all_attention_mask = []\n",
    "    all_labels         = []\n",
    "\n",
    "    for doc, entities in zip(examples[\"doc\"], examples[\"entities\"]):\n",
    "        # Tokenize mit Overflow & Stride\n",
    "        tokenized = tokenizer(\n",
    "            doc,\n",
    "            return_offsets_mapping=True,\n",
    "            truncation=True,\n",
    "            max_length=max_length,\n",
    "            stride=stride,\n",
    "            return_overflowing_tokens=True,\n",
    "        )\n",
    "\n",
    "        # Pro Chunk ein neues Beispiel erzeugen\n",
    "        for i in range(len(tokenized[\"input_ids\"])):\n",
    "            offsets       = tokenized[\"offset_mapping\"][i]\n",
    "            input_ids     = tokenized[\"input_ids\"][i]\n",
    "            attention_mask= tokenized[\"attention_mask\"][i]\n",
    "\n",
    "            # 9.2.1 O-Labels initialisieren\n",
    "            chunk_labels = [\"O\"] * len(offsets)\n",
    "\n",
    "            # 9.2.2 Mentions labeln\n",
    "            for ent in entities:\n",
    "                ent_type = ent[\"type\"]\n",
    "                for mention in ent[\"mentions\"]:\n",
    "                    start = doc.find(mention)\n",
    "                    if start < 0: \n",
    "                        continue\n",
    "                    end = start + len(mention)\n",
    "                    for idx, (o_start, o_end) in enumerate(offsets):\n",
    "                        if o_start >= start and o_end <= end:\n",
    "                            prefix = \"B\" if o_start == start else \"I\"\n",
    "                            chunk_labels[idx] = f\"{prefix}-{ent_type}\"\n",
    "\n",
    "            # 9.2.3 Labels → IDs\n",
    "            label_ids = [ label2id.get(lab, label2id[\"O\"]) for lab in chunk_labels ]\n",
    "\n",
    "            # 9.2.4 Ansammln\n",
    "            all_input_ids.append(input_ids)\n",
    "            all_attention_mask.append(attention_mask)\n",
    "            all_labels.append(label_ids)\n",
    "\n",
    "    return {\n",
    "        \"input_ids\": all_input_ids,\n",
    "        \"attention_mask\": all_attention_mask,\n",
    "        \"labels\": all_labels,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86302ab8-a368-42fe-934e-2cc690bc53ad",
   "metadata": {},
   "source": [
    "## 10. tokenization & Label-Alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b75fa31-81d1-4f2c-be6f-dd1491a2ae1d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "27baf0615e3e49169a5e78e1fe5d6261",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/51 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ffbf4edb47654d9aa6b4edb6edba16b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/23 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 10.1 Original-Spalten, die wir nicht mehr brauchen\n",
    "cols_to_remove = [\n",
    "    \"domain\",\"title\",\"doc\",\"entities\",\"triples\",\n",
    "    \"label_set\",\"entity_label_set\"\n",
    "]\n",
    "\n",
    "# 10.2 Batched map mit Flattening\n",
    "hf_train = hf_train.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove,\n",
    ")\n",
    "\n",
    "hf_dev = hf_dev.map(\n",
    "    tokenize_and_align_labels,\n",
    "    batched=True,\n",
    "    remove_columns=cols_to_remove,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0332310-93f5-4b8b-a622-0c647b85d79d",
   "metadata": {},
   "source": [
    "## 11. DataCollator & metrtics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b1ba49a6-ab46-4f7d-b424-7e4af3d6b98f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "864dd4f8-1193-4ddb-913f-6baabbe2cc32",
   "metadata": {},
   "source": [
    "### 11.2 metrics function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b5af0d68-cc7b-4e2b-bed4-99925c58042c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics_entity_only(pred):\n",
    "    preds  = pred.predictions.argmax(-1).flatten()\n",
    "    labels = pred.label_ids.flatten()\n",
    "    mask = (labels != label2id[\"O\"]) & (labels != -100)\n",
    "    if mask.sum() == 0:\n",
    "        return {\"precision\": 0.0, \"recall\": 0.0, \"f1\": 0.0}\n",
    "    p, r, f1, _ = precision_recall_fscore_support(\n",
    "        labels[mask], preds[mask], average=\"micro\"\n",
    "    )\n",
    "    return {\"precision\": p, \"recall\": r, \"f1\": f1}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc88152d-5b18-4896-a3d9-dc8676e5be37",
   "metadata": {},
   "source": [
    "## 12. Trainer & Smoke-Run"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "644f0018-c203-4142-b65a-a2d0455c1b08",
   "metadata": {},
   "source": [
    "## 20. Final test set bert evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "59524399-f6e3-471c-8c51-ac3bd09cbedf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['domain', 'document', 'RE_label_set', 'NER_label_set', 'id'])\n"
     ]
    }
   ],
   "source": [
    "print(test_docs[0].keys())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "31f5c29e-eb53-4703-b5d1-9284d6247a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anzahl Test-Dokumente: 248\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\outputs\\bert-ner-full-ft-opt with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForTokenClassification'>). See the original errors:\n\nwhile loading with AutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 573, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 272, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4317, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 982, in _get_resolved_checkpoint_files\n    raise EnvironmentError(\nOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\outputs\\bert-ner-full-ft-opt.\n\nwhile loading with TFAutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 573, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2797, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named tf_model.h5, model.safetensors or pytorch_model.bin found in directory C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\outputs\\bert-ner-full-ft-opt.\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[162], line 39\u001b[0m\n\u001b[0;32m     36\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(base_model_name, use_fast\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;66;03m# Erstelle die NER-Pipeline, die die Gewichte aus model_dir lädt\u001b[39;00m\n\u001b[1;32m---> 39\u001b[0m ner_pipe \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[0;32m     40\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     41\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mstr\u001b[39m(model_dir),\n\u001b[0;32m     42\u001b[0m     tokenizer\u001b[38;5;241m=\u001b[39mtokenizer,\n\u001b[0;32m     43\u001b[0m     aggregation_strategy\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msimple\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     44\u001b[0m     device\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m   \u001b[38;5;66;03m# GPU 0, für CPU -> device=-1\u001b[39;00m\n\u001b[0;32m     45\u001b[0m )\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# ——————————————————————————————————————————————\u001b[39;00m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# 3) Inferenz & Speicherung\u001b[39;00m\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# ——————————————————————————————————————————————\u001b[39;00m\n\u001b[0;32m     50\u001b[0m ner_results \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\__init__.py:942\u001b[0m, in \u001b[0;36mpipeline\u001b[1;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[0;32m    940\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    941\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m--> 942\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[0;32m    943\u001b[0m         adapter_path \u001b[38;5;28;01mif\u001b[39;00m adapter_path \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m model,\n\u001b[0;32m    944\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[0;32m    945\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[0;32m    946\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[0;32m    947\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[0;32m    948\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[0;32m    949\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[0;32m    950\u001b[0m     )\n\u001b[0;32m    952\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[0;32m    953\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py:304\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[1;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[0;32m    302\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m    303\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 304\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    305\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    306\u001b[0m         )\n\u001b[0;32m    308\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    309\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: Could not load model C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\outputs\\bert-ner-full-ft-opt with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForTokenClassification'>, <class 'transformers.models.auto.modeling_tf_auto.TFAutoModelForTokenClassification'>). See the original errors:\n\nwhile loading with AutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 573, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 272, in _wrapper\n    return func(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 4317, in from_pretrained\n    checkpoint_files, sharded_metadata = _get_resolved_checkpoint_files(\n                                         ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\modeling_utils.py\", line 982, in _get_resolved_checkpoint_files\n    raise EnvironmentError(\nOSError: Error no file named pytorch_model.bin, model.safetensors, tf_model.h5, model.ckpt.index or flax_model.msgpack found in directory C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\outputs\\bert-ner-full-ft-opt.\n\nwhile loading with TFAutoModelForTokenClassification, an error is thrown:\nTraceback (most recent call last):\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\pipelines\\base.py\", line 291, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\models\\auto\\auto_factory.py\", line 573, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\modeling_tf_utils.py\", line 2797, in from_pretrained\n    raise EnvironmentError(\nOSError: Error no file named tf_model.h5, model.safetensors or pytorch_model.bin found in directory C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\outputs\\bert-ner-full-ft-opt.\n\n\n"
     ]
    }
   ],
   "source": [
    "# === Kapitel 20: NER-Inferenz auf dem Test-Set mit dem besten Full-FT-BERT-Modell ===\n",
    "\n",
    "import json\n",
    "from pathlib import Path\n",
    "from transformers import pipeline, AutoTokenizer\n",
    "\n",
    "# ——————————————————————————————————————————————\n",
    "# 1) Test-Daten laden\n",
    "# ——————————————————————————————————————————————\n",
    "TEST_DIR = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\dataset\\test\")\n",
    "\n",
    "def load_docie_docs(folder: Path, recursive: bool = False):\n",
    "    docs = []\n",
    "    pattern = \"**/*.json\" if recursive else \"*.json\"\n",
    "    for file in folder.glob(pattern):\n",
    "        data = json.loads(file.read_text(encoding=\"utf-8\"))\n",
    "        if isinstance(data, list):\n",
    "            docs.extend(data)\n",
    "        else:\n",
    "            docs.append(data)\n",
    "    return docs\n",
    "\n",
    "test_docs = load_docie_docs(TEST_DIR, recursive=True)\n",
    "print(f\"Anzahl Test-Dokumente: {len(test_docs)}\")\n",
    "\n",
    "# ——————————————————————————————————————————————\n",
    "# 2) Tokenizer & Pipeline einrichten\n",
    "# ——————————————————————————————————————————————\n",
    "# Basis-Model-Name (Tokenizer kommt hierher)\n",
    "base_model_name = \"bert-base-uncased\"\n",
    "\n",
    "# Checkpoint-Ordner, den dein Trainer mit den besten Full-FT-Gewichten gefüllt hat\n",
    "model_dir = Path(r\"C:\\Users\\nmilo\\OneDrive\\Desktop\\Master\\Semester2\\NLP\\project\\outputs\\bert-ner-full-ft-opt\")\n",
    "\n",
    "# Lade den Tokenizer aus dem Basismodell\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_name, use_fast=True)\n",
    "\n",
    "# Erstelle die NER-Pipeline, die die Gewichte aus model_dir lädt\n",
    "ner_pipe = pipeline(\n",
    "    \"ner\",\n",
    "    model=str(model_dir),\n",
    "    tokenizer=tokenizer,\n",
    "    aggregation_strategy=\"simple\",\n",
    "    device=0   # GPU 0, für CPU -> device=-1\n",
    ")\n",
    "\n",
    "# ——————————————————————————————————————————————\n",
    "# 3) Inferenz & Speicherung\n",
    "# ——————————————————————————————————————————————\n",
    "ner_results = []\n",
    "for doc in test_docs:\n",
    "    # Doc-ID unter \"id\", Text unter \"document\"\n",
    "    entities = ner_pipe(doc[\"document\"])\n",
    "    ner_results.append({\n",
    "        \"id\": doc[\"id\"],\n",
    "        \"entities\": entities\n",
    "    })\n",
    "\n",
    "# Speichern\n",
    "with open(\"ner_test_preds_full_ft_bert.json\", \"w\", encoding=\"utf-8\") as fout:\n",
    "    json.dump(ner_results, fout, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(\"✅ NER-Predictions gespeichert in ner_test_preds_full_ft_bert.json\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51406937-f0ae-4ba2-a7e2-9064c42c5908",
   "metadata": {},
   "source": [
    "## 21: Setup für GPT-J"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "657e9a65-a841-44f2-8778-21f392baf051",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 21.0 Load GPT-Neo tokenizer & model first\n",
    "model_name = \"EleutherAI/gpt-neo-125M\"\n",
    "tokenizer  = AutoTokenizer.from_pretrained(model_name, use_fast=True)\n",
    "model      = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 21.1 Guarantee a PAD token exists and update the model\n",
    "if tokenizer.pad_token is None:\n",
    "    # 1) add a “[PAD]” token\n",
    "    tokenizer.add_special_tokens({\"pad_token\": \"[PAD]\"})\n",
    "    # 2) resize model embeddings\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "    # 3) tell the model to use that pad token\n",
    "    model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# 21.2 DataCollator for token classification\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# 21.3 Compute–Metrics unchanged\n",
    "def compute_metrics_entity_only(p):\n",
    "    preds = p.predictions.argmax(-1).reshape(-1)\n",
    "    labels = p.label_ids.reshape(-1)\n",
    "    mask   = labels >= 0\n",
    "    p_, r_, f_, _ = precision_recall_fscore_support(\n",
    "        labels[mask], preds[mask], average=\"micro\"\n",
    "    )\n",
    "    return {\"precision\": p_, \"recall\": r_, \"f1\": f_}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0439d9b2-5aed-4b22-bbdc-42489e289b94",
   "metadata": {},
   "source": [
    "## Chapter 22: GPT-Neo Smoke-Run Baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "6913ac2e-dc11-46c1-8c8c-019560aa771f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1913912884.py:18: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  neo_baseline_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='33' max='33' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [33/33 08:01, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.087694</td>\n",
       "      <td>0.792060</td>\n",
       "      <td>0.792060</td>\n",
       "      <td>0.792060</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.210977</td>\n",
       "      <td>0.743442</td>\n",
       "      <td>0.743442</td>\n",
       "      <td>0.743442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.484688</td>\n",
       "      <td>0.668711</td>\n",
       "      <td>0.668711</td>\n",
       "      <td>0.668711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔖 GPT-Neo Baseline Dev-F1 (3 Epochen): 0.6687107564110087\n"
     ]
    }
   ],
   "source": [
    "# === Kapitel 22 (Neo-Baseline im BERT-Stil) ===\n",
    "from transformers import TrainingArguments\n",
    "\n",
    "neo_baseline_args = TrainingArguments(\n",
    "    output_dir=\"outputs/gptneo-ner-baseline\",\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=16,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=2,\n",
    "    num_train_epochs=3,\n",
    "    learning_rate=3e-3,\n",
    "    weight_decay=0.0,\n",
    "    fp16=torch.cuda.is_available(),\n",
    "    save_only_model=True,\n",
    ")\n",
    "\n",
    "neo_baseline_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=neo_baseline_args,\n",
    "    train_dataset=hf_train,\n",
    "    eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "\n",
    "neo_baseline_trainer.train()\n",
    "baseline_metrics_neo = neo_baseline_trainer.evaluate()\n",
    "print(\"🔖 GPT-Neo Baseline Dev-F1 (3 Epochen):\", baseline_metrics_neo[\"eval_f1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a811e985-9ca3-417d-9604-262ec4d5ee4c",
   "metadata": {},
   "source": [
    "## Chapter 23: Hyperparameter-Tuning Full Fine-Tuning for GPT-Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "cb4c3480-5984-4f4e-9a98-4a0a99e192ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 01:29:23,012] A new study created in memory with name: no-name-a381adde-6e59-40bc-8ea0-daa083131c8e\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2300739685.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 07:49, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.680027</td>\n",
       "      <td>0.882108</td>\n",
       "      <td>0.882108</td>\n",
       "      <td>0.882108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636975</td>\n",
       "      <td>0.883370</td>\n",
       "      <td>0.883370</td>\n",
       "      <td>0.883370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.634295</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636623</td>\n",
       "      <td>0.882959</td>\n",
       "      <td>0.882959</td>\n",
       "      <td>0.882959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.625744</td>\n",
       "      <td>0.882959</td>\n",
       "      <td>0.882959</td>\n",
       "      <td>0.882959</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 01:37:40,447] Trial 0 finished with value: 0.8829587465524323 and parameters: {'learning_rate': 3.782759033636916e-05, 'batch_size': 4}. Best is trial 0 with value: 0.8829587465524323.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2300739685.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 25:04, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.772614</td>\n",
       "      <td>0.875682</td>\n",
       "      <td>0.875682</td>\n",
       "      <td>0.875682</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.680847</td>\n",
       "      <td>0.882167</td>\n",
       "      <td>0.882167</td>\n",
       "      <td>0.882167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.652842</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.882812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.649110</td>\n",
       "      <td>0.882871</td>\n",
       "      <td>0.882871</td>\n",
       "      <td>0.882871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.647396</td>\n",
       "      <td>0.882871</td>\n",
       "      <td>0.882871</td>\n",
       "      <td>0.882871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 02:03:27,447] Trial 1 finished with value: 0.8828707235490875 and parameters: {'learning_rate': 1.182318193795246e-05, 'batch_size': 16}. Best is trial 0 with value: 0.8829587465524323.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2300739685.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 22:44, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.689386</td>\n",
       "      <td>0.882343</td>\n",
       "      <td>0.882343</td>\n",
       "      <td>0.882343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.648881</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.882929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.634180</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.882929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.634273</td>\n",
       "      <td>0.882959</td>\n",
       "      <td>0.882959</td>\n",
       "      <td>0.882959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.628873</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 02:26:50,582] Trial 2 finished with value: 0.8829880875535473 and parameters: {'learning_rate': 2.7233372871192413e-05, 'batch_size': 16}. Best is trial 2 with value: 0.8829880875535473.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2300739685.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 12:18, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.804544</td>\n",
       "      <td>0.875213</td>\n",
       "      <td>0.875213</td>\n",
       "      <td>0.875213</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.707226</td>\n",
       "      <td>0.880699</td>\n",
       "      <td>0.880699</td>\n",
       "      <td>0.880699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.680277</td>\n",
       "      <td>0.881902</td>\n",
       "      <td>0.881902</td>\n",
       "      <td>0.881902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.668567</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.882255</td>\n",
       "      <td>0.882255</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.668243</td>\n",
       "      <td>0.882343</td>\n",
       "      <td>0.882343</td>\n",
       "      <td>0.882343</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 02:39:40,217] Trial 3 finished with value: 0.8823425855290182 and parameters: {'learning_rate': 1.0562315509311206e-05, 'batch_size': 8}. Best is trial 2 with value: 0.8829880875535473.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2300739685.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 06:59, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.714800</td>\n",
       "      <td>0.879262</td>\n",
       "      <td>0.879262</td>\n",
       "      <td>0.879262</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.659911</td>\n",
       "      <td>0.882548</td>\n",
       "      <td>0.882548</td>\n",
       "      <td>0.882548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.646126</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.882929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.651090</td>\n",
       "      <td>0.882841</td>\n",
       "      <td>0.882841</td>\n",
       "      <td>0.882841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.637015</td>\n",
       "      <td>0.882841</td>\n",
       "      <td>0.882841</td>\n",
       "      <td>0.882841</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 02:47:04,257] Trial 4 finished with value: 0.8828413825479726 and parameters: {'learning_rate': 2.5539261234013832e-05, 'batch_size': 4}. Best is trial 2 with value: 0.8829880875535473.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2300739685.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 12:09, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.742350</td>\n",
       "      <td>0.880553</td>\n",
       "      <td>0.880553</td>\n",
       "      <td>0.880553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.669068</td>\n",
       "      <td>0.882841</td>\n",
       "      <td>0.882841</td>\n",
       "      <td>0.882841</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.642779</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.882929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.637915</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636968</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:15]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 02:59:41,602] Trial 5 finished with value: 0.8829880875535473 and parameters: {'learning_rate': 2.0512760971655216e-05, 'batch_size': 8}. Best is trial 2 with value: 0.8829880875535473.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2300739685.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 22:23, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.763084</td>\n",
       "      <td>0.867555</td>\n",
       "      <td>0.867555</td>\n",
       "      <td>0.867555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.672668</td>\n",
       "      <td>0.881697</td>\n",
       "      <td>0.881697</td>\n",
       "      <td>0.881697</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.644924</td>\n",
       "      <td>0.882665</td>\n",
       "      <td>0.882665</td>\n",
       "      <td>0.882665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.644909</td>\n",
       "      <td>0.882783</td>\n",
       "      <td>0.882783</td>\n",
       "      <td>0.882783</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.640749</td>\n",
       "      <td>0.882783</td>\n",
       "      <td>0.882783</td>\n",
       "      <td>0.882783</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 03:22:42,378] Trial 6 finished with value: 0.8827827005457425 and parameters: {'learning_rate': 1.6819568284149955e-05, 'batch_size': 16}. Best is trial 2 with value: 0.8829880875535473.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2300739685.py:39: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 22:19, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.685055</td>\n",
       "      <td>0.882489</td>\n",
       "      <td>0.882489</td>\n",
       "      <td>0.882489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.648965</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.882929</td>\n",
       "      <td>0.882929</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.630707</td>\n",
       "      <td>0.882959</td>\n",
       "      <td>0.882959</td>\n",
       "      <td>0.882959</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.631525</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.626061</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 03:45:37,757] Trial 7 finished with value: 0.8829880875535473 and parameters: {'learning_rate': 3.0720514907054634e-05, 'batch_size': 16}. Best is trial 2 with value: 0.8829880875535473.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best GPT-Neo Full-FT params: {'learning_rate': 2.7233372871192413e-05, 'batch_size': 16} → Dev-F1 = 0.8829880875535473\n"
     ]
    }
   ],
   "source": [
    "# === Chapter 23: Hyperparameter-Tuning Full Fine-Tuning for GPT-Neo (fixed) ===\n",
    "\n",
    "import optuna\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "def neo_ft_objective(trial):\n",
    "    # 1) sample a learning rate and batch size\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    \n",
    "    # 2) fresh model for each trial\n",
    "    m = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id\n",
    "    )\n",
    "    # ─────────── FIX ───────────\n",
    "    # resize embeddings so that PAD token (added once at Chapter 21) fits\n",
    "    m.resize_token_embeddings(len(tokenizer))\n",
    "    m.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # ────────────────────────────\n",
    "\n",
    "    # 3) training args\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/gptneo-ft-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs*2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # 4) trainer & train\n",
    "    trainer = Trainer(\n",
    "        model=m,\n",
    "        args=args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # 5) return Dev-F1 for Optuna to maximize\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "# 6) run the study overnight\n",
    "study_neo_ft = optuna.create_study(direction=\"maximize\")\n",
    "study_neo_ft.optimize(neo_ft_objective, n_trials=8)\n",
    "\n",
    "print(\"🏆 Best GPT-Neo Full-FT params:\", study_neo_ft.best_params,\n",
    "      \"→ Dev-F1 =\", study_neo_ft.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362012fb-43b2-41b6-bf88-dd3c4a60fb3e",
   "metadata": {},
   "source": [
    "## Chapter 24: LoRA Hyperparameter-Tuning for GPT-Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "8368f5e6-bd66-45c2-985d-250067ca5e00",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 03:45:37,773] A new study created in memory with name: no-name-3d390395-92e6-42d6-85d4-e84bb04a2440\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1451816050.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 19:39, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.749797</td>\n",
       "      <td>0.880083</td>\n",
       "      <td>0.880083</td>\n",
       "      <td>0.880083</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.690616</td>\n",
       "      <td>0.881785</td>\n",
       "      <td>0.881785</td>\n",
       "      <td>0.881785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.658776</td>\n",
       "      <td>0.882489</td>\n",
       "      <td>0.882489</td>\n",
       "      <td>0.882489</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.657126</td>\n",
       "      <td>0.882548</td>\n",
       "      <td>0.882548</td>\n",
       "      <td>0.882548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.653099</td>\n",
       "      <td>0.882665</td>\n",
       "      <td>0.882665</td>\n",
       "      <td>0.882665</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 04:05:53,095] Trial 0 finished with value: 0.8826653365412828 and parameters: {'learning_rate': 0.00017023382278520056, 'r': 16, 'alpha': 32, 'dropout': 0.27867312010953715, 'batch_size': 16}. Best is trial 0 with value: 0.8826653365412828.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1451816050.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 10:37, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.778641</td>\n",
       "      <td>0.877736</td>\n",
       "      <td>0.877736</td>\n",
       "      <td>0.877736</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.720792</td>\n",
       "      <td>0.878176</td>\n",
       "      <td>0.878176</td>\n",
       "      <td>0.878176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.686131</td>\n",
       "      <td>0.881316</td>\n",
       "      <td>0.881316</td>\n",
       "      <td>0.881316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.677228</td>\n",
       "      <td>0.881961</td>\n",
       "      <td>0.881961</td>\n",
       "      <td>0.881961</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.675843</td>\n",
       "      <td>0.881990</td>\n",
       "      <td>0.881990</td>\n",
       "      <td>0.881990</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 04:16:58,245] Trial 1 finished with value: 0.8819904935156387 and parameters: {'learning_rate': 0.00019670484556358483, 'r': 16, 'alpha': 16, 'dropout': 0.1399427413465223, 'batch_size': 8}. Best is trial 0 with value: 0.8826653365412828.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1451816050.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:37, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.115436</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>0.011502</td>\n",
       "      <td>0.011502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.152608</td>\n",
       "      <td>0.117804</td>\n",
       "      <td>0.117804</td>\n",
       "      <td>0.117804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.419782</td>\n",
       "      <td>0.394724</td>\n",
       "      <td>0.394724</td>\n",
       "      <td>0.394724</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.982409</td>\n",
       "      <td>0.598791</td>\n",
       "      <td>0.598791</td>\n",
       "      <td>0.598791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.839116</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>0.657708</td>\n",
       "      <td>0.657708</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 04:22:59,148] Trial 2 finished with value: 0.6577078809928995 and parameters: {'learning_rate': 3.1048016853416664e-05, 'r': 16, 'alpha': 32, 'dropout': 0.15310587001879, 'batch_size': 4}. Best is trial 0 with value: 0.8826653365412828.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1451816050.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 10:33, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.232097</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.005399</td>\n",
       "      <td>0.005399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.249663</td>\n",
       "      <td>0.062438</td>\n",
       "      <td>0.062438</td>\n",
       "      <td>0.062438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.496342</td>\n",
       "      <td>0.316237</td>\n",
       "      <td>0.316237</td>\n",
       "      <td>0.316237</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.048756</td>\n",
       "      <td>0.549586</td>\n",
       "      <td>0.549586</td>\n",
       "      <td>0.549586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.901574</td>\n",
       "      <td>0.620386</td>\n",
       "      <td>0.620386</td>\n",
       "      <td>0.620386</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 04:34:00,047] Trial 3 finished with value: 0.6203861275746728 and parameters: {'learning_rate': 2.9884670409584333e-05, 'r': 8, 'alpha': 32, 'dropout': 0.02495077214792871, 'batch_size': 8}. Best is trial 0 with value: 0.8826653365412828.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1451816050.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 19:38, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.751109</td>\n",
       "      <td>0.874479</td>\n",
       "      <td>0.874479</td>\n",
       "      <td>0.874479</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.660312</td>\n",
       "      <td>0.882871</td>\n",
       "      <td>0.882871</td>\n",
       "      <td>0.882871</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.640613</td>\n",
       "      <td>0.883105</td>\n",
       "      <td>0.883105</td>\n",
       "      <td>0.883105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.641691</td>\n",
       "      <td>0.883105</td>\n",
       "      <td>0.883105</td>\n",
       "      <td>0.883105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.636624</td>\n",
       "      <td>0.883105</td>\n",
       "      <td>0.883105</td>\n",
       "      <td>0.883105</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 04:54:13,986] Trial 4 finished with value: 0.8831054515580071 and parameters: {'learning_rate': 0.00031630029815269686, 'r': 4, 'alpha': 32, 'dropout': 0.05831405767768296, 'batch_size': 16}. Best is trial 4 with value: 0.8831054515580071.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1451816050.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 10:32, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.169436</td>\n",
       "      <td>0.477407</td>\n",
       "      <td>0.477407</td>\n",
       "      <td>0.477407</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.101261</td>\n",
       "      <td>0.834840</td>\n",
       "      <td>0.834840</td>\n",
       "      <td>0.834840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.826886</td>\n",
       "      <td>0.869462</td>\n",
       "      <td>0.869462</td>\n",
       "      <td>0.869462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.781257</td>\n",
       "      <td>0.871310</td>\n",
       "      <td>0.871310</td>\n",
       "      <td>0.871310</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.773620</td>\n",
       "      <td>0.870782</td>\n",
       "      <td>0.870782</td>\n",
       "      <td>0.870782</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 05:05:14,654] Trial 5 finished with value: 0.8707822310897247 and parameters: {'learning_rate': 5.0282113237116235e-05, 'r': 8, 'alpha': 32, 'dropout': 0.18124097185338556, 'batch_size': 8}. Best is trial 4 with value: 0.8831054515580071.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1451816050.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 10:37, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.590802</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>0.003286</td>\n",
       "      <td>0.003286</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>4.059296</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>0.013233</td>\n",
       "      <td>0.013233</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.652030</td>\n",
       "      <td>0.036471</td>\n",
       "      <td>0.036471</td>\n",
       "      <td>0.036471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.397099</td>\n",
       "      <td>0.067572</td>\n",
       "      <td>0.067572</td>\n",
       "      <td>0.067572</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.308103</td>\n",
       "      <td>0.083035</td>\n",
       "      <td>0.083035</td>\n",
       "      <td>0.083035</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 05:16:20,081] Trial 6 finished with value: 0.08303503315533126 and parameters: {'learning_rate': 1.7679371395531382e-05, 'r': 16, 'alpha': 32, 'dropout': 0.0567647209448687, 'batch_size': 8}. Best is trial 4 with value: 0.8831054515580071.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1451816050.py:51: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 19:42, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.693647</td>\n",
       "      <td>0.882665</td>\n",
       "      <td>0.882665</td>\n",
       "      <td>0.882665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.635894</td>\n",
       "      <td>0.883105</td>\n",
       "      <td>0.883105</td>\n",
       "      <td>0.883105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.614593</td>\n",
       "      <td>0.883105</td>\n",
       "      <td>0.883105</td>\n",
       "      <td>0.883105</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.614911</td>\n",
       "      <td>0.883076</td>\n",
       "      <td>0.883076</td>\n",
       "      <td>0.883076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.611870</td>\n",
       "      <td>0.883076</td>\n",
       "      <td>0.883076</td>\n",
       "      <td>0.883076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:13]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 05:36:38,878] Trial 7 finished with value: 0.8830761105568922 and parameters: {'learning_rate': 0.000928420074451464, 'r': 4, 'alpha': 32, 'dropout': 0.2968941921119626, 'batch_size': 16}. Best is trial 4 with value: 0.8831054515580071.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best GPT-Neo LoRA params: {'learning_rate': 0.00031630029815269686, 'r': 4, 'alpha': 32, 'dropout': 0.05831405767768296, 'batch_size': 16} → Dev-F1 = 0.8831054515580071\n"
     ]
    }
   ],
   "source": [
    "# === Chapter 24: LoRA Hyperparameter-Tuning for GPT-Neo (fixed) ===\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def neo_lora_objective(trial):\n",
    "    # 1) sample hyperparameters\n",
    "    lr      = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    r       = trial.suggest_categorical(\"r\", [4, 8, 16])\n",
    "    alpha   = trial.suggest_categorical(\"alpha\", [16, 32])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    bs      = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "\n",
    "    # 2) configure LoRA adapter\n",
    "    lora_conf = LoraConfig(\n",
    "        task_type=\"TOKEN_CLS\",\n",
    "        inference_mode=False,\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=dropout,\n",
    "    )\n",
    "    base = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    lora_model = get_peft_model(base, lora_conf)\n",
    "\n",
    "    # ─────────── FIX: ensure new PAD token is in embedding matrix ───────────\n",
    "    lora_model.resize_token_embeddings(len(tokenizer))\n",
    "    lora_model.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # ──────────────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # 3) training args\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/gptneo-lora-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs*2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # 4) trainer & train\n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # 5) return Dev-F1 for Optuna\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "# 6) run the study\n",
    "study_neo_lora = optuna.create_study(direction=\"maximize\")\n",
    "study_neo_lora.optimize(neo_lora_objective, n_trials=8)\n",
    "\n",
    "print(\"🏆 Best GPT-Neo LoRA params:\", study_neo_lora.best_params,\n",
    "      \"→ Dev-F1 =\", study_neo_lora.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc878070-e50a-44a0-8d54-d4ee04b38607",
   "metadata": {},
   "source": [
    "## Chapter 25: Partial-Freeze Hyperparameter-Tuning for GPT-Neo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "8f83d5cf-f1ae-487d-b801-49dbe9b942bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 05:36:38,889] A new study created in memory with name: no-name-034ad9c7-93db-439d-b948-ac41a0dc5d17\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3689319403.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:35, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.298176</td>\n",
       "      <td>0.423039</td>\n",
       "      <td>0.423039</td>\n",
       "      <td>0.423039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.397294</td>\n",
       "      <td>0.775571</td>\n",
       "      <td>0.775571</td>\n",
       "      <td>0.775571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.031718</td>\n",
       "      <td>0.842439</td>\n",
       "      <td>0.842439</td>\n",
       "      <td>0.842439</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.909540</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.857843</td>\n",
       "      <td>0.857843</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.881030</td>\n",
       "      <td>0.860337</td>\n",
       "      <td>0.860337</td>\n",
       "      <td>0.860337</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 05:42:38,007] Trial 0 finished with value: 0.8603368346927998 and parameters: {'learning_rate': 4.693600573046882e-05, 'batch_size': 4, 'freeze_pct': 0.25052201868831997}. Best is trial 0 with value: 0.8603368346927998.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3689319403.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 10:20, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.957616</td>\n",
       "      <td>0.112171</td>\n",
       "      <td>0.112171</td>\n",
       "      <td>0.112171</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.357226</td>\n",
       "      <td>0.373452</td>\n",
       "      <td>0.373452</td>\n",
       "      <td>0.373452</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.970128</td>\n",
       "      <td>0.582272</td>\n",
       "      <td>0.582272</td>\n",
       "      <td>0.582272</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.760023</td>\n",
       "      <td>0.673024</td>\n",
       "      <td>0.673024</td>\n",
       "      <td>0.673024</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.692249</td>\n",
       "      <td>0.699284</td>\n",
       "      <td>0.699284</td>\n",
       "      <td>0.699284</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 05:53:25,410] Trial 1 finished with value: 0.699284079572795 and parameters: {'learning_rate': 2.054713098524259e-05, 'batch_size': 8, 'freeze_pct': 0.2613474753248602}. Best is trial 0 with value: 0.8603368346927998.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3689319403.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:36, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.868567</td>\n",
       "      <td>0.032040</td>\n",
       "      <td>0.032040</td>\n",
       "      <td>0.032040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.220285</td>\n",
       "      <td>0.112464</td>\n",
       "      <td>0.112464</td>\n",
       "      <td>0.112464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.777935</td>\n",
       "      <td>0.225045</td>\n",
       "      <td>0.225045</td>\n",
       "      <td>0.225045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.523161</td>\n",
       "      <td>0.312188</td>\n",
       "      <td>0.312188</td>\n",
       "      <td>0.312188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.438042</td>\n",
       "      <td>0.343524</td>\n",
       "      <td>0.343524</td>\n",
       "      <td>0.343524</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 05:59:25,421] Trial 2 finished with value: 0.34352444105392876 and parameters: {'learning_rate': 2.4083598074175995e-05, 'batch_size': 4, 'freeze_pct': 0.3274962445505534}. Best is trial 0 with value: 0.8603368346927998.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3689319403.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 10:19, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.213424</td>\n",
       "      <td>0.056716</td>\n",
       "      <td>0.056716</td>\n",
       "      <td>0.056716</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.780108</td>\n",
       "      <td>0.170999</td>\n",
       "      <td>0.170999</td>\n",
       "      <td>0.170999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.484169</td>\n",
       "      <td>0.306203</td>\n",
       "      <td>0.306203</td>\n",
       "      <td>0.306203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.313495</td>\n",
       "      <td>0.396397</td>\n",
       "      <td>0.396397</td>\n",
       "      <td>0.396397</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.256195</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.427000</td>\n",
       "      <td>0.427000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 06:10:12,138] Trial 3 finished with value: 0.4269995892259844 and parameters: {'learning_rate': 1.4124656276571285e-05, 'batch_size': 8, 'freeze_pct': 0.3726377583154495}. Best is trial 0 with value: 0.8603368346927998.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3689319403.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 18:46, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.759666</td>\n",
       "      <td>0.040667</td>\n",
       "      <td>0.040667</td>\n",
       "      <td>0.040667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.041320</td>\n",
       "      <td>0.153952</td>\n",
       "      <td>0.153952</td>\n",
       "      <td>0.153952</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.556643</td>\n",
       "      <td>0.304149</td>\n",
       "      <td>0.304149</td>\n",
       "      <td>0.304149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.282338</td>\n",
       "      <td>0.407312</td>\n",
       "      <td>0.407312</td>\n",
       "      <td>0.407312</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.191968</td>\n",
       "      <td>0.445250</td>\n",
       "      <td>0.445250</td>\n",
       "      <td>0.445250</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 06:29:32,566] Trial 4 finished with value: 0.44524969191948827 and parameters: {'learning_rate': 2.37037188874923e-05, 'batch_size': 16, 'freeze_pct': 0.445193786067657}. Best is trial 0 with value: 0.8603368346927998.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3689319403.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 18:43, Epoch 9/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.607296</td>\n",
       "      <td>0.025732</td>\n",
       "      <td>0.025732</td>\n",
       "      <td>0.025732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.884267</td>\n",
       "      <td>0.174902</td>\n",
       "      <td>0.174902</td>\n",
       "      <td>0.174902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.402962</td>\n",
       "      <td>0.418344</td>\n",
       "      <td>0.418344</td>\n",
       "      <td>0.418344</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.132607</td>\n",
       "      <td>0.566721</td>\n",
       "      <td>0.566721</td>\n",
       "      <td>0.566721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.043869</td>\n",
       "      <td>0.610410</td>\n",
       "      <td>0.610410</td>\n",
       "      <td>0.610410</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:12]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 06:48:50,667] Trial 5 finished with value: 0.6104101871955872 and parameters: {'learning_rate': 2.392840501149533e-05, 'batch_size': 16, 'freeze_pct': 0.4993138528542309}. Best is trial 0 with value: 0.8603368346927998.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3689319403.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:36, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.843037</td>\n",
       "      <td>0.192360</td>\n",
       "      <td>0.192360</td>\n",
       "      <td>0.192360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.727253</td>\n",
       "      <td>0.734288</td>\n",
       "      <td>0.734288</td>\n",
       "      <td>0.734288</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.186735</td>\n",
       "      <td>0.843466</td>\n",
       "      <td>0.843466</td>\n",
       "      <td>0.843466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.983125</td>\n",
       "      <td>0.862567</td>\n",
       "      <td>0.862567</td>\n",
       "      <td>0.862567</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.932611</td>\n",
       "      <td>0.866322</td>\n",
       "      <td>0.866322</td>\n",
       "      <td>0.866322</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:17]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 06:54:49,828] Trial 6 finished with value: 0.8663223989202512 and parameters: {'learning_rate': 4.963009046390126e-05, 'batch_size': 4, 'freeze_pct': 0.4807295614021466}. Best is trial 6 with value: 0.8663223989202512.\n",
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3689319403.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 10:19, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>No log</td>\n",
       "      <td>3.088184</td>\n",
       "      <td>0.079984</td>\n",
       "      <td>0.079984</td>\n",
       "      <td>0.079984</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.570399</td>\n",
       "      <td>0.261780</td>\n",
       "      <td>0.261780</td>\n",
       "      <td>0.261780</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.225145</td>\n",
       "      <td>0.445132</td>\n",
       "      <td>0.445132</td>\n",
       "      <td>0.445132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>No log</td>\n",
       "      <td>2.030884</td>\n",
       "      <td>0.551992</td>\n",
       "      <td>0.551992</td>\n",
       "      <td>0.551992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.966712</td>\n",
       "      <td>0.583681</td>\n",
       "      <td>0.583681</td>\n",
       "      <td>0.583681</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='5' max='5' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [5/5 00:16]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 07:05:36,987] Trial 7 finished with value: 0.5836805351798603 and parameters: {'learning_rate': 1.7247268512087645e-05, 'batch_size': 8, 'freeze_pct': 0.7004998811199978}. Best is trial 6 with value: 0.8663223989202512.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best GPT-Neo Freeze params: {'learning_rate': 4.963009046390126e-05, 'batch_size': 4, 'freeze_pct': 0.4807295614021466} → Dev-F1 = 0.8663223989202512\n"
     ]
    }
   ],
   "source": [
    "# === Chapter 25: Partial-Freeze Hyperparameter-Tuning for GPT-Neo (fixed) ===\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "def neo_freeze_objective(trial):\n",
    "    # 1) sample hyperparameters\n",
    "    lr  = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    bs  = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    pct = trial.suggest_float(\"freeze_pct\", 0.25, 0.75)\n",
    "\n",
    "    # 2) fresh model\n",
    "    m = AutoModelForTokenClassification.from_pretrained(\n",
    "        model_name,\n",
    "        num_labels=len(ner_labels),\n",
    "        id2label=id2label,\n",
    "        label2id=label2id,\n",
    "    )\n",
    "    # ─────────── FIX: ensure pad token embedding exists ───────────\n",
    "    m.resize_token_embeddings(len(tokenizer))\n",
    "    m.config.pad_token_id = tokenizer.pad_token_id\n",
    "    # ────────────────────────────────────────────────────────────────\n",
    "\n",
    "    # 3) freeze first pct of transformer layers\n",
    "    total = len([n for n,_ in m.named_parameters() if n.startswith(\"transformer.h.\")])\n",
    "    cutoff = int(total * pct)\n",
    "    for name, param in m.named_parameters():\n",
    "        if name.startswith(\"transformer.h.\") and int(name.split(\".\")[2]) < cutoff:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # 4) training args\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/gptneo-freeze-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs*2,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # 5) trainer & train\n",
    "    trainer = Trainer(\n",
    "        model=m,\n",
    "        args=args,\n",
    "        train_dataset=hf_train,\n",
    "        eval_dataset=hf_dev,\n",
    "        data_collator=data_collator,\n",
    "        tokenizer=tokenizer,\n",
    "        compute_metrics=compute_metrics_entity_only,\n",
    "    )\n",
    "    trainer.train()\n",
    "\n",
    "    # 6) return Dev-F1\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "# 7) run the study\n",
    "study_neo_freeze = optuna.create_study(direction=\"maximize\")\n",
    "study_neo_freeze.optimize(neo_freeze_objective, n_trials=8)\n",
    "\n",
    "print(\"🏆 Best GPT-Neo Freeze params:\", study_neo_freeze.best_params,\n",
    "      \"→ Dev-F1 =\", study_neo_freeze.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "356cdbca-2b6a-4967-99d8-c745b9699083",
   "metadata": {},
   "source": [
    "## Kapitel 26: Full Fine-Tuning with best Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "329cd899-2e94-420b-bc38-8deaebec0907",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1093262383.py:27: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 45:46, Epoch 18/19]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.107100</td>\n",
       "      <td>0.696423</td>\n",
       "      <td>0.881990</td>\n",
       "      <td>0.881990</td>\n",
       "      <td>0.881990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.923600</td>\n",
       "      <td>0.646867</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.866400</td>\n",
       "      <td>0.628279</td>\n",
       "      <td>0.883047</td>\n",
       "      <td>0.883047</td>\n",
       "      <td>0.883047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.883300</td>\n",
       "      <td>0.630834</td>\n",
       "      <td>0.883017</td>\n",
       "      <td>0.883017</td>\n",
       "      <td>0.883017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.769900</td>\n",
       "      <td>0.629973</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.882812</td>\n",
       "      <td>0.882812</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.737600</td>\n",
       "      <td>0.631209</td>\n",
       "      <td>0.881902</td>\n",
       "      <td>0.881902</td>\n",
       "      <td>0.881902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.691200</td>\n",
       "      <td>0.641137</td>\n",
       "      <td>0.881580</td>\n",
       "      <td>0.881580</td>\n",
       "      <td>0.881580</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.647900</td>\n",
       "      <td>0.649982</td>\n",
       "      <td>0.880729</td>\n",
       "      <td>0.880729</td>\n",
       "      <td>0.880729</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.660200</td>\n",
       "      <td>0.658206</td>\n",
       "      <td>0.880142</td>\n",
       "      <td>0.880142</td>\n",
       "      <td>0.880142</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.630700</td>\n",
       "      <td>0.660646</td>\n",
       "      <td>0.879731</td>\n",
       "      <td>0.879731</td>\n",
       "      <td>0.879731</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.8408329391479492, metrics={'train_runtime': 2760.5328, 'train_samples_per_second': 1.159, 'train_steps_per_second': 0.072, 'total_flos': 789123673681920.0, 'train_loss': 0.8408329391479492, 'epoch': 18.181818181818183})"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# 26.1 Beste Full-FT-Params aus Optuna\n",
    "best = study_neo_ft.best_params  \n",
    "# z.B. {'learning_rate': 2.7233e-05, 'batch_size': 16}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name,\n",
    "    num_labels=len(ner_labels), id2label=id2label, label2id=label2id\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs/gptneo-full-opt\",\n",
    "    per_device_train_batch_size=best[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best[\"batch_size\"] * 2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=200,\n",
    "    learning_rate=best[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=hf_train, eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84bb16db-c17a-48d7-b1d2-51d744424c93",
   "metadata": {},
   "source": [
    "## 27: GPT-Neo LoRA Fine-Tuning with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "346d31fe-ea00-4018-a969-39967c6fe1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1951697532.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 40:40, Epoch 18/19]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>1.131900</td>\n",
       "      <td>0.721940</td>\n",
       "      <td>0.878646</td>\n",
       "      <td>0.878646</td>\n",
       "      <td>0.878646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>0.937600</td>\n",
       "      <td>0.648442</td>\n",
       "      <td>0.882695</td>\n",
       "      <td>0.882695</td>\n",
       "      <td>0.882695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>0.897200</td>\n",
       "      <td>0.634618</td>\n",
       "      <td>0.883017</td>\n",
       "      <td>0.883017</td>\n",
       "      <td>0.883017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>0.936900</td>\n",
       "      <td>0.635759</td>\n",
       "      <td>0.883076</td>\n",
       "      <td>0.883076</td>\n",
       "      <td>0.883076</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.853700</td>\n",
       "      <td>0.619390</td>\n",
       "      <td>0.883047</td>\n",
       "      <td>0.883047</td>\n",
       "      <td>0.883047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.847500</td>\n",
       "      <td>0.624178</td>\n",
       "      <td>0.883047</td>\n",
       "      <td>0.883047</td>\n",
       "      <td>0.883047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.810600</td>\n",
       "      <td>0.618386</td>\n",
       "      <td>0.883047</td>\n",
       "      <td>0.883047</td>\n",
       "      <td>0.883047</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.793200</td>\n",
       "      <td>0.611720</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "      <td>0.882988</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.821200</td>\n",
       "      <td>0.612359</td>\n",
       "      <td>0.882900</td>\n",
       "      <td>0.882900</td>\n",
       "      <td>0.882900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.792600</td>\n",
       "      <td>0.611126</td>\n",
       "      <td>0.882871</td>\n",
       "      <td>0.882871</td>\n",
       "      <td>0.882871</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=0.9494753837585449, metrics={'train_runtime': 2451.9182, 'train_samples_per_second': 1.305, 'train_steps_per_second': 0.082, 'total_flos': 790769927577600.0, 'train_loss': 0.9494753837585449, 'epoch': 18.181818181818183})"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# 27.1 Beste LoRA-Params aus Optuna\n",
    "best = study_neo_lora.best_params  \n",
    "# z.B. {'learning_rate':0.0003163,'r':4,'alpha':32,'dropout':0.0583,'batch_size':16}\n",
    "\n",
    "lora_conf = LoraConfig(\n",
    "    task_type=\"TOKEN_CLS\", inference_mode=False,\n",
    "    r=best[\"r\"], lora_alpha=best[\"alpha\"], lora_dropout=best[\"dropout\"]\n",
    ")\n",
    "base = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=len(ner_labels),\n",
    "    id2label=id2label, label2id=label2id\n",
    ")\n",
    "model = get_peft_model(base, lora_conf)\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs/gptneo-lora-opt\",\n",
    "    per_device_train_batch_size=best[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best[\"batch_size\"] * 2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=200,\n",
    "    learning_rate=best[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=hf_train, eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a54d92-a7fb-417c-93a4-f7e26b30df1a",
   "metadata": {},
   "source": [
    "## 28: GPT-Neo Partial-Freeze Fine-Tuning with best params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "ecb657e1-5f98-49e0-9754-9ede802af115",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForTokenClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\874894544.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 12:48, Epoch 4/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.608200</td>\n",
       "      <td>1.997508</td>\n",
       "      <td>0.588287</td>\n",
       "      <td>0.588287</td>\n",
       "      <td>0.588287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.457000</td>\n",
       "      <td>1.059800</td>\n",
       "      <td>0.845666</td>\n",
       "      <td>0.845666</td>\n",
       "      <td>0.845666</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.015600</td>\n",
       "      <td>0.802541</td>\n",
       "      <td>0.873804</td>\n",
       "      <td>0.873804</td>\n",
       "      <td>0.873804</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.163200</td>\n",
       "      <td>0.750082</td>\n",
       "      <td>0.876621</td>\n",
       "      <td>0.876621</td>\n",
       "      <td>0.876621</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.093400</td>\n",
       "      <td>0.730350</td>\n",
       "      <td>0.877061</td>\n",
       "      <td>0.877061</td>\n",
       "      <td>0.877061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>1.026400</td>\n",
       "      <td>0.718330</td>\n",
       "      <td>0.878235</td>\n",
       "      <td>0.878235</td>\n",
       "      <td>0.878235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.954800</td>\n",
       "      <td>0.712973</td>\n",
       "      <td>0.878880</td>\n",
       "      <td>0.878880</td>\n",
       "      <td>0.878880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.983800</td>\n",
       "      <td>0.709153</td>\n",
       "      <td>0.879027</td>\n",
       "      <td>0.879027</td>\n",
       "      <td>0.879027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.950500</td>\n",
       "      <td>0.708594</td>\n",
       "      <td>0.879086</td>\n",
       "      <td>0.879086</td>\n",
       "      <td>0.879086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.974100</td>\n",
       "      <td>0.707938</td>\n",
       "      <td>0.879086</td>\n",
       "      <td>0.879086</td>\n",
       "      <td>0.879086</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=200, training_loss=1.3035770797729491, metrics={'train_runtime': 770.8462, 'train_samples_per_second': 1.038, 'train_steps_per_second': 0.259, 'total_flos': 206883665111808.0, 'train_loss': 1.3035770797729491, 'epoch': 4.761904761904762})"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import AutoModelForTokenClassification, TrainingArguments, Trainer\n",
    "\n",
    "# 28.1 Beste Freeze-Params aus Optuna\n",
    "best = study_neo_freeze.best_params  \n",
    "# z.B. {'learning_rate':4.96e-05,'batch_size':4,'freeze_pct':0.4807}\n",
    "\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_name, num_labels=len(ner_labels),\n",
    "    id2label=id2label, label2id=label2id\n",
    ")\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "\n",
    "# Freeze first pct of layers\n",
    "total = len([n for n,_ in model.named_parameters() if n.startswith(\"transformer.h.\")])\n",
    "cut = int(total * best[\"freeze_pct\"])\n",
    "for n, p in model.named_parameters():\n",
    "    if n.startswith(\"transformer.h.\") and int(n.split(\".\")[2]) < cut:\n",
    "        p.requires_grad = False\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"outputs/gptneo-freeze-opt\",\n",
    "    per_device_train_batch_size=best[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best[\"batch_size\"] * 2,\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=200,\n",
    "    learning_rate=best[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model, args=args,\n",
    "    train_dataset=hf_train, eval_dataset=hf_dev,\n",
    "    data_collator=data_collator,\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics_entity_only,\n",
    ")\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eae1922a-30e9-47fd-bf2b-d5733694fc4d",
   "metadata": {},
   "source": [
    "## 29. Relation-Extraction "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "65f835b1-7d54-4408-a33b-be4b12d34aab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['split', 'sentence', 'head', 'tail', 'label'],\n",
      "        num_rows: 1222\n",
      "    })\n",
      "    dev: Dataset({\n",
      "        features: ['split', 'sentence', 'head', 'tail', 'label'],\n",
      "        num_rows: 606\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: [],\n",
      "        num_rows: 0\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "# === Kapitel 29: Relation Extraction – Beispiele aus Train/Dev/Test aufbereiten (robust) ===\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import random\n",
    "\n",
    "examples = []\n",
    "\n",
    "for split, docs in [(\"train\", train_docs), (\"dev\", dev_docs), (\"test\", test_docs)]:\n",
    "    for d in docs:\n",
    "        # 1) Nur wirklich annotierte Docs verarbeiten\n",
    "        if not d.get(\"triples\") or not d.get(\"entities\"):\n",
    "            continue\n",
    "\n",
    "        # 2) Text-Feld wählen (alt: \"doc\", neu: \"document\")\n",
    "        text = d.get(\"doc\") or d.get(\"document\")\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        # 3) Helfer: alle Entity-Mentions (jeweils der erste Span)\n",
    "        ents = [e[\"mentions\"][0] for e in d[\"entities\"] if e.get(\"mentions\")]\n",
    "\n",
    "        # 4) Gold-Paare\n",
    "        true_pairs = {(t[\"head\"], t[\"tail\"]) for t in d[\"triples\"]}\n",
    "\n",
    "        # 5) Pro Triple: 1 Positiv + 1 Zufalls-Negativ\n",
    "        for triple in d[\"triples\"]:\n",
    "            # — Positiv\n",
    "            examples.append({\n",
    "                \"split\":    split,\n",
    "                \"sentence\": text,\n",
    "                \"head\":     triple[\"head\"],\n",
    "                \"tail\":     triple[\"tail\"],\n",
    "                \"label\":    triple[\"relation\"],\n",
    "            })\n",
    "            # — Negativ (einfach solange random, bis kein Gold-Paar)\n",
    "            while True:\n",
    "                h, t = random.sample(ents, 2)\n",
    "                if (h, t) not in true_pairs:\n",
    "                    examples.append({\n",
    "                        \"split\":    split,\n",
    "                        \"sentence\": text,\n",
    "                        \"head\":     h,\n",
    "                        \"tail\":     t,\n",
    "                        \"label\":    \"no_relation\",\n",
    "                    })\n",
    "                    break\n",
    "\n",
    "# 6) In DatasetDict nach Split aufteilen\n",
    "train_ex = [e for e in examples if e[\"split\"] == \"train\"]\n",
    "dev_ex   = [e for e in examples if e[\"split\"] == \"dev\"]\n",
    "test_ex  = [e for e in examples if e[\"split\"] == \"test\"]\n",
    "\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_list(train_ex),\n",
    "    \"dev\":   Dataset.from_list(dev_ex),\n",
    "    \"test\":  Dataset.from_list(test_ex),\n",
    "})\n",
    "\n",
    "print(ds)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c9c150-c740-486b-a4de-c0f7d3ca47c0",
   "metadata": {},
   "source": [
    "## 30. Tokenisierung & Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "70312bbc-5536-46a1-81b3-92120678d1ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Kapitel 30: DatasetDict für RE bauen ===\n",
    "\n",
    "from datasets import Dataset, DatasetDict\n",
    "import pandas as pd\n",
    "\n",
    "# 30.1: Aus den examples (Kapitel 29) ein DataFrame machen\n",
    "# examples = [\n",
    "#   {\"split\":\"train\",\"sentence\":...,\"head\":...,\"tail\":...,\"label\":...}, ...\n",
    "# ]\n",
    "df = pd.DataFrame(examples)\n",
    "\n",
    "# 30.2: Train/Dev trennen\n",
    "train_df = df[df.split == \"train\"].reset_index(drop=True)\n",
    "dev_df   = df[df.split   == \"dev\"].reset_index(drop=True)\n",
    "\n",
    "# 30.3: Huggingface DatasetDict anlegen\n",
    "ds = DatasetDict({\n",
    "    \"train\": Dataset.from_pandas(train_df),\n",
    "    \"dev\":   Dataset.from_pandas(dev_df),\n",
    "})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "59f248e4-0952-45e1-9399-212253c36bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_re_ds = ds "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f886c001-02f5-4345-8f57-d609f669cc77",
   "metadata": {},
   "source": [
    "## Kapitel 31: Tokenisierung & Label-Mapping für RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "3baf1169-5eec-4e39-bc5c-88da262760fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5e0d46d0d047eab57e3c8a68515e1d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1222 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54f3b219f0c14fdaa36e5ebcd319fe97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/606 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# === Kapitel 31: Tokenisierung & Label-Mapping für RE ===\n",
    "\n",
    "# 31.1: Label2id für alle Relationsklassen (inkl. \"no_relation\")\n",
    "all_labels = sorted(df.label.unique())\n",
    "label2id_re = {lab:i for i,lab in enumerate(all_labels)}\n",
    "\n",
    "# 31.2: Tokenizer & Max-Length festlegen (hier z.B. Bert)\n",
    "from transformers import AutoTokenizer\n",
    "tokenizer_re = AutoTokenizer.from_pretrained(\"bert-base-uncased\", use_fast=True)\n",
    "max_length_re = 128\n",
    "\n",
    "# 31.3: Tokenisierungsfunktion\n",
    "def tokenize_re(example):\n",
    "    # Wir packen head, tail und sentence in eines: \"[HEAD] [SEP] [TAIL] [SEP] [SENTENCE]\"\n",
    "    enc = tokenizer_re(\n",
    "        example[\"head\"],\n",
    "        example[\"tail\"] + \" \" + example[\"sentence\"],\n",
    "        truncation=True,\n",
    "        max_length=max_length_re,\n",
    "        padding=\"max_length\"\n",
    "    )\n",
    "    enc[\"labels\"] = label2id_re[example[\"label\"]]\n",
    "    return enc\n",
    "\n",
    "# 31.4: Map auf ds\n",
    "ds = ds.map(\n",
    "    tokenize_re,\n",
    "    batched=False,\n",
    "    remove_columns=ds[\"train\"].column_names\n",
    ")\n",
    "\n",
    "# 31.5: Für PyTorch vorbereiten\n",
    "ds.set_format(\"torch\", columns=[\"input_ids\",\"attention_mask\",\"labels\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf2e414-16c1-4dfc-a5e5-f8e0c591d003",
   "metadata": {},
   "source": [
    "## Kapitel 32: Baseline-Trainingsloop für RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e41edf70-f952-4aa3-8a06-cfb93fd282d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3739544909.py:34: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_re = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [  6/459 00:04 < 08:43, 0.87 it/s, Epoch 0.03/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[127], line 44\u001b[0m\n\u001b[0;32m     34\u001b[0m trainer_re \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     35\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel_re,\n\u001b[0;32m     36\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args_re,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     40\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics_re\n\u001b[0;32m     41\u001b[0m )\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 32.5: Train & Eval\u001b[39;00m\n\u001b[1;32m---> 44\u001b[0m trainer_re\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     45\u001b[0m metrics_re \u001b[38;5;241m=\u001b[39m trainer_re\u001b[38;5;241m.\u001b[39mevaluate()\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔖 RE Baseline Dev-F1:\u001b[39m\u001b[38;5;124m\"\u001b[39m, metrics_re[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meval_f1\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   2246\u001b[0m         args\u001b[38;5;241m=\u001b[39margs,\n\u001b[0;32m   2247\u001b[0m         resume_from_checkpoint\u001b[38;5;241m=\u001b[39mresume_from_checkpoint,\n\u001b[0;32m   2248\u001b[0m         trial\u001b[38;5;241m=\u001b[39mtrial,\n\u001b[0;32m   2249\u001b[0m         ignore_keys_for_eval\u001b[38;5;241m=\u001b[39mignore_keys_for_eval,\n\u001b[0;32m   2250\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\transformers\\trainer.py:2589\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2584\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[0;32m   2585\u001b[0m         amp\u001b[38;5;241m.\u001b[39mmaster_params(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer),\n\u001b[0;32m   2586\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[0;32m   2587\u001b[0m     )\n\u001b[0;32m   2588\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2589\u001b[0m     _grad_norm \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mclip_grad_norm_(\n\u001b[0;32m   2590\u001b[0m         model\u001b[38;5;241m.\u001b[39mparameters(),\n\u001b[0;32m   2591\u001b[0m         args\u001b[38;5;241m.\u001b[39mmax_grad_norm,\n\u001b[0;32m   2592\u001b[0m     )\n\u001b[0;32m   2594\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   2595\u001b[0m     is_accelerate_available()\n\u001b[0;32m   2596\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maccelerator\u001b[38;5;241m.\u001b[39mdistributed_type \u001b[38;5;241m==\u001b[39m DistributedType\u001b[38;5;241m.\u001b[39mDEEPSPEED\n\u001b[0;32m   2597\u001b[0m ):\n\u001b[0;32m   2598\u001b[0m     grad_norm \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mget_global_grad_norm()\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\accelerate\\accelerator.py:2480\u001b[0m, in \u001b[0;36mAccelerator.clip_grad_norm_\u001b[1;34m(self, parameters, max_norm, norm_type)\u001b[0m\n\u001b[0;32m   2478\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m model\u001b[38;5;241m.\u001b[39mclip_grad_norm_(max_norm, norm_type)\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munscale_gradients()\n\u001b[1;32m-> 2480\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(parameters, max_norm, norm_type\u001b[38;5;241m=\u001b[39mnorm_type)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:34\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:215\u001b[0m, in \u001b[0;36mclip_grad_norm_\u001b[1;34m(parameters, max_norm, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m    213\u001b[0m     parameters \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(parameters)\n\u001b[0;32m    214\u001b[0m grads \u001b[38;5;241m=\u001b[39m [p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;28;01mfor\u001b[39;00m p \u001b[38;5;129;01min\u001b[39;00m parameters \u001b[38;5;28;01mif\u001b[39;00m p\u001b[38;5;241m.\u001b[39mgrad \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m]\n\u001b[1;32m--> 215\u001b[0m total_norm \u001b[38;5;241m=\u001b[39m _get_total_norm(grads, norm_type, error_if_nonfinite, foreach)\n\u001b[0;32m    216\u001b[0m _clip_grads_with_norm_(parameters, max_norm, total_norm, foreach)\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m total_norm\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:34\u001b[0m, in \u001b[0;36m_no_grad.<locals>._no_grad_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_no_grad_wrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m     33\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m---> 34\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\torch\\nn\\utils\\clip_grad.py:87\u001b[0m, in \u001b[0;36m_get_total_norm\u001b[1;34m(tensors, norm_type, error_if_nonfinite, foreach)\u001b[0m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m (device, _), ([device_tensors], _) \u001b[38;5;129;01min\u001b[39;00m grouped_tensors\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (foreach \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m _has_foreach_support(device_tensors, device)) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[0;32m     85\u001b[0m         foreach \u001b[38;5;129;01mand\u001b[39;00m _device_has_foreach_support(device)\n\u001b[0;32m     86\u001b[0m     ):\n\u001b[1;32m---> 87\u001b[0m         norms\u001b[38;5;241m.\u001b[39mextend(torch\u001b[38;5;241m.\u001b[39m_foreach_norm(device_tensors, norm_type))\n\u001b[0;32m     88\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m foreach:\n\u001b[0;32m     89\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m     90\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mforeach=True was passed, but can\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt use the foreach API on \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdevice\u001b[38;5;241m.\u001b[39mtype\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tensors\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     91\u001b[0m         )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# === Kapitel 32: Baseline-Trainingsloop für RE ===\n",
    "\n",
    "from transformers import AutoModelForSequenceClassification, TrainingArguments, Trainer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# 32.1: Model laden\n",
    "model_re = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label2id_re)\n",
    ")\n",
    "\n",
    "# 32.2: TrainingArguments\n",
    "training_args_re = TrainingArguments(\n",
    "    output_dir=\"outputs/bert-re-baseline\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available()\n",
    ")\n",
    "\n",
    "# 32.3: Compute-Metrics\n",
    "def compute_metrics_re(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    return {\"eval_f1\": f1_score(p.label_ids, preds, average=\"macro\")}\n",
    "\n",
    "# 32.4: Trainer instanziieren\n",
    "trainer_re = Trainer(\n",
    "    model=model_re,\n",
    "    args=training_args_re,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"dev\"],\n",
    "    tokenizer=tokenizer_re,\n",
    "    compute_metrics=compute_metrics_re\n",
    ")\n",
    "\n",
    "# 32.5: Train & Eval\n",
    "trainer_re.train()\n",
    "metrics_re = trainer_re.evaluate()\n",
    "print(\"🔖 RE Baseline Dev-F1:\", metrics_re[\"eval_f1\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d07615c-245d-4bc4-8092-e288431fb937",
   "metadata": {},
   "source": [
    "## 33: RE Hyperparameter‐Tuning – Full Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "b2bf4ed1-e394-461e-8ba9-ea1871625c24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 16:47:20,361] A new study created in memory with name: no-name-e30ea694-5a9a-4969-88c5-23e06590f2a5\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\4116539554.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 06:36, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.827400</td>\n",
       "      <td>3.300838</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.976100</td>\n",
       "      <td>2.830147</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.073300</td>\n",
       "      <td>2.755363</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.704700</td>\n",
       "      <td>2.710929</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.636600</td>\n",
       "      <td>2.691566</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.524200</td>\n",
       "      <td>2.652421</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.488900</td>\n",
       "      <td>2.657772</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.442000</td>\n",
       "      <td>2.626473</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.188800</td>\n",
       "      <td>2.620448</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.045900</td>\n",
       "      <td>2.624360</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 16:54:19,938] Trial 0 finished with value: 0.5 and parameters: {'learning_rate': 4.3575643120387554e-05, 'batch_size': 16}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\4116539554.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:14, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.235000</td>\n",
       "      <td>4.106583</td>\n",
       "      <td>0.219472</td>\n",
       "      <td>0.219472</td>\n",
       "      <td>0.219472</td>\n",
       "      <td>0.219472</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.005400</td>\n",
       "      <td>3.849129</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.475248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.744200</td>\n",
       "      <td>3.648604</td>\n",
       "      <td>0.481848</td>\n",
       "      <td>0.481848</td>\n",
       "      <td>0.481848</td>\n",
       "      <td>0.481848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.510800</td>\n",
       "      <td>3.494017</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.587500</td>\n",
       "      <td>3.336197</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.391600</td>\n",
       "      <td>3.198136</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.204600</td>\n",
       "      <td>3.114041</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.032100</td>\n",
       "      <td>3.066108</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.011500</td>\n",
       "      <td>3.037939</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.132900</td>\n",
       "      <td>3.026117</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 16:59:57,198] Trial 1 finished with value: 0.5 and parameters: {'learning_rate': 1.1302804238596893e-05, 'batch_size': 8}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\4116539554.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.151100</td>\n",
       "      <td>3.865378</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.542300</td>\n",
       "      <td>3.111426</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.770200</td>\n",
       "      <td>2.797960</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.633100</td>\n",
       "      <td>2.774837</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.085200</td>\n",
       "      <td>2.821862</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.848000</td>\n",
       "      <td>2.760433</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.808700</td>\n",
       "      <td>2.778891</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.616400</td>\n",
       "      <td>2.725522</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.523700</td>\n",
       "      <td>2.718717</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.663500</td>\n",
       "      <td>2.717334</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 17:05:37,639] Trial 2 finished with value: 0.5 and parameters: {'learning_rate': 3.698405938088476e-05, 'batch_size': 8}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\4116539554.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.323600</td>\n",
       "      <td>4.160428</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.093700</td>\n",
       "      <td>3.881290</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.653000</td>\n",
       "      <td>3.509754</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.398600</td>\n",
       "      <td>3.245943</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.356400</td>\n",
       "      <td>3.054633</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.103500</td>\n",
       "      <td>2.960650</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.047300</td>\n",
       "      <td>2.885209</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.845700</td>\n",
       "      <td>2.852964</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.710500</td>\n",
       "      <td>2.834198</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.885400</td>\n",
       "      <td>2.828054</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 17:11:17,778] Trial 3 finished with value: 0.5 and parameters: {'learning_rate': 1.5358595750311107e-05, 'batch_size': 8}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\4116539554.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:46, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.234700</td>\n",
       "      <td>3.863283</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.455446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.682400</td>\n",
       "      <td>3.309893</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.089300</td>\n",
       "      <td>2.896867</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.769200</td>\n",
       "      <td>2.770700</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.395100</td>\n",
       "      <td>2.721347</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.673300</td>\n",
       "      <td>2.701469</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.802900</td>\n",
       "      <td>2.699732</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.648300</td>\n",
       "      <td>2.673278</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.630500</td>\n",
       "      <td>2.671589</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.443000</td>\n",
       "      <td>2.672390</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 17:21:28,920] Trial 4 finished with value: 0.5 and parameters: {'learning_rate': 2.371455620448719e-05, 'batch_size': 32}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\4116539554.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.026800</td>\n",
       "      <td>3.790518</td>\n",
       "      <td>0.402640</td>\n",
       "      <td>0.402640</td>\n",
       "      <td>0.402640</td>\n",
       "      <td>0.402640</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.448900</td>\n",
       "      <td>3.146526</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.731700</td>\n",
       "      <td>2.802326</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.628300</td>\n",
       "      <td>2.783183</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.035600</td>\n",
       "      <td>2.750516</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.817300</td>\n",
       "      <td>2.696826</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.750000</td>\n",
       "      <td>2.695716</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.523800</td>\n",
       "      <td>2.660494</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.441800</td>\n",
       "      <td>2.646850</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.553600</td>\n",
       "      <td>2.639212</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 17:26:53,339] Trial 5 finished with value: 0.5 and parameters: {'learning_rate': 4.3873912745852076e-05, 'batch_size': 8}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\4116539554.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:04, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.114200</td>\n",
       "      <td>3.754726</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.434900</td>\n",
       "      <td>2.964732</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.682700</td>\n",
       "      <td>2.794878</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.564700</td>\n",
       "      <td>2.773169</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.053400</td>\n",
       "      <td>2.779618</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.843700</td>\n",
       "      <td>2.736444</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.765000</td>\n",
       "      <td>2.720011</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.560300</td>\n",
       "      <td>2.714036</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.456000</td>\n",
       "      <td>2.706724</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.624600</td>\n",
       "      <td>2.701765</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 17:32:18,785] Trial 6 finished with value: 0.5 and parameters: {'learning_rate': 4.247069521480185e-05, 'batch_size': 8}. Best is trial 0 with value: 0.5.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\4116539554.py:41: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 06:39, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.297500</td>\n",
       "      <td>4.077745</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.028053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.845000</td>\n",
       "      <td>3.605651</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.452100</td>\n",
       "      <td>3.092050</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.053800</td>\n",
       "      <td>2.872872</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.779800</td>\n",
       "      <td>2.789164</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.717100</td>\n",
       "      <td>2.750902</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.703400</td>\n",
       "      <td>2.738456</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.644500</td>\n",
       "      <td>2.726763</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.415500</td>\n",
       "      <td>2.721800</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.327600</td>\n",
       "      <td>2.720441</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 17:39:21,046] Trial 7 finished with value: 0.5 and parameters: {'learning_rate': 2.0376354374117534e-05, 'batch_size': 16}. Best is trial 0 with value: 0.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best RE Full-FT params: {'learning_rate': 4.3575643120387554e-05, 'batch_size': 16} → Dev-F1 = 0.5\n"
     ]
    }
   ],
   "source": [
    "# === Chapter 33: RE Hyperparameter-Tuning (Full Fine-Tuning) with Steps ===\n",
    "\n",
    "# 33.1: Build your label2id/id2label exactly as before\n",
    "re_labels   = sorted(set(raw_re_ds[\"train\"][\"label\"]) | set(raw_re_ds[\"dev\"][\"label\"]))\n",
    "label2id_re = {l:i for i,l in enumerate(re_labels)}\n",
    "id2label_re = {i:l for l,i in label2id_re.items()}\n",
    "\n",
    "hf_train_re = ds[\"train\"]\n",
    "hf_dev_re   = ds[\"dev\"]\n",
    "\n",
    "def compute_metrics_re(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    labs  = p.label_ids\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(labs, preds, average=\"micro\", zero_division=0)\n",
    "    acc = accuracy_score(labs, preds)\n",
    "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "def re_ft_objective(trial):\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/re-ft-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=len(label2id_re),\n",
    "        id2label=id2label_re,\n",
    "        label2id=label2id_re,\n",
    "    )\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=hf_train_re,\n",
    "        eval_dataset=hf_dev_re,\n",
    "        tokenizer=tokenizer_re,\n",
    "        compute_metrics=compute_metrics_re,\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "study_re_ft = optuna.create_study(direction=\"maximize\")\n",
    "study_re_ft.optimize(re_ft_objective, n_trials=8)\n",
    "print(\"🏆 Best RE Full-FT params:\", study_re_ft.best_params, \"→ Dev-F1 =\", study_re_ft.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "168b4ff9-b454-46b1-aaed-2e7d8045bd4e",
   "metadata": {},
   "source": [
    "## Chapter 34: RE Hyperparameter-Tuning (LoRA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "38684823-435e-4edb-b300-a1715b7d7ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 17:39:21,055] A new study created in memory with name: no-name-ae73170c-7a86-408d-bfbc-c7d26bcee63a\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1207078838.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:24, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.252000</td>\n",
       "      <td>4.306705</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.028053</td>\n",
       "      <td>0.028053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.204700</td>\n",
       "      <td>4.246798</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.029703</td>\n",
       "      <td>0.029703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.109700</td>\n",
       "      <td>4.192232</td>\n",
       "      <td>0.057756</td>\n",
       "      <td>0.057756</td>\n",
       "      <td>0.057756</td>\n",
       "      <td>0.057756</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.017800</td>\n",
       "      <td>4.138948</td>\n",
       "      <td>0.087459</td>\n",
       "      <td>0.087459</td>\n",
       "      <td>0.087459</td>\n",
       "      <td>0.087459</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.096200</td>\n",
       "      <td>4.088309</td>\n",
       "      <td>0.176568</td>\n",
       "      <td>0.176568</td>\n",
       "      <td>0.176568</td>\n",
       "      <td>0.176568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.990800</td>\n",
       "      <td>4.044281</td>\n",
       "      <td>0.247525</td>\n",
       "      <td>0.247525</td>\n",
       "      <td>0.247525</td>\n",
       "      <td>0.247525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.931600</td>\n",
       "      <td>4.006651</td>\n",
       "      <td>0.290429</td>\n",
       "      <td>0.290429</td>\n",
       "      <td>0.290429</td>\n",
       "      <td>0.290429</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.826200</td>\n",
       "      <td>3.978817</td>\n",
       "      <td>0.328383</td>\n",
       "      <td>0.328383</td>\n",
       "      <td>0.328383</td>\n",
       "      <td>0.328383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.827300</td>\n",
       "      <td>3.961478</td>\n",
       "      <td>0.339934</td>\n",
       "      <td>0.339934</td>\n",
       "      <td>0.339934</td>\n",
       "      <td>0.339934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.843700</td>\n",
       "      <td>3.955620</td>\n",
       "      <td>0.344884</td>\n",
       "      <td>0.344884</td>\n",
       "      <td>0.344884</td>\n",
       "      <td>0.344884</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 17:44:06,969] Trial 0 finished with value: 0.3448844884488449 and parameters: {'learning_rate': 3.819414458965424e-05, 'r': 16, 'alpha': 16, 'dropout': 0.030171143674280452, 'batch_size': 8}. Best is trial 0 with value: 0.3448844884488449.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1207078838.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:53, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.628400</td>\n",
       "      <td>4.467794</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.436400</td>\n",
       "      <td>4.330542</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.225800</td>\n",
       "      <td>4.190660</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.024000</td>\n",
       "      <td>4.042905</td>\n",
       "      <td>0.156766</td>\n",
       "      <td>0.156766</td>\n",
       "      <td>0.156766</td>\n",
       "      <td>0.156766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.835800</td>\n",
       "      <td>3.878495</td>\n",
       "      <td>0.392739</td>\n",
       "      <td>0.392739</td>\n",
       "      <td>0.392739</td>\n",
       "      <td>0.392739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.700800</td>\n",
       "      <td>3.711313</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.458746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.530100</td>\n",
       "      <td>3.571662</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.410900</td>\n",
       "      <td>3.471841</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.233700</td>\n",
       "      <td>3.413323</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.129700</td>\n",
       "      <td>3.392461</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 17:50:24,275] Trial 1 finished with value: 0.49834983498349833 and parameters: {'learning_rate': 8.386716846610819e-05, 'r': 8, 'alpha': 16, 'dropout': 0.11397258975734581, 'batch_size': 16}. Best is trial 1 with value: 0.49834983498349833.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1207078838.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:55, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.938100</td>\n",
       "      <td>3.393328</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.754000</td>\n",
       "      <td>2.704489</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.869900</td>\n",
       "      <td>2.786217</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.488400</td>\n",
       "      <td>2.885568</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.488900</td>\n",
       "      <td>2.909358</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.516500</td>\n",
       "      <td>2.876363</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.460600</td>\n",
       "      <td>2.861381</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.420100</td>\n",
       "      <td>2.850537</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.204400</td>\n",
       "      <td>2.861014</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.041200</td>\n",
       "      <td>2.873683</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:20]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 17:56:42,957] Trial 2 finished with value: 0.5 and parameters: {'learning_rate': 0.00032032068892167987, 'r': 8, 'alpha': 32, 'dropout': 0.19912026952009523, 'batch_size': 16}. Best is trial 2 with value: 0.5.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1207078838.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:28, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.030500</td>\n",
       "      <td>3.784569</td>\n",
       "      <td>0.387789</td>\n",
       "      <td>0.387789</td>\n",
       "      <td>0.387789</td>\n",
       "      <td>0.387789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.342300</td>\n",
       "      <td>3.000968</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.630100</td>\n",
       "      <td>2.723487</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.421300</td>\n",
       "      <td>2.796836</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.114100</td>\n",
       "      <td>2.842041</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.388100</td>\n",
       "      <td>2.866793</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.670000</td>\n",
       "      <td>2.868516</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.455000</td>\n",
       "      <td>2.864289</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.475400</td>\n",
       "      <td>2.870556</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.316000</td>\n",
       "      <td>2.871215</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 18:05:38,132] Trial 3 finished with value: 0.5 and parameters: {'learning_rate': 0.00021580480439878258, 'r': 4, 'alpha': 16, 'dropout': 0.16005040240833235, 'batch_size': 32}. Best is trial 2 with value: 0.5.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1207078838.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:21, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.074200</td>\n",
       "      <td>4.178323</td>\n",
       "      <td>0.133663</td>\n",
       "      <td>0.133663</td>\n",
       "      <td>0.133663</td>\n",
       "      <td>0.133663</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.093300</td>\n",
       "      <td>4.129025</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.198020</td>\n",
       "      <td>0.198020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.988400</td>\n",
       "      <td>4.082238</td>\n",
       "      <td>0.278878</td>\n",
       "      <td>0.278878</td>\n",
       "      <td>0.278878</td>\n",
       "      <td>0.278878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.948100</td>\n",
       "      <td>4.037941</td>\n",
       "      <td>0.315182</td>\n",
       "      <td>0.315182</td>\n",
       "      <td>0.315182</td>\n",
       "      <td>0.315182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.835700</td>\n",
       "      <td>3.997550</td>\n",
       "      <td>0.346535</td>\n",
       "      <td>0.346535</td>\n",
       "      <td>0.346535</td>\n",
       "      <td>0.346535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.865300</td>\n",
       "      <td>3.961963</td>\n",
       "      <td>0.371287</td>\n",
       "      <td>0.371287</td>\n",
       "      <td>0.371287</td>\n",
       "      <td>0.371287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.864500</td>\n",
       "      <td>3.933297</td>\n",
       "      <td>0.387789</td>\n",
       "      <td>0.387789</td>\n",
       "      <td>0.387789</td>\n",
       "      <td>0.387789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.824500</td>\n",
       "      <td>3.912088</td>\n",
       "      <td>0.391089</td>\n",
       "      <td>0.391089</td>\n",
       "      <td>0.391089</td>\n",
       "      <td>0.391089</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.799700</td>\n",
       "      <td>3.898384</td>\n",
       "      <td>0.392739</td>\n",
       "      <td>0.392739</td>\n",
       "      <td>0.392739</td>\n",
       "      <td>0.392739</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.739300</td>\n",
       "      <td>3.893719</td>\n",
       "      <td>0.392739</td>\n",
       "      <td>0.392739</td>\n",
       "      <td>0.392739</td>\n",
       "      <td>0.392739</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 18:14:25,864] Trial 4 finished with value: 0.3927392739273928 and parameters: {'learning_rate': 1.7526175007357064e-05, 'r': 8, 'alpha': 16, 'dropout': 0.1255252236159072, 'batch_size': 32}. Best is trial 2 with value: 0.5.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1207078838.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:25, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.987300</td>\n",
       "      <td>4.001821</td>\n",
       "      <td>0.364686</td>\n",
       "      <td>0.364686</td>\n",
       "      <td>0.364686</td>\n",
       "      <td>0.364686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.848800</td>\n",
       "      <td>3.759772</td>\n",
       "      <td>0.462046</td>\n",
       "      <td>0.462046</td>\n",
       "      <td>0.462046</td>\n",
       "      <td>0.462046</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.569000</td>\n",
       "      <td>3.536906</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.354200</td>\n",
       "      <td>3.336749</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.994300</td>\n",
       "      <td>3.171877</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.970800</td>\n",
       "      <td>3.046108</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.011400</td>\n",
       "      <td>2.959699</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.784700</td>\n",
       "      <td>2.906876</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.761900</td>\n",
       "      <td>2.880267</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.614500</td>\n",
       "      <td>2.871498</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:21]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 18:23:17,393] Trial 5 finished with value: 0.5 and parameters: {'learning_rate': 4.9824137594537844e-05, 'r': 16, 'alpha': 32, 'dropout': 0.11096296747671479, 'batch_size': 32}. Best is trial 2 with value: 0.5.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1207078838.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:18, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>2.745500</td>\n",
       "      <td>3.315737</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.111300</td>\n",
       "      <td>3.077214</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.563000</td>\n",
       "      <td>2.974251</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.397500</td>\n",
       "      <td>3.040703</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.808000</td>\n",
       "      <td>2.919446</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.904500</td>\n",
       "      <td>2.883625</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.620500</td>\n",
       "      <td>2.934661</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.365300</td>\n",
       "      <td>2.920622</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.523900</td>\n",
       "      <td>2.916737</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.463400</td>\n",
       "      <td>2.911949</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 18:27:56,351] Trial 6 finished with value: 0.5 and parameters: {'learning_rate': 0.000726832978635972, 'r': 4, 'alpha': 32, 'dropout': 0.05691534174746442, 'batch_size': 8}. Best is trial 2 with value: 0.5.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1207078838.py:37: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:24, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.489500</td>\n",
       "      <td>4.238732</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.019802</td>\n",
       "      <td>0.019802</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.967100</td>\n",
       "      <td>3.748173</td>\n",
       "      <td>0.430693</td>\n",
       "      <td>0.430693</td>\n",
       "      <td>0.430693</td>\n",
       "      <td>0.430693</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.410600</td>\n",
       "      <td>3.275074</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.886800</td>\n",
       "      <td>2.868564</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.296100</td>\n",
       "      <td>2.721523</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.436300</td>\n",
       "      <td>2.775628</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.695500</td>\n",
       "      <td>2.793723</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.473400</td>\n",
       "      <td>2.791213</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.480600</td>\n",
       "      <td>2.799067</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.314200</td>\n",
       "      <td>2.801023</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:22]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 18:36:47,777] Trial 7 finished with value: 0.5 and parameters: {'learning_rate': 0.00018133579482868897, 'r': 16, 'alpha': 32, 'dropout': 0.13780930049521228, 'batch_size': 32}. Best is trial 2 with value: 0.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best RE LoRA params: {'learning_rate': 0.00032032068892167987, 'r': 8, 'alpha': 32, 'dropout': 0.19912026952009523, 'batch_size': 16} → Dev-F1 = 0.5\n"
     ]
    }
   ],
   "source": [
    "# === Chapter 34: RE Hyperparameter-Tuning (LoRA) with Steps ===\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "def re_lora_objective(trial):\n",
    "    lr      = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    r       = trial.suggest_categorical(\"r\", [4, 8, 16])\n",
    "    alpha   = trial.suggest_categorical(\"alpha\", [16, 32])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    bs      = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "\n",
    "    lora_conf = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\", inference_mode=False,\n",
    "        r=r, lora_alpha=alpha, lora_dropout=dropout,\n",
    "    )\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=len(label2id_re),\n",
    "        id2label=id2label_re,\n",
    "        label2id=label2id_re,\n",
    "    )\n",
    "    lora_model = get_peft_model(base, lora_conf)\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/re-lora-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=args,\n",
    "        train_dataset=hf_train_re,\n",
    "        eval_dataset=hf_dev_re,\n",
    "        tokenizer=tokenizer_re,\n",
    "        compute_metrics=compute_metrics_re,\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "study_re_lora = optuna.create_study(direction=\"maximize\")\n",
    "study_re_lora.optimize(re_lora_objective, n_trials=8)\n",
    "print(\"🏆 Best RE LoRA params:\", study_re_lora.best_params, \"→ Dev-F1 =\", study_re_lora.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faeaac5f-2ee0-49e2-8639-b954e4d396be",
   "metadata": {},
   "source": [
    "## Chapter 35: RE Hyperparameter-Tuning (Partial-Freeze)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "2adc1a9a-f925-4f50-a92d-c4120d546049",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 18:36:47,788] A new study created in memory with name: no-name-cfceb62a-ae33-477e-9451-e56f1393b9df\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2784176985.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:07, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.957300</td>\n",
       "      <td>3.909965</td>\n",
       "      <td>0.364686</td>\n",
       "      <td>0.364686</td>\n",
       "      <td>0.364686</td>\n",
       "      <td>0.364686</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.756700</td>\n",
       "      <td>3.644211</td>\n",
       "      <td>0.422442</td>\n",
       "      <td>0.422442</td>\n",
       "      <td>0.422442</td>\n",
       "      <td>0.422442</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.437600</td>\n",
       "      <td>3.425158</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.228300</td>\n",
       "      <td>3.248410</td>\n",
       "      <td>0.478548</td>\n",
       "      <td>0.478548</td>\n",
       "      <td>0.478548</td>\n",
       "      <td>0.478548</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.865600</td>\n",
       "      <td>3.116383</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.488449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.955000</td>\n",
       "      <td>3.022203</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.039500</td>\n",
       "      <td>2.962284</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.862400</td>\n",
       "      <td>2.924422</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.826600</td>\n",
       "      <td>2.903704</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.652900</td>\n",
       "      <td>2.897018</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 18:45:19,494] Trial 0 finished with value: 0.49504950495049505 and parameters: {'learning_rate': 2.9876834036079085e-05, 'batch_size': 32, 'freeze_pct': 0.4892829271955335}. Best is trial 0 with value: 0.49504950495049505.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2784176985.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:15, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.190200</td>\n",
       "      <td>4.162264</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>0.033003</td>\n",
       "      <td>0.033003</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.060300</td>\n",
       "      <td>4.025545</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.316832</td>\n",
       "      <td>0.316832</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.887000</td>\n",
       "      <td>3.913080</td>\n",
       "      <td>0.394389</td>\n",
       "      <td>0.394389</td>\n",
       "      <td>0.394389</td>\n",
       "      <td>0.394389</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.761000</td>\n",
       "      <td>3.812786</td>\n",
       "      <td>0.420792</td>\n",
       "      <td>0.420792</td>\n",
       "      <td>0.420792</td>\n",
       "      <td>0.420792</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.873700</td>\n",
       "      <td>3.735861</td>\n",
       "      <td>0.442244</td>\n",
       "      <td>0.442244</td>\n",
       "      <td>0.442244</td>\n",
       "      <td>0.442244</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.732200</td>\n",
       "      <td>3.671133</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.455446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.658400</td>\n",
       "      <td>3.620020</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.458746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.482100</td>\n",
       "      <td>3.584121</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.465347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.452800</td>\n",
       "      <td>3.561368</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.481600</td>\n",
       "      <td>3.553952</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 18:49:55,271] Trial 1 finished with value: 0.466996699669967 and parameters: {'learning_rate': 2.1344895723176682e-05, 'batch_size': 8, 'freeze_pct': 0.4751237028768773}. Best is trial 0 with value: 0.49504950495049505.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2784176985.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 07:59, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.523700</td>\n",
       "      <td>4.460560</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.371200</td>\n",
       "      <td>4.355582</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.252800</td>\n",
       "      <td>4.263684</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.142800</td>\n",
       "      <td>4.182294</td>\n",
       "      <td>0.051155</td>\n",
       "      <td>0.051155</td>\n",
       "      <td>0.051155</td>\n",
       "      <td>0.051155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.069300</td>\n",
       "      <td>4.113589</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.148515</td>\n",
       "      <td>0.148515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.027000</td>\n",
       "      <td>4.056708</td>\n",
       "      <td>0.252475</td>\n",
       "      <td>0.252475</td>\n",
       "      <td>0.252475</td>\n",
       "      <td>0.252475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.992100</td>\n",
       "      <td>4.012719</td>\n",
       "      <td>0.338284</td>\n",
       "      <td>0.338284</td>\n",
       "      <td>0.338284</td>\n",
       "      <td>0.338284</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.950200</td>\n",
       "      <td>3.981887</td>\n",
       "      <td>0.371287</td>\n",
       "      <td>0.371287</td>\n",
       "      <td>0.371287</td>\n",
       "      <td>0.371287</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.917200</td>\n",
       "      <td>3.963177</td>\n",
       "      <td>0.400990</td>\n",
       "      <td>0.400990</td>\n",
       "      <td>0.400990</td>\n",
       "      <td>0.400990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.853500</td>\n",
       "      <td>3.956617</td>\n",
       "      <td>0.404290</td>\n",
       "      <td>0.404290</td>\n",
       "      <td>0.404290</td>\n",
       "      <td>0.404290</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 18:58:17,826] Trial 2 finished with value: 0.4042904290429043 and parameters: {'learning_rate': 1.3216685877568885e-05, 'batch_size': 32, 'freeze_pct': 0.7254800724399941}. Best is trial 0 with value: 0.49504950495049505.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2784176985.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:03, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.178800</td>\n",
       "      <td>4.120126</td>\n",
       "      <td>0.127063</td>\n",
       "      <td>0.127063</td>\n",
       "      <td>0.127063</td>\n",
       "      <td>0.127063</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.021800</td>\n",
       "      <td>3.952847</td>\n",
       "      <td>0.381188</td>\n",
       "      <td>0.381188</td>\n",
       "      <td>0.381188</td>\n",
       "      <td>0.381188</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.831700</td>\n",
       "      <td>3.806957</td>\n",
       "      <td>0.425743</td>\n",
       "      <td>0.425743</td>\n",
       "      <td>0.425743</td>\n",
       "      <td>0.425743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.676400</td>\n",
       "      <td>3.677771</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.458746</td>\n",
       "      <td>0.458746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.466600</td>\n",
       "      <td>3.569592</td>\n",
       "      <td>0.468647</td>\n",
       "      <td>0.468647</td>\n",
       "      <td>0.468647</td>\n",
       "      <td>0.468647</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.472000</td>\n",
       "      <td>3.480932</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.488449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.447700</td>\n",
       "      <td>3.413135</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.368000</td>\n",
       "      <td>3.367398</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.301000</td>\n",
       "      <td>3.340787</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.199700</td>\n",
       "      <td>3.331587</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 19:06:45,618] Trial 3 finished with value: 0.4966996699669967 and parameters: {'learning_rate': 2.1641651415244203e-05, 'batch_size': 32, 'freeze_pct': 0.5514573431729268}. Best is trial 3 with value: 0.4966996699669967.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2784176985.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:31, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.221600</td>\n",
       "      <td>4.218982</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.141100</td>\n",
       "      <td>4.133258</td>\n",
       "      <td>0.112211</td>\n",
       "      <td>0.112211</td>\n",
       "      <td>0.112211</td>\n",
       "      <td>0.112211</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.033900</td>\n",
       "      <td>4.062064</td>\n",
       "      <td>0.239274</td>\n",
       "      <td>0.239274</td>\n",
       "      <td>0.239274</td>\n",
       "      <td>0.239274</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.949500</td>\n",
       "      <td>3.998391</td>\n",
       "      <td>0.341584</td>\n",
       "      <td>0.341584</td>\n",
       "      <td>0.341584</td>\n",
       "      <td>0.341584</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>4.046000</td>\n",
       "      <td>3.949198</td>\n",
       "      <td>0.377888</td>\n",
       "      <td>0.377888</td>\n",
       "      <td>0.377888</td>\n",
       "      <td>0.377888</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.950200</td>\n",
       "      <td>3.907484</td>\n",
       "      <td>0.396040</td>\n",
       "      <td>0.396040</td>\n",
       "      <td>0.396040</td>\n",
       "      <td>0.396040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.909900</td>\n",
       "      <td>3.874328</td>\n",
       "      <td>0.399340</td>\n",
       "      <td>0.399340</td>\n",
       "      <td>0.399340</td>\n",
       "      <td>0.399340</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.760200</td>\n",
       "      <td>3.850776</td>\n",
       "      <td>0.409241</td>\n",
       "      <td>0.409241</td>\n",
       "      <td>0.409241</td>\n",
       "      <td>0.409241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.753800</td>\n",
       "      <td>3.835855</td>\n",
       "      <td>0.417492</td>\n",
       "      <td>0.417492</td>\n",
       "      <td>0.417492</td>\n",
       "      <td>0.417492</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.769800</td>\n",
       "      <td>3.830971</td>\n",
       "      <td>0.417492</td>\n",
       "      <td>0.417492</td>\n",
       "      <td>0.417492</td>\n",
       "      <td>0.417492</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 19:11:38,363] Trial 4 finished with value: 0.41749174917491755 and parameters: {'learning_rate': 1.3329082649775433e-05, 'batch_size': 8, 'freeze_pct': 0.5818039532995977}. Best is trial 3 with value: 0.4966996699669967.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2784176985.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:02, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.471000</td>\n",
       "      <td>4.360388</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.230200</td>\n",
       "      <td>4.171486</td>\n",
       "      <td>0.074257</td>\n",
       "      <td>0.074257</td>\n",
       "      <td>0.074257</td>\n",
       "      <td>0.074257</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.021100</td>\n",
       "      <td>4.005983</td>\n",
       "      <td>0.344884</td>\n",
       "      <td>0.344884</td>\n",
       "      <td>0.344884</td>\n",
       "      <td>0.344884</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.832400</td>\n",
       "      <td>3.859475</td>\n",
       "      <td>0.438944</td>\n",
       "      <td>0.438944</td>\n",
       "      <td>0.438944</td>\n",
       "      <td>0.438944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.651100</td>\n",
       "      <td>3.737576</td>\n",
       "      <td>0.476898</td>\n",
       "      <td>0.476898</td>\n",
       "      <td>0.476898</td>\n",
       "      <td>0.476898</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.608300</td>\n",
       "      <td>3.636594</td>\n",
       "      <td>0.490099</td>\n",
       "      <td>0.490099</td>\n",
       "      <td>0.490099</td>\n",
       "      <td>0.490099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.572600</td>\n",
       "      <td>3.560004</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.466900</td>\n",
       "      <td>3.507465</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.423100</td>\n",
       "      <td>3.476377</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.302700</td>\n",
       "      <td>3.465648</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 19:20:04,837] Trial 5 finished with value: 0.49834983498349833 and parameters: {'learning_rate': 2.3948435737103438e-05, 'batch_size': 32, 'freeze_pct': 0.45040950263606655}. Best is trial 5 with value: 0.49834983498349833.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2784176985.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:16, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.085500</td>\n",
       "      <td>3.974114</td>\n",
       "      <td>0.361386</td>\n",
       "      <td>0.361386</td>\n",
       "      <td>0.361386</td>\n",
       "      <td>0.361386</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.793500</td>\n",
       "      <td>3.668435</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.455446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.405800</td>\n",
       "      <td>3.429710</td>\n",
       "      <td>0.481848</td>\n",
       "      <td>0.481848</td>\n",
       "      <td>0.481848</td>\n",
       "      <td>0.481848</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.164100</td>\n",
       "      <td>3.237098</td>\n",
       "      <td>0.490099</td>\n",
       "      <td>0.490099</td>\n",
       "      <td>0.490099</td>\n",
       "      <td>0.490099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.346100</td>\n",
       "      <td>3.109392</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.106900</td>\n",
       "      <td>3.015908</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.974700</td>\n",
       "      <td>2.953489</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.737600</td>\n",
       "      <td>2.918473</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.701300</td>\n",
       "      <td>2.897051</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.776200</td>\n",
       "      <td>2.890987</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:18]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 19:24:40,761] Trial 6 finished with value: 0.49834983498349833 and parameters: {'learning_rate': 4.84701464382748e-05, 'batch_size': 8, 'freeze_pct': 0.5770077788946468}. Best is trial 5 with value: 0.49834983498349833.\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\2784176985.py:33: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 08:02, Epoch 2/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.485000</td>\n",
       "      <td>4.387046</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.267700</td>\n",
       "      <td>4.220619</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>4.082500</td>\n",
       "      <td>4.074772</td>\n",
       "      <td>0.231023</td>\n",
       "      <td>0.231023</td>\n",
       "      <td>0.231023</td>\n",
       "      <td>0.231023</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.914500</td>\n",
       "      <td>3.945223</td>\n",
       "      <td>0.410891</td>\n",
       "      <td>0.410891</td>\n",
       "      <td>0.410891</td>\n",
       "      <td>0.410891</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>3.761600</td>\n",
       "      <td>3.836937</td>\n",
       "      <td>0.448845</td>\n",
       "      <td>0.448845</td>\n",
       "      <td>0.448845</td>\n",
       "      <td>0.448845</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.718800</td>\n",
       "      <td>3.747179</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.475248</td>\n",
       "      <td>0.475248</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.681900</td>\n",
       "      <td>3.678478</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.485149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.592400</td>\n",
       "      <td>3.630522</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.550800</td>\n",
       "      <td>3.601624</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.445000</td>\n",
       "      <td>3.591578</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:19]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-17 19:33:07,005] Trial 7 finished with value: 0.4966996699669967 and parameters: {'learning_rate': 2.108399631517385e-05, 'batch_size': 32, 'freeze_pct': 0.46364901563028477}. Best is trial 5 with value: 0.49834983498349833.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best RE Freeze params: {'learning_rate': 2.3948435737103438e-05, 'batch_size': 32, 'freeze_pct': 0.45040950263606655} → Dev-F1 = 0.49834983498349833\n"
     ]
    }
   ],
   "source": [
    "# === Chapter 35: RE Hyperparameter-Tuning (Partial-Freeze) with Steps ===\n",
    "\n",
    "def re_freeze_objective(trial):\n",
    "    lr  = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    bs  = trial.suggest_categorical(\"batch_size\", [8, 16, 32])\n",
    "    pct = trial.suggest_float(\"freeze_pct\", 0.25, 0.75)\n",
    "\n",
    "    m = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=len(label2id_re),\n",
    "        id2label=id2label_re,\n",
    "        label2id=label2id_re,\n",
    "    )\n",
    "    total = len([n for n,_ in m.named_parameters() if n.startswith(\"bert.encoder.layer.\")])\n",
    "    cut   = int(total * pct)\n",
    "    for name, param in m.named_parameters():\n",
    "        if name.startswith(\"bert.encoder.layer.\") and int(name.split(\".\")[3]) < cut:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/re-freeze-{trial.number}\",\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs,\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=10,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=m,\n",
    "        args=args,\n",
    "        train_dataset=hf_train_re,\n",
    "        eval_dataset=hf_dev_re,\n",
    "        tokenizer=tokenizer_re,\n",
    "        compute_metrics=compute_metrics_re,\n",
    "    )\n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "study_re_freeze = optuna.create_study(direction=\"maximize\")\n",
    "study_re_freeze.optimize(re_freeze_objective, n_trials=8)\n",
    "print(\"🏆 Best RE Freeze params:\", study_re_freeze.best_params, \"→ Dev-F1 =\", study_re_freeze.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "676a08fc-c682-4dff-826c-b14c9b2d6669",
   "metadata": {},
   "source": [
    "## 36: Final Full Fine-Tuning für RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "81776df9-41f9-498d-8308-373276f651c0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1876714405.py:53: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  ft_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 13:49, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.702700</td>\n",
       "      <td>2.904846</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.694800</td>\n",
       "      <td>2.563470</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.568700</td>\n",
       "      <td>2.506935</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.534700</td>\n",
       "      <td>2.445755</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.520200</td>\n",
       "      <td>2.419361</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.378500</td>\n",
       "      <td>2.339497</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.392700</td>\n",
       "      <td>2.286658</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.210200</td>\n",
       "      <td>2.253080</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.301100</td>\n",
       "      <td>2.197468</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.308200</td>\n",
       "      <td>2.202842</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:24]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[143], line 66\u001b[0m\n\u001b[0;32m     64\u001b[0m ft_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     65\u001b[0m dev_metrics  \u001b[38;5;241m=\u001b[39m ft_trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 66\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m ft_trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔖 RE Final Full-FT Dev:\u001b[39m\u001b[38;5;124m\"\u001b[39m,  dev_metrics)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🏁 RE Final Full-FT Test:\u001b[39m\u001b[38;5;124m\"\u001b[39m, test_metrics)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\dataset_dict.py:82\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(k)\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     85\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m     86\u001b[0m         ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "# === Kapitel 36: Final Full Fine-Tuning für RE (korrigiert) ===\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import (\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "\n",
    "# --- compute_metrics noch mal definieren, falls nicht global ---\n",
    "def compute_metrics_re(p):\n",
    "    preds  = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    mask   = labels >= 0\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels[mask], preds[mask], average=\"micro\"\n",
    "    )\n",
    "    acc = accuracy_score(labels[mask], preds[mask])\n",
    "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "# 1) Train+Dev zusammenfassen\n",
    "full_train = concatenate_datasets([ds[\"train\"], ds[\"dev\"]])\n",
    "\n",
    "# 2) Beste FT-Parameter (aus Kap.33)\n",
    "best_ft = study_re_ft.best_params\n",
    "\n",
    "# 3) DataCollator & Modell laden\n",
    "data_collator_re = DataCollatorWithPadding(tokenizer=tokenizer_re)\n",
    "ft_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label2id_re),\n",
    "    id2label=id2label_re,\n",
    "    label2id=label2id_re,\n",
    ")\n",
    "\n",
    "# 4) Trainings-Arguments\n",
    "ft_args = TrainingArguments(\n",
    "    output_dir=\"outputs/re-final-ft\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=200,\n",
    "    per_device_train_batch_size=best_ft[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_ft[\"batch_size\"] * 2,\n",
    "    learning_rate=best_ft[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# 5) Trainer initialisieren — jetzt MIT eval_dataset!\n",
    "ft_trainer = Trainer(\n",
    "    model=ft_model,\n",
    "    args=ft_args,\n",
    "    train_dataset=full_train,\n",
    "    eval_dataset=ds[\"dev\"],           # ← hier fehlt(e) es vorher\n",
    "    tokenizer=tokenizer_re,\n",
    "    data_collator=data_collator_re,\n",
    "    compute_metrics=compute_metrics_re,  # ← und hier die Metriken\n",
    ")\n",
    "\n",
    "# 6) Train & abschließende Dev-/Test-Evaluation\n",
    "ft_trainer.train()\n",
    "dev_metrics  = ft_trainer.evaluate(eval_dataset=ds[\"dev\"])\n",
    "test_metrics = ft_trainer.evaluate(eval_dataset=ds[\"test\"])\n",
    "print(\"🔖 RE Final Full-FT Dev:\",  dev_metrics)\n",
    "print(\"🏁 RE Final Full-FT Test:\", test_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ef5b777-aff3-4265-9583-a9c37a31f76c",
   "metadata": {},
   "source": [
    "## Kapitel 37: Final LoRA Fine-Tuning für RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "6b8d1c8c-cef1-4400-b4d2-593773b01700",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unmatched ')' (1851520589.py, line 87)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[156], line 87\u001b[1;36m\u001b[0m\n\u001b[1;33m    )\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unmatched ')'\n"
     ]
    }
   ],
   "source": [
    "# === Kapitel 37: Final LoRA Fine-Tuning für RE ===\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "import torch\n",
    "\n",
    "# 1) Metrics-Funktion\n",
    "def compute_metrics(p):\n",
    "    preds  = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    mask   = labels >= 0\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels[mask], preds[mask], average=\"micro\"\n",
    "    )\n",
    "    acc = accuracy_score(labels[mask], preds[mask])\n",
    "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "# 2) Deine besten LoRA-Hyperparams\n",
    "lr      = 0.00032032068892167987\n",
    "r       = 8\n",
    "alpha   = 32\n",
    "dropout = 0.19912026952009523\n",
    "bs      = 16\n",
    "\n",
    "# 3) Modell + LoRA-Adapter\n",
    "base_model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label2id_re),\n",
    "    id2label=id2label_re,\n",
    "    label2id=label2id_re,\n",
    ")\n",
    "lora_conf = LoraConfig(\n",
    "    task_type=\"SEQ_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=r,\n",
    "    lora_alpha=alpha,\n",
    "    lora_dropout=dropout,\n",
    ")\n",
    "model = get_peft_model(base_model, lora_conf)\n",
    "\n",
    "# 4) DataCollator & TrainingsArguments\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer_re)\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"outputs/re-final-lora\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=200,\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs * 2,\n",
    "    learning_rate=lr,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# 5) Train+Dev zusammenfassen\n",
    "full_train = concatenate_datasets([ds[\"train\"], ds[\"dev\"]])\n",
    "\n",
    "# 6) Trainer initialisieren\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=full_train,\n",
    "    eval_dataset=ds[\"dev\"],\n",
    "    tokenizer=tokenizer_re,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 7) Trainieren und auswerten\n",
    "trainer.train()\n",
    "dev_metrics  = trainer.evaluate(eval_dataset=ds[\"dev\"])\n",
    "test_metrics = trainer.evaluate(eval_dataset=ds[\"test\"])\n",
    "\n",
    "print(\"🔖 Final RE LoRA — Dev:\",  dev_metrics)\n",
    "print(\"🏁 Final RE LoRA — Test:\", test_metrics)\n",
    "mpute_metrics_re,\r\n",
    ")\r\n",
    "lora_trainer.train()\r\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5288b9f4-7cc1-4302-8750-63e2598fb38b",
   "metadata": {},
   "source": [
    "## 38: Final Partial-Freeze Fine-Tuning für RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "ed2593bf-5c8b-4f11-aac0-5261969c07dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\499469787.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  freeze_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='200' max='200' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [200/200 16:36, Epoch 3/4]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.151100</td>\n",
       "      <td>3.904321</td>\n",
       "      <td>0.400990</td>\n",
       "      <td>0.400990</td>\n",
       "      <td>0.400990</td>\n",
       "      <td>0.400990</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.721800</td>\n",
       "      <td>3.458064</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.347300</td>\n",
       "      <td>3.103712</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.029500</td>\n",
       "      <td>2.846551</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.908600</td>\n",
       "      <td>2.686925</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.646700</td>\n",
       "      <td>2.598918</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>2.606700</td>\n",
       "      <td>2.554867</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>2.587400</td>\n",
       "      <td>2.531546</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>2.641600</td>\n",
       "      <td>2.518265</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>2.531000</td>\n",
       "      <td>2.513658</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='10' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [10/10 00:23]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyError",
     "evalue": "'test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[148], line 63\u001b[0m\n\u001b[0;32m     61\u001b[0m freeze_trainer\u001b[38;5;241m.\u001b[39mtrain()\n\u001b[0;32m     62\u001b[0m dev_metrics  \u001b[38;5;241m=\u001b[39m freeze_trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdev\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m---> 63\u001b[0m test_metrics \u001b[38;5;241m=\u001b[39m freeze_trainer\u001b[38;5;241m.\u001b[39mevaluate(eval_dataset\u001b[38;5;241m=\u001b[39mds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔖 RE Final Freeze Dev:\u001b[39m\u001b[38;5;124m\"\u001b[39m,  dev_metrics)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\datasets\\dataset_dict.py:82\u001b[0m, in \u001b[0;36mDatasetDict.__getitem__\u001b[1;34m(self, k)\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__getitem__\u001b[39m(\u001b[38;5;28mself\u001b[39m, k) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Dataset:\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, (\u001b[38;5;28mstr\u001b[39m, NamedSplit)) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 82\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__getitem__\u001b[39m(k)\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m         available_suggested_splits \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     85\u001b[0m             split \u001b[38;5;28;01mfor\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m (Split\u001b[38;5;241m.\u001b[39mTRAIN, Split\u001b[38;5;241m.\u001b[39mTEST, Split\u001b[38;5;241m.\u001b[39mVALIDATION) \u001b[38;5;28;01mif\u001b[39;00m split \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[0;32m     86\u001b[0m         ]\n",
      "\u001b[1;31mKeyError\u001b[0m: 'test'"
     ]
    }
   ],
   "source": [
    "# === Kapitel 38: Final Partial-Freeze Fine-Tuning für RE ===\n",
    "\n",
    "def compute_metrics_re(p):\n",
    "    preds  = p.predictions.argmax(-1)\n",
    "    labels = p.label_ids\n",
    "    mask   = labels >= 0\n",
    "    prec, rec, f1, _ = precision_recall_fscore_support(\n",
    "        labels[mask], preds[mask], average=\"micro\"\n",
    "    )\n",
    "    acc = accuracy_score(labels[mask], preds[mask])\n",
    "    return {\"precision\": prec, \"recall\": rec, \"f1\": f1, \"accuracy\": acc}\n",
    "\n",
    "# 1) Train+Dev zusammenfassen\n",
    "full_train = concatenate_datasets([ds[\"train\"], ds[\"dev\"]])\n",
    "\n",
    "# 2) Beste Freeze-Parameter (aus Kapitel 35)\n",
    "best_freeze = study_re_freeze.best_params\n",
    "pct         = best_freeze[\"freeze_pct\"]\n",
    "bs          = best_freeze[\"batch_size\"]\n",
    "lr          = best_freeze[\"learning_rate\"]\n",
    "\n",
    "# 3) Modell laden und einfrieren\n",
    "m = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"bert-base-uncased\",\n",
    "    num_labels=len(label2id_re),\n",
    "    id2label=id2label_re,\n",
    "    label2id=label2id_re,\n",
    ")\n",
    "total_layers = len([n for n,_ in m.named_parameters() if n.startswith(\"bert.encoder.layer.\")])\n",
    "cutoff = int(total_layers * pct)\n",
    "for name, param in m.named_parameters():\n",
    "    if name.startswith(\"bert.encoder.layer.\") and int(name.split(\".\")[3]) < cutoff:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# 4) DataCollator & TrainingArguments\n",
    "data_collator_re = DataCollatorWithPadding(tokenizer=tokenizer_re)\n",
    "freeze_args = TrainingArguments(\n",
    "    output_dir=\"outputs/re-final-freeze\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=20,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=200,\n",
    "    per_device_train_batch_size=bs,\n",
    "    per_device_eval_batch_size=bs * 2,\n",
    "    learning_rate=lr,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# 5) Trainer & Train/Metriken\n",
    "freeze_trainer = Trainer(\n",
    "    model=m,\n",
    "    args=freeze_args,\n",
    "    train_dataset=full_train,\n",
    "    eval_dataset=ds[\"dev\"],\n",
    "    tokenizer=tokenizer_re,\n",
    "    data_collator=data_collator_re,\n",
    "    compute_metrics=compute_metrics_re,\n",
    ")\n",
    "freeze_trainer.train()\n",
    "dev_metrics  = freeze_trainer.evaluate(eval_dataset=ds[\"dev\"])\n",
    "test_metrics = freeze_trainer.evaluate(eval_dataset=ds[\"test\"])\n",
    "print(\"🔖 RE Final Freeze Dev:\",  dev_metrics)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff3923d-5376-4dbb-8e52-5a7baca273af",
   "metadata": {},
   "source": [
    "## Kapitel 39: Hyperparameter-Tuning Full Fine-Tuning für GPT-Neo (RE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "ebad240c-9d9d-4a0b-b344-ef2032d4c356",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 02:51:22,154] A new study created in memory with name: no-name-e975b468-04f0-46a6-a5bd-82e233d60e48\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3330673949.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 07:15, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.698800</td>\n",
       "      <td>3.065078</td>\n",
       "      <td>0.490099</td>\n",
       "      <td>0.490099</td>\n",
       "      <td>0.490099</td>\n",
       "      <td>0.490099</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.533500</td>\n",
       "      <td>2.902600</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.393800</td>\n",
       "      <td>2.871885</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.288800</td>\n",
       "      <td>2.851628</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.755500</td>\n",
       "      <td>2.866771</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 02:59:15,366] Trial 0 finished with value: 0.5 and parameters: {'learning_rate': 2.1593610305165778e-05, 'batch_size': 16}. Best is trial 0 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3330673949.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:11, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.960400</td>\n",
       "      <td>3.273297</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.229900</td>\n",
       "      <td>2.957267</td>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.491749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.443800</td>\n",
       "      <td>2.874211</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.933700</td>\n",
       "      <td>2.857775</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.953800</td>\n",
       "      <td>2.849956</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:31]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:04:01,707] Trial 1 finished with value: 0.49834983498349833 and parameters: {'learning_rate': 1.6017733468724378e-05, 'batch_size': 4}. Best is trial 0 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3330673949.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:44, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.364300</td>\n",
       "      <td>3.138107</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.805100</td>\n",
       "      <td>2.843173</td>\n",
       "      <td>0.501650</td>\n",
       "      <td>0.501650</td>\n",
       "      <td>0.501650</td>\n",
       "      <td>0.501650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.010800</td>\n",
       "      <td>2.834888</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.566900</td>\n",
       "      <td>2.820672</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.515500</td>\n",
       "      <td>2.801967</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:09:20,255] Trial 2 finished with value: 0.49834983498349833 and parameters: {'learning_rate': 1.3189981191712024e-05, 'batch_size': 8}. Best is trial 0 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3330673949.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 06:21, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.774400</td>\n",
       "      <td>2.906713</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.563700</td>\n",
       "      <td>2.783278</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.241000</td>\n",
       "      <td>2.798984</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.235500</td>\n",
       "      <td>2.771923</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.626500</td>\n",
       "      <td>2.806891</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:16:13,521] Trial 3 finished with value: 0.5 and parameters: {'learning_rate': 4.312986845029776e-05, 'batch_size': 16}. Best is trial 0 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3330673949.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 04:34, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.972200</td>\n",
       "      <td>2.980735</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.523000</td>\n",
       "      <td>2.847092</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.834500</td>\n",
       "      <td>2.829383</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.339100</td>\n",
       "      <td>2.829108</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.393000</td>\n",
       "      <td>2.811469</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:21:19,210] Trial 4 finished with value: 0.5 and parameters: {'learning_rate': 4.690273591206409e-05, 'batch_size': 8}. Best is trial 0 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3330673949.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:41, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.994500</td>\n",
       "      <td>3.266162</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.263800</td>\n",
       "      <td>2.956435</td>\n",
       "      <td>0.432343</td>\n",
       "      <td>0.432343</td>\n",
       "      <td>0.432343</td>\n",
       "      <td>0.432343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.316100</td>\n",
       "      <td>2.780778</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.991200</td>\n",
       "      <td>2.771970</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.880100</td>\n",
       "      <td>2.765402</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:25:30,021] Trial 5 finished with value: 0.5 and parameters: {'learning_rate': 2.1586648731832532e-05, 'batch_size': 4}. Best is trial 0 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3330673949.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 06:16, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.993000</td>\n",
       "      <td>2.925017</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.879600</td>\n",
       "      <td>2.817191</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.423200</td>\n",
       "      <td>2.803146</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.485149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.491300</td>\n",
       "      <td>2.774386</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.488449</td>\n",
       "      <td>0.488449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.874300</td>\n",
       "      <td>2.781367</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:32:19,567] Trial 6 finished with value: 0.4966996699669967 and parameters: {'learning_rate': 1.2559225392801358e-05, 'batch_size': 16}. Best is trial 0 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3330673949.py:40: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 06:17, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.004200</td>\n",
       "      <td>3.069555</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.842700</td>\n",
       "      <td>2.905274</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.576300</td>\n",
       "      <td>2.831778</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.396000</td>\n",
       "      <td>2.793318</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.992100</td>\n",
       "      <td>2.788931</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:39:09,607] Trial 7 finished with value: 0.5 and parameters: {'learning_rate': 1.0147296366379076e-05, 'batch_size': 16}. Best is trial 0 with value: 0.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best GPT-Neo RE Full-FT params: {'learning_rate': 2.1593610305165778e-05, 'batch_size': 16} → Dev-F1 = 0.5\n"
     ]
    }
   ],
   "source": [
    "# === Kapitel 39: GPT-Neo RE Full Fine-Tuning Hyperparam-Tuning ===\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "def neo_re_ft_objective(trial):\n",
    "    # 1) Hyper-Parameter\n",
    "    lr = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    bs = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "\n",
    "    # 2) Modell laden\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"EleutherAI/gpt-neo-125M\",\n",
    "        num_labels=len(label2id_re),\n",
    "        id2label=id2label_re,\n",
    "        label2id=label2id_re,\n",
    "    )\n",
    "    # ↑ Fix: pad_token_id setzen auf Dein RE-Tokenizer pad (BERT-pad=0)\n",
    "    model.config.pad_token_id = tokenizer_re.pad_token_id\n",
    "\n",
    "    # 3) Trainings-Argumente\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/neo-re-ft-{trial.number}\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs*2,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # 4) Trainer initialisieren\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"dev\"],\n",
    "        tokenizer=tokenizer_re,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer_re),\n",
    "        compute_metrics=compute_metrics_re,\n",
    "    )\n",
    "\n",
    "    # 5) Train & Eval\n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "\n",
    "study_neo_re_ft = optuna.create_study(direction=\"maximize\")\n",
    "study_neo_re_ft.optimize(neo_re_ft_objective, n_trials=8)\n",
    "\n",
    "print(\"🏆 Best GPT-Neo RE Full-FT params:\", study_neo_re_ft.best_params,\n",
    "      \"→ Dev-F1 =\", study_neo_re_ft.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f96d5fc1-70d6-43aa-8f4b-1427aa4b82dc",
   "metadata": {},
   "source": [
    "## Kapitel 40: GPT-Neo RE LoRA Hyperparam-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "be812e76-badb-4ead-8e47-39cd029f3dd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:39:09,619] A new study created in memory with name: no-name-3a553752-5f24-4ca7-9016-2c585c7090c3\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3332536071.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.764900</td>\n",
       "      <td>5.215204</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.940400</td>\n",
       "      <td>4.878578</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.795100</td>\n",
       "      <td>4.649555</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.578300</td>\n",
       "      <td>4.511791</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.367900</td>\n",
       "      <td>4.463237</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.024752</td>\n",
       "      <td>0.024752</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:43:31,260] Trial 0 finished with value: 0.024752475247524754 and parameters: {'learning_rate': 2.0656131108914558e-05, 'r': 16, 'alpha': 16, 'dropout': 0.026479857787921366, 'batch_size': 8}. Best is trial 0 with value: 0.024752475247524754.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3332536071.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:50, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.104700</td>\n",
       "      <td>3.496833</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.476700</td>\n",
       "      <td>3.250067</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.000800</td>\n",
       "      <td>3.265537</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.325500</td>\n",
       "      <td>3.245646</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.543900</td>\n",
       "      <td>3.229023</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:47:53,577] Trial 1 finished with value: 0.5 and parameters: {'learning_rate': 0.0002827866072068735, 'r': 16, 'alpha': 16, 'dropout': 0.255526969488984, 'batch_size': 8}. Best is trial 1 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3332536071.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.564600</td>\n",
       "      <td>3.922038</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.714500</td>\n",
       "      <td>3.370856</td>\n",
       "      <td>0.480198</td>\n",
       "      <td>0.480198</td>\n",
       "      <td>0.480198</td>\n",
       "      <td>0.480198</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.504900</td>\n",
       "      <td>3.273753</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.125600</td>\n",
       "      <td>3.295169</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.989600</td>\n",
       "      <td>3.273296</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:51:14,711] Trial 2 finished with value: 0.49834983498349833 and parameters: {'learning_rate': 0.00043383939848244343, 'r': 4, 'alpha': 32, 'dropout': 0.09455558574559031, 'batch_size': 4}. Best is trial 1 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3332536071.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:51, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.361500</td>\n",
       "      <td>3.794927</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.784200</td>\n",
       "      <td>3.441583</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "      <td>0.466997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.541700</td>\n",
       "      <td>3.250133</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.106200</td>\n",
       "      <td>3.308109</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.094900</td>\n",
       "      <td>3.293999</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:54:34,629] Trial 3 finished with value: 0.49834983498349833 and parameters: {'learning_rate': 0.0003421651976998089, 'r': 4, 'alpha': 16, 'dropout': 0.1559480095913568, 'batch_size': 4}. Best is trial 1 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3332536071.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:51, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.156800</td>\n",
       "      <td>3.858274</td>\n",
       "      <td>0.100660</td>\n",
       "      <td>0.100660</td>\n",
       "      <td>0.100660</td>\n",
       "      <td>0.100660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.643400</td>\n",
       "      <td>3.086512</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.920200</td>\n",
       "      <td>3.057168</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.379300</td>\n",
       "      <td>3.067980</td>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.491749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.604200</td>\n",
       "      <td>3.069056</td>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.491749</td>\n",
       "      <td>0.491749</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 03:58:58,397] Trial 4 finished with value: 0.49174917491749176 and parameters: {'learning_rate': 9.501753675852592e-05, 'r': 4, 'alpha': 32, 'dropout': 0.24248496327515434, 'batch_size': 8}. Best is trial 1 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3332536071.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:49, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.335000</td>\n",
       "      <td>4.123280</td>\n",
       "      <td>0.041254</td>\n",
       "      <td>0.041254</td>\n",
       "      <td>0.041254</td>\n",
       "      <td>0.041254</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.884600</td>\n",
       "      <td>3.088180</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.465347</td>\n",
       "      <td>0.465347</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.923200</td>\n",
       "      <td>3.063572</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.402800</td>\n",
       "      <td>3.074744</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.621500</td>\n",
       "      <td>3.074718</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:03:19,248] Trial 5 finished with value: 0.5 and parameters: {'learning_rate': 9.797567727942517e-05, 'r': 16, 'alpha': 16, 'dropout': 0.10715511849770963, 'batch_size': 8}. Best is trial 1 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3332536071.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:51, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.422900</td>\n",
       "      <td>3.693619</td>\n",
       "      <td>0.339934</td>\n",
       "      <td>0.339934</td>\n",
       "      <td>0.339934</td>\n",
       "      <td>0.339934</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.620400</td>\n",
       "      <td>3.393147</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.035900</td>\n",
       "      <td>3.188347</td>\n",
       "      <td>0.501650</td>\n",
       "      <td>0.501650</td>\n",
       "      <td>0.501650</td>\n",
       "      <td>0.501650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.275600</td>\n",
       "      <td>3.198689</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.398600</td>\n",
       "      <td>3.165586</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:07:42,179] Trial 6 finished with value: 0.5 and parameters: {'learning_rate': 0.0007101062145830903, 'r': 4, 'alpha': 32, 'dropout': 0.10464782707469777, 'batch_size': 8}. Best is trial 1 with value: 0.5.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3332536071.py:52: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n",
      "No label_names provided for model class `PeftModelForSequenceClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 02:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.517500</td>\n",
       "      <td>5.494432</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.093200</td>\n",
       "      <td>5.195218</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.315300</td>\n",
       "      <td>4.975839</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.838100</td>\n",
       "      <td>4.840773</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.695600</td>\n",
       "      <td>4.799141</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:26]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:11:03,549] Trial 7 finished with value: 0.009900990099009901 and parameters: {'learning_rate': 2.288000633868335e-05, 'r': 4, 'alpha': 16, 'dropout': 0.08153525636079527, 'batch_size': 4}. Best is trial 1 with value: 0.5.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best GPT-Neo RE LoRA params: {'learning_rate': 0.0002827866072068735, 'r': 16, 'alpha': 16, 'dropout': 0.255526969488984, 'batch_size': 8} → Dev-F1 = 0.5\n"
     ]
    }
   ],
   "source": [
    "# === Kapitel 40: GPT-Neo RE LoRA Hyperparam-Tuning ===\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "def neo_re_lora_objective(trial):\n",
    "    # 1) Hyper-Params\n",
    "    lr      = trial.suggest_float(\"learning_rate\", 1e-5, 1e-3, log=True)\n",
    "    r       = trial.suggest_categorical(\"r\", [4, 8, 16])\n",
    "    alpha   = trial.suggest_categorical(\"alpha\", [16, 32])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.0, 0.3)\n",
    "    bs      = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "\n",
    "    # 2) LoRA aufsetzen\n",
    "    lora_conf = LoraConfig(\n",
    "        task_type=\"SEQ_CLS\",\n",
    "        inference_mode=False,\n",
    "        r=r,\n",
    "        lora_alpha=alpha,\n",
    "        lora_dropout=dropout,\n",
    "    )\n",
    "    base = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"EleutherAI/gpt-neo-125M\",\n",
    "        num_labels=len(label2id_re),\n",
    "        id2label=id2label_re,\n",
    "        label2id=label2id_re,\n",
    "    )\n",
    "    # Fix pad_token_id\n",
    "    base.config.pad_token_id = tokenizer_re.pad_token_id\n",
    "    lora_model = get_peft_model(base, lora_conf)\n",
    "\n",
    "    # 3) Trainings-Args\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/neo-re-lora-{trial.number}\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs*2,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # 4) Trainer\n",
    "    trainer = Trainer(\n",
    "        model=lora_model,\n",
    "        args=args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"dev\"],\n",
    "        tokenizer=tokenizer_re,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer_re),\n",
    "        compute_metrics=compute_metrics_re,\n",
    "    )\n",
    "\n",
    "    # 5) Train & Eval\n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "\n",
    "study_neo_re_lora = optuna.create_study(direction=\"maximize\")\n",
    "study_neo_re_lora.optimize(neo_re_lora_objective, n_trials=8)\n",
    "\n",
    "print(\"🏆 Best GPT-Neo RE LoRA params:\", study_neo_re_lora.best_params,\n",
    "      \"→ Dev-F1 =\", study_neo_re_lora.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6fe155-730a-4b50-926c-fafa0a2f47a9",
   "metadata": {},
   "source": [
    "## Kapitel 41: GPT-Neo RE Partial-Freeze Hyperparam-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "fe8bda63-1535-43da-9e4f-f96856989cf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:11:03,560] A new study created in memory with name: no-name-0d2a3390-569c-4772-93c5-83461bd75b45\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3963042673.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:55, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.284000</td>\n",
       "      <td>5.385854</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.024400</td>\n",
       "      <td>5.002536</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "      <td>0.008251</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.602100</td>\n",
       "      <td>4.758359</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "      <td>0.009901</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.046400</td>\n",
       "      <td>4.618761</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>0.014851</td>\n",
       "      <td>0.014851</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.255500</td>\n",
       "      <td>4.570947</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "      <td>0.016502</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:15:29,880] Trial 0 finished with value: 0.0165016501650165 and parameters: {'learning_rate': 2.351527731326995e-05, 'batch_size': 8, 'freeze_pct': 0.48507277200501586}. Best is trial 0 with value: 0.0165016501650165.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3963042673.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>4.941900</td>\n",
       "      <td>4.781651</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.467400</td>\n",
       "      <td>4.246407</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>3.970900</td>\n",
       "      <td>3.908218</td>\n",
       "      <td>0.067657</td>\n",
       "      <td>0.067657</td>\n",
       "      <td>0.067657</td>\n",
       "      <td>0.067657</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.583500</td>\n",
       "      <td>3.724397</td>\n",
       "      <td>0.132013</td>\n",
       "      <td>0.132013</td>\n",
       "      <td>0.132013</td>\n",
       "      <td>0.132013</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>3.524700</td>\n",
       "      <td>3.662180</td>\n",
       "      <td>0.155116</td>\n",
       "      <td>0.155116</td>\n",
       "      <td>0.155116</td>\n",
       "      <td>0.155116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:28]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:19:56,666] Trial 1 finished with value: 0.1551155115511551 and parameters: {'learning_rate': 3.464080464231026e-05, 'batch_size': 8, 'freeze_pct': 0.4393379142919582}. Best is trial 1 with value: 0.1551155115511551.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3963042673.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:23, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.453400</td>\n",
       "      <td>5.178004</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.114400</td>\n",
       "      <td>4.934453</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.744100</td>\n",
       "      <td>4.764187</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.679000</td>\n",
       "      <td>4.661656</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.534000</td>\n",
       "      <td>4.627132</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:25:52,008] Trial 2 finished with value: 0.006600660066006601 and parameters: {'learning_rate': 1.3682997912141893e-05, 'batch_size': 16, 'freeze_pct': 0.5351551386727706}. Best is trial 1 with value: 0.1551155115511551.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3963042673.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.503500</td>\n",
       "      <td>5.419997</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.657100</td>\n",
       "      <td>5.260497</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>5.193200</td>\n",
       "      <td>5.147106</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.532500</td>\n",
       "      <td>5.077394</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>5.296500</td>\n",
       "      <td>5.056298</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:29:21,943] Trial 3 finished with value: 0.0016501650165016502 and parameters: {'learning_rate': 1.0390261096975955e-05, 'batch_size': 4, 'freeze_pct': 0.3403133527773655}. Best is trial 1 with value: 0.1551155115511551.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3963042673.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 05:25, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.448700</td>\n",
       "      <td>5.173009</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.105000</td>\n",
       "      <td>4.925798</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.731300</td>\n",
       "      <td>4.753035</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.663900</td>\n",
       "      <td>4.649024</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.515400</td>\n",
       "      <td>4.614009</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:35:18,965] Trial 4 finished with value: 0.006600660066006601 and parameters: {'learning_rate': 1.3896797585741956e-05, 'batch_size': 16, 'freeze_pct': 0.4348566215743214}. Best is trial 1 with value: 0.1551155115511551.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3963042673.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:03, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.202400</td>\n",
       "      <td>5.096635</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "      <td>0.001650</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>5.119800</td>\n",
       "      <td>4.690338</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011551</td>\n",
       "      <td>0.011551</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.265700</td>\n",
       "      <td>4.415658</td>\n",
       "      <td>0.031353</td>\n",
       "      <td>0.031353</td>\n",
       "      <td>0.031353</td>\n",
       "      <td>0.031353</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.455100</td>\n",
       "      <td>4.253765</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>0.059406</td>\n",
       "      <td>0.059406</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.371600</td>\n",
       "      <td>4.206640</td>\n",
       "      <td>0.066007</td>\n",
       "      <td>0.066007</td>\n",
       "      <td>0.066007</td>\n",
       "      <td>0.066007</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:38:50,142] Trial 5 finished with value: 0.066006600660066 and parameters: {'learning_rate': 2.7867044773781677e-05, 'batch_size': 4, 'freeze_pct': 0.6510004317785507}. Best is trial 1 with value: 0.1551155115511551.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3963042673.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:54, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.150900</td>\n",
       "      <td>5.031020</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "      <td>0.003300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.950000</td>\n",
       "      <td>4.672598</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.515900</td>\n",
       "      <td>4.434186</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.266000</td>\n",
       "      <td>4.297263</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.201200</td>\n",
       "      <td>4.249631</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:27]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:43:15,119] Trial 6 finished with value: 0.0049504950495049506 and parameters: {'learning_rate': 2.2426496344383954e-05, 'batch_size': 8, 'freeze_pct': 0.5896594545402926}. Best is trial 1 with value: 0.1551155115511551.\n",
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3963042673.py:46: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 03:02, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>5.293500</td>\n",
       "      <td>5.083532</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.686400</td>\n",
       "      <td>4.770689</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>4.556400</td>\n",
       "      <td>4.554609</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "      <td>0.004950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>4.466800</td>\n",
       "      <td>4.422428</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.319700</td>\n",
       "      <td>4.381373</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "      <td>0.006601</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:25]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[I 2025-05-18 04:46:45,066] Trial 7 finished with value: 0.006600660066006601 and parameters: {'learning_rate': 2.2559916493357485e-05, 'batch_size': 4, 'freeze_pct': 0.3952181431193523}. Best is trial 1 with value: 0.1551155115511551.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🏆 Best GPT-Neo RE Freeze params: {'learning_rate': 3.464080464231026e-05, 'batch_size': 8, 'freeze_pct': 0.4393379142919582} → Dev-F1 = 0.1551155115511551\n"
     ]
    }
   ],
   "source": [
    "# === Kapitel 41: GPT-Neo RE Partial-Freeze Hyperparam-Tuning ===\n",
    "\n",
    "import optuna\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification, Trainer, TrainingArguments,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "def neo_re_freeze_objective(trial):\n",
    "    # 1) Hyper-Params\n",
    "    lr  = trial.suggest_float(\"learning_rate\", 1e-5, 5e-5, log=True)\n",
    "    bs  = trial.suggest_categorical(\"batch_size\", [4, 8, 16])\n",
    "    pct = trial.suggest_float(\"freeze_pct\", 0.25, 0.75)\n",
    "\n",
    "    # 2) Modell laden & einfrieren\n",
    "    m = AutoModelForSequenceClassification.from_pretrained(\n",
    "        \"EleutherAI/gpt-neo-125M\",\n",
    "        num_labels=len(label2id_re),\n",
    "        id2label=id2label_re,\n",
    "        label2id=label2id_re,\n",
    "    )\n",
    "    m.config.pad_token_id = tokenizer_re.pad_token_id\n",
    "\n",
    "    total = len([n for n,_ in m.named_parameters() if n.startswith(\"transformer.h.\")])\n",
    "    cut   = int(total * pct)\n",
    "    for name,param in m.named_parameters():\n",
    "        if name.startswith(\"transformer.h.\") and int(name.split(\".\")[2]) < cut:\n",
    "            param.requires_grad = False\n",
    "\n",
    "    # 3) Trainings-Args\n",
    "    args = TrainingArguments(\n",
    "        output_dir=f\"tmp/neo-re-freeze-{trial.number}\",\n",
    "        evaluation_strategy=\"steps\",\n",
    "        eval_steps=20,\n",
    "        logging_steps=10,\n",
    "        save_strategy=\"no\",\n",
    "        max_steps=100,\n",
    "        per_device_train_batch_size=bs,\n",
    "        per_device_eval_batch_size=bs*2,\n",
    "        learning_rate=lr,\n",
    "        fp16=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # 4) Trainer\n",
    "    trainer = Trainer(\n",
    "        model=m,\n",
    "        args=args,\n",
    "        train_dataset=ds[\"train\"],\n",
    "        eval_dataset=ds[\"dev\"],\n",
    "        tokenizer=tokenizer_re,\n",
    "        data_collator=DataCollatorWithPadding(tokenizer_re),\n",
    "        compute_metrics=compute_metrics_re,\n",
    "    )\n",
    "\n",
    "    # 5) Train & Eval\n",
    "    trainer.train()\n",
    "    return trainer.evaluate()[\"eval_f1\"]\n",
    "\n",
    "\n",
    "study_neo_re_freeze = optuna.create_study(direction=\"maximize\")\n",
    "study_neo_re_freeze.optimize(neo_re_freeze_objective, n_trials=8)\n",
    "\n",
    "print(\"🏆 Best GPT-Neo RE Freeze params:\", study_neo_re_freeze.best_params,\n",
    "      \"→ Dev-F1 =\", study_neo_re_freeze.best_value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e01e7d9-011e-41de-b9de-efb3b1a06d88",
   "metadata": {},
   "source": [
    "## 42: Final Full Fine-Tuning für GPT-Neo RE mit den besten Hyper-Params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "02aa076e-c7c5-4664-bdb7-b541ae35d28e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1737116126.py:48: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  ft_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 09:52, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.968900</td>\n",
       "      <td>3.269184</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.293500</td>\n",
       "      <td>2.821665</td>\n",
       "      <td>0.407591</td>\n",
       "      <td>0.407591</td>\n",
       "      <td>0.407591</td>\n",
       "      <td>0.407591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.773200</td>\n",
       "      <td>2.515672</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.567400</td>\n",
       "      <td>2.463848</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.528600</td>\n",
       "      <td>2.420529</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.485149</td>\n",
       "      <td>0.485149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.407200</td>\n",
       "      <td>2.352352</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.492100</td>\n",
       "      <td>2.322120</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.405900</td>\n",
       "      <td>2.305504</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.464700</td>\n",
       "      <td>2.294711</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.415100</td>\n",
       "      <td>2.288176</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔖 GPT-Neo RE Final Full-FT Dev-F1:      0.5\n",
      "🔖 GPT-Neo RE Final Full-FT Dev-Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# === Kapitel 42: Final Full Fine-Tuning für GPT-Neo RE mit den besten Hyper-Params ===\n",
    "\n",
    "from datasets import concatenate_datasets\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# 1) Train+Dev zusammenführen\n",
    "full_train = concatenate_datasets([ds[\"train\"], ds[\"dev\"]])\n",
    "\n",
    "# 2) Beste Hyper-Params aus Optuna-Study\n",
    "best_params = {\n",
    "    \"learning_rate\": 2.1593610305165778e-05,\n",
    "    \"batch_size\":    16,\n",
    "}\n",
    "\n",
    "# 3) Modell & Pad-Token konfigurieren\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-125M\",\n",
    "    num_labels=len(label2id_re),\n",
    "    id2label=id2label_re,\n",
    "    label2id=label2id_re,\n",
    ")\n",
    "# Wichtig für GPT-Neo: sag dem Modell, welches Token es als PAD ignorieren soll\n",
    "model.config.pad_token_id = tokenizer_re.pad_token_id\n",
    "\n",
    "# 4) Data-Collator\n",
    "data_collator_re = DataCollatorWithPadding(tokenizer_re)\n",
    "\n",
    "# 5) Trainings-Argumente\n",
    "ft_args = TrainingArguments(\n",
    "    output_dir=\"outputs/neo-re-final-ft\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=100,\n",
    "    learning_rate=best_params[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_params[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_params[\"batch_size\"] * 2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# 6) Trainer initialisieren\n",
    "ft_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=ft_args,\n",
    "    train_dataset=full_train,\n",
    "    eval_dataset=ds[\"dev\"],\n",
    "    tokenizer=tokenizer_re,\n",
    "    data_collator=data_collator_re,\n",
    "    compute_metrics=compute_metrics_re,\n",
    ")\n",
    "\n",
    "# 7) Training & Auswertung\n",
    "ft_trainer.train()\n",
    "dev_metrics = ft_trainer.evaluate()\n",
    "print(\"🔖 GPT-Neo RE Final Full-FT Dev-F1:     \", dev_metrics[\"eval_f1\"])\n",
    "print(\"🔖 GPT-Neo RE Final Full-FT Dev-Accuracy:\", dev_metrics.get(\"eval_accuracy\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d81355-7da3-4d5a-b3d1-72283393ffc4",
   "metadata": {},
   "source": [
    "## 43: Final LoRA Fine-Tuning für GPT-Neo RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "0f586562-732c-48f1-9868-6f1b87bc1008",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid task type: 'SEQUENCE_CLS'. Must be one of the following task types: SEQ_CLS, SEQ_2_SEQ_LM, CAUSAL_LM, TOKEN_CLS, QUESTION_ANS, FEATURE_EXTRACTION.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[174], line 34\u001b[0m\n\u001b[0;32m     31\u001b[0m base\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mpad_token_id \u001b[38;5;241m=\u001b[39m tokenizer_re\u001b[38;5;241m.\u001b[39mpad_token_id\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# 4) LoRA konfigurieren & anwenden\u001b[39;00m\n\u001b[1;32m---> 34\u001b[0m lora_conf \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[0;32m     35\u001b[0m     task_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSEQUENCE_CLS\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     36\u001b[0m     inference_mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[0;32m     37\u001b[0m     r\u001b[38;5;241m=\u001b[39mbest_lora[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     38\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39mbest_lora[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124malpha\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     39\u001b[0m     lora_dropout\u001b[38;5;241m=\u001b[39mbest_lora[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdropout\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[0;32m     40\u001b[0m )\n\u001b[0;32m     41\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(base, lora_conf)\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m# 5) DataCollator\u001b[39;00m\n",
      "File \u001b[1;32m<string>:33\u001b[0m, in \u001b[0;36m__init__\u001b[1;34m(self, task_type, peft_type, auto_mapping, base_model_name_or_path, revision, inference_mode, r, target_modules, exclude_modules, lora_alpha, lora_dropout, fan_in_fan_out, bias, use_rslora, modules_to_save, init_lora_weights, layers_to_transform, layers_pattern, rank_pattern, alpha_pattern, megatron_config, megatron_core, trainable_token_indices, loftq_config, eva_config, corda_config, use_dora, layer_replication, runtime_config, lora_bias)\u001b[0m\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\peft\\tuners\\lora\\config.py:539\u001b[0m, in \u001b[0;36mLoraConfig.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m--> 539\u001b[0m     \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m__post_init__()\n\u001b[0;32m    540\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpeft_type \u001b[38;5;241m=\u001b[39m PeftType\u001b[38;5;241m.\u001b[39mLORA\n\u001b[0;32m    541\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_modules \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m    542\u001b[0m         \u001b[38;5;28mset\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_modules) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_modules, \u001b[38;5;28mlist\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_modules\n\u001b[0;32m    543\u001b[0m     )\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\peft\\config.py:67\u001b[0m, in \u001b[0;36mPeftConfigMixin.__post_init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__post_init__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# check for invalid task type\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(TaskType)):\n\u001b[1;32m---> 67\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     68\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid task type: \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtask_type\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Must be one of the following task types: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(TaskType)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     69\u001b[0m         )\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid task type: 'SEQUENCE_CLS'. Must be one of the following task types: SEQ_CLS, SEQ_2_SEQ_LM, CAUSAL_LM, TOKEN_CLS, QUESTION_ANS, FEATURE_EXTRACTION."
     ]
    }
   ],
   "source": [
    "# === Kapitel 43: Final LoRA Fine-Tuning für GPT-Neo RE ===\n",
    "\n",
    "from peft import LoraConfig, get_peft_model\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "\n",
    "# 1) Train+Dev zusammenführen\n",
    "full_train = concatenate_datasets([ds[\"train\"], ds[\"dev\"]])\n",
    "\n",
    "# 2) Beste LoRA-Hyper-Params\n",
    "best_lora = {\n",
    "    \"learning_rate\": 0.0002827866072068735,\n",
    "    \"r\":             16,\n",
    "    \"alpha\":         16,\n",
    "    \"dropout\":       0.255526969488984,\n",
    "    \"batch_size\":    8,\n",
    "}\n",
    "\n",
    "# 3) Basis-Modell laden\n",
    "base = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-125M\",\n",
    "    num_labels=len(label2id_re),\n",
    "    id2label=id2label_re,\n",
    "    label2id=label2id_re,\n",
    ")\n",
    "# Pad-Token für GPT-Neo setzen\n",
    "base.config.pad_token_id = tokenizer_re.pad_token_id\n",
    "\n",
    "# 4) LoRA konfigurieren & anwenden\n",
    "lora_conf = LoraConfig(\n",
    "    task_type=\"SEQUENCE_CLS\",\n",
    "    inference_mode=False,\n",
    "    r=best_lora[\"r\"],\n",
    "    lora_alpha=best_lora[\"alpha\"],\n",
    "    lora_dropout=best_lora[\"dropout\"],\n",
    ")\n",
    "model = get_peft_model(base, lora_conf)\n",
    "\n",
    "# 5) DataCollator\n",
    "data_collator_re = DataCollatorWithPadding(tokenizer_re)\n",
    "\n",
    "# 6) Trainings-Argumente\n",
    "lora_args = TrainingArguments(\n",
    "    output_dir=\"outputs/neo-re-final-lora\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=20,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=200,\n",
    "    learning_rate=best_lora[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_lora[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_lora[\"batch_size\"] * 2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# 7) Trainer initialisieren & trainieren\n",
    "lora_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=lora_args,\n",
    "    train_dataset=full_train,\n",
    "    eval_dataset=ds[\"dev\"],\n",
    "    tokenizer=tokenizer_re,\n",
    "    data_collator=data_collator_re,\n",
    "    compute_metrics=compute_metrics_re,\n",
    ")\n",
    "\n",
    "lora_trainer.train()\n",
    "lora_metrics = lora_trainer.evaluate()\n",
    "print(\"🔖 GPT-Neo RE Final LoRA Dev-F1:     \", lora_metrics[\"eval_f1\"])\n",
    "print(\"🔖 GPT-Neo RE Final LoRA Dev-Accuracy:\", lora_metrics.get(\"eval_accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "335a4587-310b-414e-a226-863771cb2bcc",
   "metadata": {},
   "source": [
    "## 44: Final Partial-Freeze Fine-Tuning für GPT-Neo RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "4a1bccfa-420f-4c23-b871-e03598996c9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\3462525868.py:38: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  freeze_trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 07:20, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.448600</td>\n",
       "      <td>3.223630</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.819800</td>\n",
       "      <td>2.905955</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.951000</td>\n",
       "      <td>2.799154</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "      <td>0.493399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>3.080500</td>\n",
       "      <td>2.639364</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.664100</td>\n",
       "      <td>2.558183</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.499800</td>\n",
       "      <td>2.519373</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.147000</td>\n",
       "      <td>2.487896</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.896200</td>\n",
       "      <td>2.488675</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.211500</td>\n",
       "      <td>2.477239</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>2.825400</td>\n",
       "      <td>2.469461</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='38' max='38' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [38/38 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔖 GPT-Neo RE Final Freeze Dev-F1:      0.5\n",
      "🔖 GPT-Neo RE Final Freeze Dev-Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "best_freeze = {\n",
    "    \"learning_rate\": 3.464080464231026e-05,\n",
    "    \"batch_size\":    8,\n",
    "    \"freeze_pct\":    0.4393379142919582,\n",
    "}\n",
    "\n",
    "# 3) Modell laden & Pad-Token setzen\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-125M\",\n",
    "    num_labels=len(label2id_re),\n",
    "    id2label=id2label_re,\n",
    "    label2id=label2id_re,\n",
    ")\n",
    "model.config.pad_token_id = tokenizer_re.pad_token_id\n",
    "\n",
    "# 4) Freeze-Anteil der Layers\n",
    "total_layers = model.config.num_layers\n",
    "cutoff = int(total_layers * best_freeze[\"freeze_pct\"])\n",
    "for name, param in model.named_parameters():\n",
    "    if name.startswith(\"transformer.h.\") and int(name.split(\".\")[2]) < cutoff:\n",
    "        param.requires_grad = False\n",
    "\n",
    "# 5) DataCollator & Trainings-Argumente\n",
    "freeze_args = TrainingArguments(\n",
    "    output_dir=\"outputs/neo-re-final-freeze\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"no\",\n",
    "    max_steps=100,\n",
    "    learning_rate=best_freeze[\"learning_rate\"],\n",
    "    per_device_train_batch_size=best_freeze[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_freeze[\"batch_size\"] * 2,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# 6) Trainer initialisieren & trainieren\n",
    "freeze_trainer = Trainer(\n",
    "    model=model,\n",
    "    args=freeze_args,\n",
    "    train_dataset=full_train,\n",
    "    eval_dataset=ds[\"dev\"],\n",
    "    tokenizer=tokenizer_re,\n",
    "    data_collator=data_collator_re,\n",
    "    compute_metrics=compute_metrics_re,\n",
    ")\n",
    "\n",
    "freeze_trainer.train()\n",
    "freeze_metrics = freeze_trainer.evaluate()\n",
    "print(\"🔖 GPT-Neo RE Final Freeze Dev-F1:     \", freeze_metrics[\"eval_f1\"])\n",
    "print(\"🔖 GPT-Neo RE Final Freeze Dev-Accuracy:\", freeze_metrics.get(\"eval_accuracy\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4287711-f00d-4e88-b510-531f7cfa5d45",
   "metadata": {},
   "source": [
    "## Basline GPT RE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d66dd654-027a-4a4a-82b6-87962a220cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\1766561571.py:49: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_neo_re = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='459' max='459' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [459/459 12:46, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2.356100</td>\n",
       "      <td>2.842299</td>\n",
       "      <td>0.021505</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2.006500</td>\n",
       "      <td>2.919954</td>\n",
       "      <td>0.021553</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>1.511700</td>\n",
       "      <td>2.988148</td>\n",
       "      <td>0.023764</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='76' max='76' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [76/76 00:30]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔖 GPT-Neo RE Baseline Dev-F1: 0.02376404835672195\n",
      "🔖 GPT-Neo RE Baseline Dev-Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "# === Kapitel 32b: Baseline-Training & -Eval für RE mit GPT-Neo-125M ===\n",
    "\n",
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding\n",
    ")\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# 32b.1: GPT-Neo als Sequence-Classifier laden\n",
    "model_neo_re = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-125M\",\n",
    "    num_labels=len(label2id_re),\n",
    "    id2label=id2label_re,\n",
    "    label2id=label2id_re,\n",
    ")\n",
    "# sicherheitshalber Pad-Token setzen (GPT-Neo braucht das zum Padden)\n",
    "model_neo_re.config.pad_token_id = tokenizer_re.pad_token_id\n",
    "\n",
    "# 32b.2: TrainingArguments sehr ähnlich zu BERT-Baseline\n",
    "neo_re_baseline_args = TrainingArguments(\n",
    "    output_dir=\"outputs/neo-re-baseline\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_f1\",\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    learning_rate=2e-5,\n",
    "    weight_decay=0.01,\n",
    "    logging_steps=50,\n",
    "    save_total_limit=1,\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# 32b.3: DataCollator (pad auf max der Batch)\n",
    "data_collator_re = DataCollatorWithPadding(tokenizer=tokenizer_re)\n",
    "\n",
    "# 32b.4: Compute-Metrics (F1 + Accuracy)\n",
    "def compute_metrics_re(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    return {\n",
    "        \"eval_f1\":       f1_score(p.label_ids, preds, average=\"macro\"),\n",
    "        \"eval_accuracy\": accuracy_score(p.label_ids, preds),\n",
    "    }\n",
    "\n",
    "# 32b.5: Trainer instanziieren\n",
    "trainer_neo_re = Trainer(\n",
    "    model=model_neo_re,\n",
    "    args=neo_re_baseline_args,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"dev\"],\n",
    "    tokenizer=tokenizer_re,\n",
    "    data_collator=data_collator_re,\n",
    "    compute_metrics=compute_metrics_re,\n",
    ")\n",
    "\n",
    "# 32b.6: Train & Eval\n",
    "trainer_neo_re.train()\n",
    "metrics_neo_re = trainer_neo_re.evaluate()\n",
    "print(\"🔖 GPT-Neo RE Baseline Dev-F1:\",       metrics_neo_re[\"eval_f1\"])\n",
    "print(\"🔖 GPT-Neo RE Baseline Dev-Accuracy:\", metrics_neo_re[\"eval_accuracy\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d9727a-185f-4c31-892e-1ac6d0a196f3",
   "metadata": {},
   "source": [
    "## LOOP with NEO same as bert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "aa7fa007-c8fe-431c-ac09-e055ba1f12c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of GPTNeoForSequenceClassification were not initialized from the model checkpoint at EleutherAI/gpt-neo-125M and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\nmilo\\anaconda3\\Lib\\site-packages\\transformers\\training_args.py:1611: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n",
      "C:\\Users\\nmilo\\AppData\\Local\\Temp\\ipykernel_49888\\793071264.py:54: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer_re_neo_ft = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 10:08, Epoch 1/2]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.993100</td>\n",
       "      <td>3.277524</td>\n",
       "      <td>0.021577</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.969500</td>\n",
       "      <td>2.907632</td>\n",
       "      <td>0.033351</td>\n",
       "      <td>0.473597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>3.004600</td>\n",
       "      <td>2.811207</td>\n",
       "      <td>0.021577</td>\n",
       "      <td>0.498350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>2.669900</td>\n",
       "      <td>2.835491</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>2.441600</td>\n",
       "      <td>2.780799</td>\n",
       "      <td>0.021481</td>\n",
       "      <td>0.493399</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.316600</td>\n",
       "      <td>2.782464</td>\n",
       "      <td>0.021457</td>\n",
       "      <td>0.491749</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>2.433100</td>\n",
       "      <td>2.775737</td>\n",
       "      <td>0.021482</td>\n",
       "      <td>0.495050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>2.362100</td>\n",
       "      <td>2.756645</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>0.496700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>2.092300</td>\n",
       "      <td>2.771260</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>1.761400</td>\n",
       "      <td>2.775944</td>\n",
       "      <td>0.021529</td>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='19' max='19' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [19/19 00:29]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔖 GPT-Neo RE Final-FT Dev-F1: 0.021529060679266732\n",
      "🔖 GPT-Neo RE Final-FT Dev-Accuracy: 0.5\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoModelForSequenceClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorWithPadding,\n",
    ")\n",
    "import torch\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "\n",
    "# 1) Load GPT-Neo for sequence classification with your RE label‐maps\n",
    "model_re_neo_ft = AutoModelForSequenceClassification.from_pretrained(\n",
    "    \"EleutherAI/gpt-neo-125M\",\n",
    "    num_labels=len(label2id_re),\n",
    "    id2label=id2label_re,\n",
    "    label2id=label2id_re,\n",
    ")\n",
    "# make sure Neo has a pad token\n",
    "model_re_neo_ft.config.pad_token_id = tokenizer_re.pad_token_id\n",
    "\n",
    "# 2) Your best FT hyperparameters for GPT-Neo RE\n",
    "best_neo_ft = {\n",
    "    \"learning_rate\": 2.1593610305165778e-05,\n",
    "    \"batch_size\":    16,\n",
    "}\n",
    "\n",
    "# 3) TrainingArguments (100 steps, log & eval every 10)\n",
    "training_args_neo_ft = TrainingArguments(\n",
    "    output_dir=\"outputs/re-neo-final-ft\",\n",
    "    evaluation_strategy=\"steps\",\n",
    "    eval_steps=10,\n",
    "    logging_strategy=\"steps\",\n",
    "    logging_steps=10,\n",
    "    save_strategy=\"epoch\",\n",
    "    save_total_limit=1,\n",
    "    max_steps=100,\n",
    "    per_device_train_batch_size=best_neo_ft[\"batch_size\"],\n",
    "    per_device_eval_batch_size=best_neo_ft[\"batch_size\"] * 2,\n",
    "    learning_rate=best_neo_ft[\"learning_rate\"],\n",
    "    fp16=torch.cuda.is_available(),\n",
    ")\n",
    "\n",
    "# 4) Data collator\n",
    "data_collator_re = DataCollatorWithPadding(tokenizer=tokenizer_re)\n",
    "\n",
    "# 5) Metrics (F1 + accuracy)\n",
    "def compute_metrics_neo_ft(p):\n",
    "    preds = p.predictions.argmax(-1)\n",
    "    return {\n",
    "        \"eval_f1\":       f1_score(p.label_ids, preds, average=\"macro\"),\n",
    "        \"eval_accuracy\": accuracy_score(p.label_ids, preds),\n",
    "    }\n",
    "\n",
    "# 6) Trainer\n",
    "trainer_re_neo_ft = Trainer(\n",
    "    model=model_re_neo_ft,\n",
    "    args=training_args_neo_ft,\n",
    "    train_dataset=ds[\"train\"],\n",
    "    eval_dataset=ds[\"dev\"],\n",
    "    tokenizer=tokenizer_re,\n",
    "    data_collator=data_collator_re,\n",
    "    compute_metrics=compute_metrics_neo_ft,\n",
    ")\n",
    "\n",
    "# 7) Train & Evaluate\n",
    "trainer_re_neo_ft.train()\n",
    "metrics = trainer_re_neo_ft.evaluate()\n",
    "print(\"🔖 GPT-Neo RE Final-FT Dev-F1:\",       metrics[\"eval_f1\"])\n",
    "print(\"🔖 GPT-Neo RE Final-FT Dev-Accuracy:\", metrics[\"eval_accuracy\"])\n",
    "\n",
    "# 8) (Optional) Save\n",
    "trainer_re_neo_ft.save_model(\"outputs/re-neo-final-ft\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d6585-93f3-436f-8567-a6b34e2b32b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
