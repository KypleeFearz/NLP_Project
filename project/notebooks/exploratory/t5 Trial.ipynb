{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":11844417,"sourceType":"datasetVersion","datasetId":7441866}],"dockerImageVersionId":31041,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"notebook-setup-markdown","cell_type":"markdown","source":"## 1. Notebook Setup and Configuration\n\nThis notebook performs Named Entity Recognition (NER) and Relation Extraction (RE) using transformer models.\n\n**CRITICAL WARNING ON DATASET SIZE AND EXPECTED PERFORMANCE:**\nThe dataset currently configured for this notebook (Train: ~50 documents, Dev: ~20 documents) is **extremely small** for training modern transformer models for complex tasks like NER (with many labels) and RE. \n\n**Consequences of Small Dataset Size:**\n1.  **Poor Generalization:** Models trained on such limited data are highly unlikely to generalize well to unseen data. Expect low F1 scores, precision, recall, and accuracy on development and test sets.\n2.  **Overfitting:** Models might easily memorize the small training set, showing deceptively good training loss but performing badly on evaluation.\n3.  **Unreliable Hyperparameter Optimization:** HPO results will not be stable or indicative of true optimal parameters for a larger, representative dataset.\n\n**Purpose of this Notebook (Given Data Limitation):**\n* To demonstrate the **end-to-end pipeline** for setting up NER and RE tasks.\n* To show how to use various fine-tuning strategies (full, LoRA, partial-freeze).\n* To illustrate data preprocessing, metric calculation, and baseline evaluations.\n\n**To achieve meaningful performance, a significantly larger and more diverse dataset is essential.** The training parameters (epochs, steps) have been adjusted to be more reasonable than in the original notebook, but they are still tuned for a quick demonstration rather than state-of-the-art results on this tiny dataset.","metadata":{}},{"id":"0a35b13a-2f03-4fd6-a518-b033c36afe71","cell_type":"code","source":"!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n\n!pip install datasets evaluate transformers[sentencepiece] peft optuna scikit-learn pandas matplotlib seaborn nltk\n!pip install evaluate seqeval","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:50:28.481209Z","iopub.execute_input":"2025-05-17T14:50:28.481552Z","iopub.status.idle":"2025-05-17T14:50:38.441586Z","shell.execute_reply.started":"2025-05-17T14:50:28.481530Z","shell.execute_reply":"2025-05-17T14:50:38.440781Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Looking in indexes: https://download.pytorch.org/whl/cu118\nRequirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu118)\nRequirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\nRequirement already satisfied: torchaudio in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\nRequirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.13.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.0)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch) (11.8.86)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch) (1.3.0)\nRequirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (1.26.4)\nRequirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.1.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy->torchvision) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy->torchvision) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy->torchvision) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy->torchvision) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy->torchvision) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\nRequirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: peft in /usr/local/lib/python3.11/dist-packages (0.14.0)\nRequirement already satisfied: optuna in /usr/local/lib/python3.11/dist-packages (4.3.0)\nRequirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.3)\nRequirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (3.7.2)\nRequirement already satisfied: seaborn in /usr/local/lib/python3.11/dist-packages (0.12.2)\nRequirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (3.9.1)\nRequirement already satisfied: transformers[sentencepiece] in /usr/local/lib/python3.11/dist-packages (4.51.3)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets) (3.18.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (1.26.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (19.0.1)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.8)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.11/dist-packages (from datasets) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.11/dist-packages (from datasets) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2025.3.0,>=2023.1.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from datasets) (25.0)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets) (6.0.2)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.21.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.5.3)\nRequirement already satisfied: sentencepiece!=0.1.92,>=0.1.91 in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (0.2.0)\nRequirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from transformers[sentencepiece]) (3.20.3)\nRequirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from peft) (7.0.0)\nRequirement already satisfied: torch>=1.13.0 in /usr/local/lib/python3.11/dist-packages (from peft) (2.6.0+cu118)\nRequirement already satisfied: accelerate>=0.21.0 in /usr/local/lib/python3.11/dist-packages (from peft) (1.5.2)\nRequirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (1.15.2)\nRequirement already satisfied: colorlog in /usr/local/lib/python3.11/dist-packages (from optuna) (6.9.0)\nRequirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\nRequirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.3.1)\nRequirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (0.12.1)\nRequirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (4.57.0)\nRequirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (1.4.8)\nRequirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (11.1.0)\nRequirement already satisfied: pyparsing<3.1,>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib) (3.0.9)\nRequirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk) (8.1.8)\nRequirement already satisfied: Mako in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (1.3.10)\nRequirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.18)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.24.0->datasets) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->datasets) (2.4.1)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.32.2->datasets) (2025.4.26)\nRequirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.1.1)\nRequirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.1.6)\nRequirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.8.89)\nRequirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.8.89)\nRequirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.8.87)\nRequirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (9.1.0.70)\nRequirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.11.3.6)\nRequirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.9.0.58)\nRequirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (10.3.0.86)\nRequirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.4.1.48)\nRequirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.7.5.86)\nRequirement already satisfied: nvidia-nccl-cu11==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (2.21.5)\nRequirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (11.8.86)\nRequirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (3.2.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.13.0->peft) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch>=1.13.0->peft) (1.3.0)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.13.0->peft) (3.0.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->datasets) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->datasets) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->datasets) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->datasets) (2024.2.0)\n","output_type":"stream"},{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"},{"name":"stdout","text":"Requirement already satisfied: evaluate in /usr/local/lib/python3.11/dist-packages (0.4.3)\nRequirement already satisfied: seqeval in /usr/local/lib/python3.11/dist-packages (1.2.2)\nRequirement already satisfied: datasets>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.6.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from evaluate) (1.26.4)\nRequirement already satisfied: dill in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.2.3)\nRequirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (2.32.3)\nRequirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.11/dist-packages (from evaluate) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from evaluate) (3.5.0)\nRequirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.70.16)\nRequirement already satisfied: fsspec>=2021.05.0 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (2025.3.0)\nRequirement already satisfied: huggingface-hub>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from evaluate) (0.31.1)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from evaluate) (25.0)\nRequirement already satisfied: scikit-learn>=0.21.3 in /usr/local/lib/python3.11/dist-packages (from seqeval) (1.2.2)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (3.18.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (19.0.1)\nRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from datasets>=2.0.0->evaluate) (6.0.2)\nRequirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.05.0->evaluate) (3.11.18)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (4.13.2)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.7.0->evaluate) (1.1.0)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->evaluate) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.4.2)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2.4.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->evaluate) (2025.4.26)\nRequirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.15.2)\nRequirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (1.5.0)\nRequirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.21.3->seqeval) (3.6.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->evaluate) (2025.2)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (2.6.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.3.2)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (25.3.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.6.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (6.4.3)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (0.3.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2021.05.0->evaluate) (1.20.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.17.0)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->evaluate) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->evaluate) (1.3.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->evaluate) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->evaluate) (2024.2.0)\n","output_type":"stream"}],"execution_count":53},{"id":"config-params-code","cell_type":"code","source":"# --- Core Configuration Parameters ---\n# Note: Adjust these based on your actual dataset size and available compute time.\n# FOR THE CURRENT VERY SMALL DATASET, EVEN THESE VALUES ARE FOR DEMONSTRATION\n# AND HIGH PERFORMANCE IS NOT EXPECTED.\n\n# --- Model Configuration ---\nMODEL_NAME_NER = \"t5-small\" \nMODEL_NAME_RE = \"t5-small\"  \n\n# --- NER Training Parameters ---\n# HPO for NER\nNUM_OPTUNA_TRIALS_NER = 3  # Number of Optuna trials\nNUM_EPOCHS_OPTUNA_NER = 4  # Epochs per Optuna trial for NER\n# Final NER Training\nNUM_TRAIN_EPOCHS_FINAL_NER = 20 # Target epochs for final NER training (use with EarlyStopping)\n\n# --- RE Training Parameters ---\n# HPO for RE (Set NUM_OPTUNA_TRIALS_RE > 0 to enable)\nNUM_OPTUNA_TRIALS_RE = 0 \nNUM_EPOCHS_OPTUNA_RE = 3   # Epochs per Optuna trial for RE\n# Final RE Training\nNUM_TRAIN_EPOCHS_FINAL_RE = 20 # Target epochs for final RE training (use with EarlyStopping)\n\n# --- General Settings ---\nSEED = 42 \nOUTPUT_BASE_DIR = \"/kaggle/working/outputs\" \nTMP_BASE_DIR = \"/kaggle/working/tmp\"      \n    \n\nPath(OUTPUT_BASE_DIR).mkdir(parents=True, exist_ok=True)\nPath(TMP_BASE_DIR).mkdir(parents=True, exist_ok=True)\n\n# Dictionary to store all evaluation results\nall_experiment_results = defaultdict(dict)\n\nprint(f\"--- Configuration Summary ---\")\nprint(f\"NER Model: {MODEL_NAME_NER}\")\nprint(f\"RE Model: {MODEL_NAME_RE}\")\nprint(f\"NER Optuna Trials: {NUM_OPTUNA_TRIALS_NER} trials, {NUM_EPOCHS_OPTUNA_NER} epochs/trial\")\nprint(f\"NER Final Training Epochs: {NUM_TRAIN_EPOCHS_FINAL_NER}\")\nprint(f\"RE Optuna Trials: {NUM_OPTUNA_TRIALS_RE} trials, {NUM_EPOCHS_OPTUNA_RE} epochs/trial (0 means skip)\")\nprint(f\"RE Final Training Epochs: {NUM_TRAIN_EPOCHS_FINAL_RE}\")\nprint(f\"Global Seed: {SEED}\")\nprint(f\"Output Base Directory: {OUTPUT_BASE_DIR}\")\nprint(f\"Temp Base Directory for HPO: {TMP_BASE_DIR}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:40.391235Z","iopub.execute_input":"2025-05-17T14:51:40.391631Z","iopub.status.idle":"2025-05-17T14:51:40.399139Z","shell.execute_reply.started":"2025-05-17T14:51:40.391608Z","shell.execute_reply":"2025-05-17T14:51:40.398403Z"}},"outputs":[{"name":"stdout","text":"--- Configuration Summary ---\nNER Model: t5-small\nRE Model: t5-small\nNER Optuna Trials: 3 trials, 4 epochs/trial\nNER Final Training Epochs: 15\nRE Optuna Trials: 0 trials, 3 epochs/trial (0 means skip)\nRE Final Training Epochs: 10\nGlobal Seed: 42\nOutput Base Directory: /kaggle/working/outputs\nTemp Base Directory for HPO: /kaggle/working/tmp\n","output_type":"stream"}],"execution_count":55},{"id":"imports-markdown","cell_type":"markdown","source":"## 2. Imports and Environment Setup","metadata":{}},{"id":"imports-code","cell_type":"code","source":"import os\nimport re\nimport random\nimport json\nimport time\nfrom pathlib import Path\nfrom collections import Counter, defaultdict\nimport copy\nimport warnings\n\nimport torch\nimport numpy as np\nimport pandas as pd # For results table\nimport sklearn\nfrom sklearn.metrics import precision_recall_fscore_support, accuracy_score, confusion_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport optuna\nimport nltk\n\nfrom datasets import Dataset # Keep this\nimport evaluate # <--- ADD THIS IMPORT\n\nfrom transformers import (\n    AutoTokenizer,\n    AutoConfig,\n    AutoModelForTokenClassification,\n    AutoModelForSequenceClassification,\n    Trainer,\n    TrainingArguments,\n    DataCollatorForTokenClassification,\n    DataCollatorWithPadding,\n    T5Config,\n    set_seed\n)\nfrom peft import get_peft_model, LoraConfig, TaskType\n\n# Set seed for reproducibility early\n# SEED should be defined in your config cell, e.g., SEED = 42\nif 'SEED' not in globals():\n    SEED = 42 \n    print(f\"Warning: SEED not found globally, setting to {SEED}\")\nset_seed(SEED)\n\n# NLTK Punkt Tokenizer for sentence splitting\ntry:\n    nltk.data.find('tokenizers/punkt')\nexcept nltk.downloader.DownloadError:\n    print(\"NLTK 'punkt' tokenizer not found. Downloading...\")\n    nltk.download('punkt', quiet=True)\n    print(\"'punkt' tokenizer downloaded.\")\n\n# Suppress some common warnings for cleaner output\nwarnings.filterwarnings(\"ignore\", category=UserWarning, module='torch.nn.parallel._functions')\nwarnings.filterwarnings(\"ignore\", category=UserWarning, message=\"Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\")\n\n\nprint(f\"Libraries imported and seed set to {SEED}.\")\nprint(f\"The 'evaluate' library version is: {evaluate.__version__}\") # Good to check version","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:40.400546Z","iopub.execute_input":"2025-05-17T14:51:40.400782Z","iopub.status.idle":"2025-05-17T14:51:40.415791Z","shell.execute_reply.started":"2025-05-17T14:51:40.400766Z","shell.execute_reply":"2025-05-17T14:51:40.415006Z"}},"outputs":[{"name":"stdout","text":"Libraries imported and seed set to 42.\nThe 'evaluate' library version is: 0.4.3\n","output_type":"stream"}],"execution_count":56},{"id":"paths-markdown","cell_type":"markdown","source":"## 3. Data Paths and Loading","metadata":{}},{"id":"paths-loading-code","cell_type":"code","source":"DATA_ROOT = Path(\"/kaggle/input/nlp-dataset\") \n\nTRAIN_DIR = DATA_ROOT / \"train\"\nDEV_DIR   = DATA_ROOT / \"dev\"\nTEST_DIR  = DATA_ROOT / \"test\"\n\nprint(f\"Looking for data in: {DATA_ROOT}\")\nassert TRAIN_DIR.exists(), f\"Train folder not found: {TRAIN_DIR}\"\nassert DEV_DIR.exists(),   f\"Dev folder not found:   {DEV_DIR}\"\nassert TEST_DIR.exists(),  f\"Test folder not found:  {TEST_DIR}\"\n\ndef load_docie_docs(folder: Path, recursive: bool = False):\n    docs = []\n    pattern = \"**/*.json\" if recursive else \"*.json\"\n    file_paths = sorted(list(folder.glob(pattern))) \n    if not file_paths:\n        print(f\"Warning: No JSON files found in {folder} with pattern '{pattern}'\")\n        return docs\n        \n    for file_path in file_paths:\n        try:\n            data = json.loads(file_path.read_text(encoding=\"utf-8\"))\n            doc_id_from_file = file_path.stem\n            if isinstance(data, list):\n                for sub_doc_idx, sub_doc_item in enumerate(data):\n                    if isinstance(sub_doc_item, dict):\n                        sub_doc_item['id'] = sub_doc_item.get('id', f\"{doc_id_from_file}_{sub_doc_idx}\")\n                        docs.append(sub_doc_item)\n            elif isinstance(data, dict):\n                data['id'] = data.get('id', doc_id_from_file)\n                docs.append(data)\n        except json.JSONDecodeError as e:\n            print(f\"Error decoding JSON from {file_path}: {e}\")\n        except Exception as e:\n            print(f\"Error loading {file_path}: {e}\")\n    return docs\n\nprint(\"Loading raw documents...\")\ntrain_docs_raw = load_docie_docs(TRAIN_DIR)\ndev_docs_raw = load_docie_docs(DEV_DIR)\ntest_docs_raw = load_docie_docs(TEST_DIR, recursive=True)\n\nprint(f\"Raw documents loaded: Train: {len(train_docs_raw)} │ Dev: {len(dev_docs_raw)} │ Test: {len(test_docs_raw)}\")\n\nif not train_docs_raw or not dev_docs_raw:\n    raise ValueError(\"CRITICAL: Training or development data is empty. Cannot proceed.\")\n\n# Standardize document text key and ensure essential keys exist\nprint(\"\\nStandardizing document keys...\")\nfor doc_list_name, doc_list in zip([\"Train\", \"Dev\", \"Test\"], [train_docs_raw, dev_docs_raw, test_docs_raw]):\n    print(f\"Processing {doc_list_name} docs...\")\n    for i, doc_item in enumerate(doc_list):\n        if not isinstance(doc_item, dict):\n            print(f\"Warning: Item {i} in {doc_list_name} is not a dict, it's a {type(doc_item)}. Skipping.\")\n            doc_list[i] = None # Mark for removal\n            continue\n        if 'document' in doc_item and 'document_text' not in doc_item:\n            doc_item['document_text'] = doc_item.pop('document')\n        elif 'doc' in doc_item and 'document_text' not in doc_item:\n            doc_item['document_text'] = doc_item.pop('doc')\n        elif 'document_text' not in doc_item:\n            print(f\"Warning: Document {doc_item.get('id', 'UNKNOWN_ID')} in {doc_list_name} missing text field ('document' or 'doc'). Setting to empty string.\")\n            doc_item['document_text'] = \"\"\n        \n        if 'entities' not in doc_item:\n            doc_item['entities'] = [] \n        if 'triples' not in doc_item:\n            doc_item['triples'] = []   \n    # Remove None items\n    train_docs_raw = [d for d in train_docs_raw if d is not None]\n    dev_docs_raw = [d for d in dev_docs_raw if d is not None]\n    test_docs_raw = [d for d in test_docs_raw if d is not None]\n\nif train_docs_raw:\n    print(f\"\\nKeys in first standardized training document ('{train_docs_raw[0].get('id', 'N/A')}'): {train_docs_raw[0].keys()}\")\nelse:\n    print(\"No training documents available after standardization.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:40.416622Z","iopub.execute_input":"2025-05-17T14:51:40.416900Z","iopub.status.idle":"2025-05-17T14:51:40.864330Z","shell.execute_reply.started":"2025-05-17T14:51:40.416884Z","shell.execute_reply":"2025-05-17T14:51:40.863698Z"}},"outputs":[{"name":"stdout","text":"Looking for data in: /kaggle/input/nlp-dataset\nLoading raw documents...\nRaw documents loaded: Train: 51 │ Dev: 23 │ Test: 248\n\nStandardizing document keys...\nProcessing Train docs...\nProcessing Dev docs...\nProcessing Test docs...\n\nKeys in first standardized training document ('Communication_all_examples_0'): dict_keys(['domain', 'title', 'entities', 'triples', 'label_set', 'entity_label_set', 'id', 'document_text'])\n","output_type":"stream"}],"execution_count":57},{"id":"eda-markdown","cell_type":"markdown","source":"## 4. Exploratory Data Analysis (EDA)","metadata":{}},{"id":"eda-doc-length-code","cell_type":"code","source":"print(\"--- EDA: Document Length ---\")\nif train_docs_raw:\n    lengths = [len(doc.get(\"document_text\", \"\").split()) for doc in train_docs_raw]\n    if lengths:\n        print(f\"Avg Tokens per doc (approx): {np.mean(lengths):.2f}, Max Tokens: {np.max(lengths)}, Min Tokens: {np.min(lengths)}\")\n    else:\n        print(\"No documents with text found for length analysis.\")\nelse:\n    print(\"No training documents for length EDA.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:40.865756Z","iopub.execute_input":"2025-05-17T14:51:40.865964Z","iopub.status.idle":"2025-05-17T14:51:40.873910Z","shell.execute_reply.started":"2025-05-17T14:51:40.865949Z","shell.execute_reply":"2025-05-17T14:51:40.873095Z"}},"outputs":[{"name":"stdout","text":"--- EDA: Document Length ---\nAvg Tokens per doc (approx): 919.08, Max Tokens: 2560, Min Tokens: 128\n","output_type":"stream"}],"execution_count":58},{"id":"eda-ner-types-code","cell_type":"code","source":"print(\"--- EDA: NER Entity Types Distribution (Training Set) ---\")\nif train_docs_raw:\n    all_ner_entity_types_eda = []\n    for doc in train_docs_raw:\n        for ent in doc.get('entities', []):\n            all_ner_entity_types_eda.append(ent.get('type', 'UNKNOWN_TYPE'))\n    if all_ner_entity_types_eda:\n        ner_ctr_eda = Counter(all_ner_entity_types_eda)\n        print(f\"Total unique NER entity types found in data: {len(ner_ctr_eda)}\")\n        print(f\"NER Entity Types (Top 20): {ner_ctr_eda.most_common(20)}\")\n    else:\n        print(\"No entities found in training documents for NER EDA.\")\nelse:\n    print(\"No training documents for NER entity EDA.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:40.874678Z","iopub.execute_input":"2025-05-17T14:51:40.874934Z","iopub.status.idle":"2025-05-17T14:51:40.888354Z","shell.execute_reply.started":"2025-05-17T14:51:40.874911Z","shell.execute_reply":"2025-05-17T14:51:40.887796Z"}},"outputs":[{"name":"stdout","text":"--- EDA: NER Entity Types Distribution (Training Set) ---\nTotal unique NER entity types found in data: 19\nNER Entity Types (Top 20): [('DATE', 647), ('MISC', 417), ('PERSON', 242), ('ORG', 241), ('CARDINAL', 224), ('GPE', 157), ('WORK_OF_ART', 65), ('NORP', 59), ('ORDINAL', 55), ('QUANTITY', 42), ('EVENT', 35), ('PRODUCT', 30), ('FAC', 30), ('MONEY', 29), ('PERCENT', 28), ('LOC', 24), ('LANGUAGE', 10), ('LAW', 9), ('TIME', 8)]\n","output_type":"stream"}],"execution_count":59},{"id":"eda-re-types-code","cell_type":"code","source":"print(\"--- EDA: RE Relation Types Distribution (Training Set) ---\")\nif train_docs_raw:\n    all_re_relation_types_eda = []\n    for doc in train_docs_raw:\n        for t in doc.get('triples', []):\n            all_re_relation_types_eda.append(t.get('relation', 'UNKNOWN_RELATION'))\n    if all_re_relation_types_eda:\n        re_ctr_eda = Counter(all_re_relation_types_eda)\n        print(f\"Total unique RE relation types found in data: {len(re_ctr_eda)}\")\n        print(f\"RE Relation Types (Top 20): {re_ctr_eda.most_common(20)}\")\n    else:\n        print(\"No triples found in training documents for RE EDA.\")\nelse:\n    print(\"No training documents for RE relation EDA.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:40.889081Z","iopub.execute_input":"2025-05-17T14:51:40.889298Z","iopub.status.idle":"2025-05-17T14:51:40.905560Z","shell.execute_reply.started":"2025-05-17T14:51:40.889283Z","shell.execute_reply":"2025-05-17T14:51:40.904860Z"}},"outputs":[{"name":"stdout","text":"--- EDA: RE Relation Types Distribution (Training Set) ---\nTotal unique RE relation types found in data: 68\nRE Relation Types (Top 20): [('HasPart', 82), ('HasEffect', 67), ('DiplomaticRelation', 45), ('LocatedIn', 44), ('InterestedIn', 38), ('OwnerOf', 32), ('NominatedFor', 25), ('SaidToBeTheSameAs', 25), ('PartOf', 18), ('Creator', 17), ('Founded', 13), ('Country', 13), ('DifferentFrom', 11), ('SignificantEvent', 11), ('PrimeFactor', 11), ('InfluencedBy', 10), ('Follows', 10), ('UsedBy', 9), ('InspiredBy', 9), ('Uses', 8)]\n","output_type":"stream"}],"execution_count":60},{"id":"ner-section-start-markdown","cell_type":"markdown","source":"# Part I: Named Entity Recognition (NER)","metadata":{}},{"id":"ner-label-mapping-markdown","cell_type":"markdown","source":"## 5. NER: Label Mapping","metadata":{}},{"id":"ner-label-mapping-code","cell_type":"code","source":"print(\"--- NER Label Mapping ---\")\nner_entity_type_source = []\nif train_docs_raw:\n    # Prefer 'NER_label_set' if available in the first doc, otherwise derive from all entities\n    if 'NER_label_set' in train_docs_raw[0] and isinstance(train_docs_raw[0]['NER_label_set'], list):\n        ner_entity_type_source = sorted(list(set(train_docs_raw[0][\"NER_label_set\"])))\n        print(\"Using 'NER_label_set' from the first document for NER labels.\")\n    elif 'entity_label_set' in train_docs_raw[0] and isinstance(train_docs_raw[0]['entity_label_set'], list):\n        ner_entity_type_source = sorted(list(set(train_docs_raw[0][\"entity_label_set\"])))\n        print(\"Using 'entity_label_set' from the first document for NER labels.\")\n    else:\n        print(\"Deriving NER labels from all unique entity types found in training data.\")\n        all_types = set()\n        for doc in train_docs_raw:\n            for entity in doc.get('entities', []):\n                all_types.add(entity.get('type', 'UNKNOWN_TYPE'))\n        ner_entity_type_source = sorted(list(all_types - {'UNKNOWN_TYPE'}))\n\nif not ner_entity_type_source:\n    print(\"CRITICAL WARNING: No NER entity types determined. Using minimal fallback. NER will perform poorly.\")\n    ner_entity_type_source = [\"MISC\"]\n\nner_labels_list = [\"O\"] \nfor t in ner_entity_type_source:\n    ner_labels_list += [f\"B-{t}\", f\"I-{t}\"]\n\nner_label_to_id = {label: i for i, label in enumerate(ner_labels_list)}\nner_id_to_label = {i: label for i, label in enumerate(ner_labels_list)}\nnum_ner_labels = len(ner_labels_list)\n\nprint(f\"Number of NER Labels: {num_ner_labels}\")\nprint(f\"Sample NER Labels (first 10): {ner_labels_list[:min(10, num_ner_labels)]}\")\nprint(f\"ner_label_to_id['O'] = {ner_label_to_id.get('O', 'ERROR: O not found')}\")\n\n%store ner_label_to_id\n%store ner_id_to_label\n%store num_ner_labels\n%store ner_labels_list","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:40.906400Z","iopub.execute_input":"2025-05-17T14:51:40.906665Z","iopub.status.idle":"2025-05-17T14:51:40.924765Z","shell.execute_reply.started":"2025-05-17T14:51:40.906642Z","shell.execute_reply":"2025-05-17T14:51:40.924218Z"}},"outputs":[{"name":"stdout","text":"--- NER Label Mapping ---\nUsing 'entity_label_set' from the first document for NER labels.\nNumber of NER Labels: 39\nSample NER Labels (first 10): ['O', 'B-CARDINAL', 'I-CARDINAL', 'B-DATE', 'I-DATE', 'B-EVENT', 'I-EVENT', 'B-FAC', 'I-FAC', 'B-GPE']\nner_label_to_id['O'] = 0\nStored 'ner_label_to_id' (dict)\nStored 'ner_id_to_label' (dict)\nStored 'num_ner_labels' (int)\nStored 'ner_labels_list' (list)\n","output_type":"stream"}],"execution_count":61},{"id":"ner-tokenizer-init-markdown","cell_type":"markdown","source":"## 6. NER: Tokenizer Initialization","metadata":{}},{"id":"ner-tokenizer-init-code","cell_type":"code","source":"%store -r MODEL_NAME_NER \nprint(f\"--- Initializing Tokenizer for NER model: {MODEL_NAME_NER} ---\")\nner_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_NER, use_fast=True)\nprint(f\"NER Tokenizer for '{MODEL_NAME_NER}' loaded.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:40.925346Z","iopub.execute_input":"2025-05-17T14:51:40.925544Z","iopub.status.idle":"2025-05-17T14:51:41.343851Z","shell.execute_reply.started":"2025-05-17T14:51:40.925530Z","shell.execute_reply":"2025-05-17T14:51:41.343220Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_NER\n--- Initializing Tokenizer for NER model: t5-small ---\nNER Tokenizer for 't5-small' loaded.\n","output_type":"stream"}],"execution_count":62},{"id":"ner-dataset-prep-markdown","cell_type":"markdown","source":"## 7. NER: Dataset Preparation and Tokenization","metadata":{}},{"id":"ner-create-hf-datasets-code","cell_type":"code","source":"print(\"--- Creating Hugging Face Datasets for NER from raw documents ---\")\nner_hf_train_raw = Dataset.from_list(train_docs_raw)\nner_hf_dev_raw = Dataset.from_list(dev_docs_raw)\nprint(f\"Raw NER HF Datasets created. Train: {len(ner_hf_train_raw)}, Dev: {len(ner_hf_dev_raw)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:41.345789Z","iopub.execute_input":"2025-05-17T14:51:41.345973Z","iopub.status.idle":"2025-05-17T14:51:41.402276Z","shell.execute_reply.started":"2025-05-17T14:51:41.345960Z","shell.execute_reply":"2025-05-17T14:51:41.401763Z"}},"outputs":[{"name":"stdout","text":"--- Creating Hugging Face Datasets for NER from raw documents ---\nRaw NER HF Datasets created. Train: 51, Dev: 23\n","output_type":"stream"}],"execution_count":63},{"id":"ner-tokenize-align-func-code","cell_type":"code","source":"%store -r ner_label_to_id # Retrieve from mapping cell\n\nMAX_SEQ_LENGTH_NER = 512 \nTOKEN_STRIDE_NER = 128   \n\ndef tokenize_and_align_labels_ner_revised(examples, tokenizer_obj, label_to_id_map):\n    \"\"\"\n    Tokenizes documents and aligns NER labels. Uses 'document_text' and 'entities'.\n    Assumes each mention in entity['mentions'] is a STRING.\n    Uses string.find() to locate mentions, which might be ambiguous if mention text is repeated.\n    Handles long documents by chunking with stride.\n    \"\"\"\n    tokenized_inputs = tokenizer_obj(\n        examples[\"document_text\"],\n        truncation=True,\n        max_length=MAX_SEQ_LENGTH_NER,\n        stride=TOKEN_STRIDE_NER,\n        return_overflowing_tokens=True, \n        return_offsets_mapping=True,    \n        padding=False                   \n    )\n    chunk_labels_list = []\n    \n    for i, original_doc_idx in enumerate(tokenized_inputs[\"overflow_to_sample_mapping\"]):\n        doc_entities = examples[\"entities\"][original_doc_idx]\n        # Get the full original text for the current document for .find()\n        original_document_text = examples[\"document_text\"][original_doc_idx] \n        \n        offset_mapping = tokenized_inputs[\"offset_mapping\"][i]\n        current_chunk_labels = [label_to_id_map[\"O\"]] * len(offset_mapping)\n        \n        chunk_char_start_original_doc = -1\n        chunk_char_end_original_doc = -1\n        for token_start_char, token_end_char in offset_mapping:\n            if token_start_char == 0 and token_end_char == 0: continue\n            if chunk_char_start_original_doc == -1: chunk_char_start_original_doc = token_start_char\n            chunk_char_end_original_doc = token_end_char\n            \n        if chunk_char_start_original_doc == -1:\n            chunk_labels_list.append(current_chunk_labels)\n            continue\n            \n        for entity in doc_entities:\n            entity_type = entity.get(\"type\", \"UNKNOWN_TYPE\") # Handle if 'type' key is missing\n            for mention_text_str in entity.get(\"mentions\", []): # mention_text_str is a string\n                if not isinstance(mention_text_str, str):\n                    # print(f\"Warning: Expected mention to be a string, but got {type(mention_text_str)}. Skipping mention: {mention_text_str}\")\n                    continue\n\n                # Find start/end of mention_text_str in the *original, unchunked* document text.\n                # This is the original notebook's approach. Can be ambiguous if mention_text_str repeats.\n                mention_orig_char_start = original_document_text.find(mention_text_str)\n                \n                if mention_orig_char_start == -1:\n                    # print(f\"Warning: Mention text '{mention_text_str}' not found in document. Skipping this mention.\")\n                    continue\n                mention_orig_char_end = mention_orig_char_start + len(mention_text_str)\n\n                # Check if the mention (found via .find()) is within the character span of THIS CHUNK\n                if not (mention_orig_char_end > chunk_char_start_original_doc and \\\n                        mention_orig_char_start < chunk_char_end_original_doc):\n                    continue # Entity mention (this occurrence) not relevant to this chunk\n                \n                first_token_of_mention_in_chunk = True\n                for token_idx, (token_orig_char_start, token_orig_char_end) in enumerate(offset_mapping):\n                    if token_orig_char_start == 0 and token_orig_char_end == 0: continue # Special token\n\n                    # Is this token part of the current mention?\n                    if max(mention_orig_char_start, token_orig_char_start) < min(mention_orig_char_end, token_orig_char_end):\n                        label_key_b = f\"B-{entity_type}\"\n                        label_key_i = f\"I-{entity_type}\"\n                        if first_token_of_mention_in_chunk:\n                            current_chunk_labels[token_idx] = label_to_id_map.get(label_key_b, label_to_id_map[\"O\"])\n                            first_token_of_mention_in_chunk = False\n                        else:\n                            current_chunk_labels[token_idx] = label_to_id_map.get(label_key_i, label_to_id_map[\"O\"])\n        chunk_labels_list.append(current_chunk_labels)\n    \n    tokenized_inputs[\"labels\"] = chunk_labels_list\n    return tokenized_inputs\n\nprint(\"Tokenizing and aligning labels for NER datasets (revised function to handle string mentions)...\\n\")\n\n# Ensure required columns exist from the raw HF datasets\nif 'document_text' not in ner_hf_train_raw.column_names or 'entities' not in ner_hf_train_raw.column_names:\n    raise ValueError(\"NER datasets must contain 'document_text' and 'entities' columns. Check data loading (cell 'paths-loading-code') and standardization.\")\n\n# Remove potentially problematic examples if 'entities' or 'mentions' have unexpected types\n# This is a defensive step. Ideally, data loading should clean this.\ndef filter_problematic_ner_examples(example):\n    if not isinstance(example.get('entities'), list): return False\n    for entity in example['entities']:\n        if not isinstance(entity, dict): return False\n        if not isinstance(entity.get('mentions'), list): return False\n        for mention in entity['mentions']:\n            if not isinstance(mention, str): # Assuming mentions are strings now\n                # print(f\"Found non-string mention: {mention} in doc id {example.get('id')}\")\n                return False # Filter out examples with non-string mentions if that's unexpected\n    return True\n\nprint(\"Filtering raw NER datasets for valid structure before mapping...\")\nner_hf_train_filtered = ner_hf_train_raw.filter(filter_problematic_ner_examples)\nner_hf_dev_filtered = ner_hf_dev_raw.filter(filter_problematic_ner_examples)\nprint(f\"NER Train: {len(ner_hf_train_filtered)} from {len(ner_hf_train_raw)}. NER Dev: {len(ner_hf_dev_filtered)} from {len(ner_hf_dev_raw)} after filtering.\")\n\n\nif len(ner_hf_train_filtered) > 0:\n    ner_tokenized_train = ner_hf_train_filtered.map(\n        tokenize_and_align_labels_ner_revised, \n        batched=True, \n        fn_kwargs={'tokenizer_obj': ner_tokenizer, 'label_to_id_map': ner_label_to_id},\n        remove_columns=ner_hf_train_filtered.column_names \n    )\nelse:\n    ner_tokenized_train = ner_hf_train_filtered # Keep empty if it became empty\n    print(\"Warning: NER training dataset is empty after filtering. Tokenization skipped.\")\n\n\nif len(ner_hf_dev_filtered) > 0:\n    ner_tokenized_dev = ner_hf_dev_filtered.map(\n        tokenize_and_align_labels_ner_revised, \n        batched=True, \n        fn_kwargs={'tokenizer_obj': ner_tokenizer, 'label_to_id_map': ner_label_to_id},\n        remove_columns=ner_hf_dev_filtered.column_names\n    )\nelse:\n    ner_tokenized_dev = ner_hf_dev_filtered # Keep empty\n    print(\"Warning: NER development dataset is empty after filtering. Tokenization skipped.\")\n\n\nprint(\"\\nNER datasets tokenized.\")\nprint(f\"Processed NER Train samples (after chunking): {len(ner_tokenized_train)}\")\nprint(f\"Processed NER Dev samples (after chunking): {len(ner_tokenized_dev)}\")\nif len(ner_tokenized_train) > 0:\n    print(f\"Features in tokenized NER train set: {ner_tokenized_train.features}\")\n\n%store ner_tokenized_train\n%store ner_tokenized_dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:41.403160Z","iopub.execute_input":"2025-05-17T14:51:41.403464Z","iopub.status.idle":"2025-05-17T14:51:42.878058Z","shell.execute_reply.started":"2025-05-17T14:51:41.403419Z","shell.execute_reply":"2025-05-17T14:51:42.877326Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias #\nno stored variable or alias Retrieve\nno stored variable or alias from\nno stored variable or alias mapping\nno stored variable or alias cell\nTokenizing and aligning labels for NER datasets (revised function to handle string mentions)...\n\nFiltering raw NER datasets for valid structure before mapping...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/51 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ca21027e197746da856de1733c1fac66"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/23 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4592cab34c2b4d27a539e8d267c3afad"}},"metadata":{}},{"name":"stdout","text":"NER Train: 51 from 51. NER Dev: 23 from 23 after filtering.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/51 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9bb20f702a1f4811adc17c3706975390"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/23 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b120e8cc00fc498b82f4f650d86c5246"}},"metadata":{}},{"name":"stdout","text":"\nNER datasets tokenized.\nProcessed NER Train samples (after chunking): 182\nProcessed NER Dev samples (after chunking): 81\nFeatures in tokenized NER train set: {'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None), 'offset_mapping': Sequence(feature=Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None), length=-1, id=None), 'overflow_to_sample_mapping': Value(dtype='int64', id=None), 'labels': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None)}\nStored 'ner_tokenized_train' (Dataset)\nStored 'ner_tokenized_dev' (Dataset)\n","output_type":"stream"}],"execution_count":64},{"id":"ner-collator-metrics-markdown","cell_type":"markdown","source":"## 8. NER: Data Collator and Metrics Function","metadata":{}},{"id":"ner-collator-metrics-code","cell_type":"code","source":"%store -r ner_id_to_label ner_labels_list ner_label_to_id # Retrieve from mapping cell\n\nner_data_collator = DataCollatorForTokenClassification(tokenizer=ner_tokenizer, padding='longest')\nprint(\"NER Data Collator initialized.\")\n\nseqeval_metric_ner = evaluate.load(\"seqeval\")\n\ndef compute_metrics_ner_revised(p):\n    predictions_logits, labels = p\n    predictions = np.argmax(predictions_logits, axis=2)\n\n    true_predictions_str = [\n        [ner_id_to_label.get(p_val, \"O\") for (p_val, l_val) in zip(pred_seq, label_seq) if l_val != -100]\n        for pred_seq, label_seq in zip(predictions, labels)\n    ]\n    true_labels_str = [\n        [ner_id_to_label.get(l_val, \"O\") for (p_val, l_val) in zip(pred_seq, label_seq) if l_val != -100]\n        for pred_seq, label_seq in zip(predictions, labels)\n    ]\n\n    seqeval_results = seqeval_metric_ner.compute(predictions=true_predictions_str, references=true_labels_str, zero_division=0)\n    \n    flat_predictions_token = predictions.flatten()\n    flat_labels_token = labels.flatten()\n    active_mask = flat_labels_token != -100\n    active_preds = flat_predictions_token[active_mask]\n    active_labels = flat_labels_token[active_mask]\n    token_accuracy = accuracy_score(active_labels, active_preds) if len(active_labels) > 0 else 0.0\n\n    return {\n        \"precision\": seqeval_results.get(\"overall_precision\", 0.0),\n        \"recall\": seqeval_results.get(\"overall_recall\", 0.0),\n        \"f1\": seqeval_results.get(\"overall_f1\", 0.0),\n        \"accuracy\": seqeval_results.get(\"overall_accuracy\", 0.0), \n        \"token_accuracy\": token_accuracy\n    }\n\nprint(\"NER Metrics function (revised with seqeval and token accuracy) defined.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:42.879247Z","iopub.execute_input":"2025-05-17T14:51:42.879462Z","iopub.status.idle":"2025-05-17T14:51:44.287882Z","shell.execute_reply.started":"2025-05-17T14:51:42.879430Z","shell.execute_reply":"2025-05-17T14:51:44.287281Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias #\nno stored variable or alias Retrieve\nno stored variable or alias from\nno stored variable or alias mapping\nno stored variable or alias cell\nNER Data Collator initialized.\nNER Metrics function (revised with seqeval and token accuracy) defined.\n","output_type":"stream"}],"execution_count":65},{"id":"ner-raw-model-eval-markdown","cell_type":"markdown","source":"## 9. NER: Raw Pre-trained Model Evaluation (Baseline before Fine-tuning)\n\nThis evaluates the performance of the chosen NER model with its pre-trained weights and a randomly initialized token classification head, *before* any fine-tuning on our specific dataset. This helps establish a performance floor.","metadata":{}},{"id":"ner-raw-model-eval-code","cell_type":"code","source":"%store -r MODEL_NAME_NER num_ner_labels ner_id_to_label ner_label_to_id ner_tokenized_dev OUTPUT_BASE_DIR SEED all_experiment_results\n\nprint(f\"--- NER Raw Model Evaluation for {MODEL_NAME_NER} ---\")\n\nif len(ner_tokenized_dev) == 0:\n    print(\"Skipping NER raw model evaluation as tokenized dev dataset is empty.\")\n    raw_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\nelse:\n    try:\n        raw_ner_model = AutoModelForTokenClassification.from_pretrained(\n            MODEL_NAME_NER,\n            num_labels=num_ner_labels,\n            id2label=ner_id_to_label,\n            label2id=ner_label_to_id,\n            ignore_mismatched_sizes=True # Important if base model is not for token classification\n        )\n\n        raw_ner_output_dir = Path(OUTPUT_BASE_DIR) / f\"{MODEL_NAME_NER.replace('/', '_')}-ner-raw-eval\"\n        \n        # Minimal TrainingArguments, just for evaluation\n        raw_ner_eval_args = TrainingArguments(\n            output_dir=str(raw_ner_output_dir),\n            per_device_eval_batch_size=16,\n            do_train=False,\n            do_eval=True,\n            report_to=\"none\",\n            seed=SEED\n        )\n\n        raw_ner_trainer = Trainer(\n            model=raw_ner_model,\n            args=raw_ner_eval_args,\n            eval_dataset=ner_tokenized_dev,\n            data_collator=ner_data_collator,\n            compute_metrics=compute_metrics_ner_revised\n        )\n\n        print(\"Evaluating raw NER model...\")\n        raw_ner_metrics = raw_ner_trainer.evaluate()\n        print(f\"NER Raw Model Metrics ({MODEL_NAME_NER}):\")\n        for k, v_metric in raw_ner_metrics.items():\n            print(f\"  {k}: {v_metric:.4f}\")\n    except Exception as e:\n        print(f\"Error during NER raw model evaluation: {e}\")\n        raw_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\n        import traceback\n        traceback.print_exc()\n\nall_experiment_results['NER Raw Model'] = raw_ner_metrics\n%store all_experiment_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:44.288582Z","iopub.execute_input":"2025-05-17T14:51:44.288781Z","iopub.status.idle":"2025-05-17T14:51:45.763288Z","shell.execute_reply.started":"2025-05-17T14:51:44.288765Z","shell.execute_reply":"2025-05-17T14:51:45.762648Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_NER\nno stored variable or alias OUTPUT_BASE_DIR\nno stored variable or alias SEED\n--- NER Raw Model Evaluation for t5-small ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Evaluating raw NER model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"NER Raw Model Metrics (t5-small):\n  eval_loss: 8.0843\n  eval_precision: 0.0002\n  eval_recall: 0.0060\n  eval_f1: 0.0005\n  eval_accuracy: 0.0371\n  eval_token_accuracy: 0.0371\n  eval_runtime: 1.0664\n  eval_samples_per_second: 75.9570\n  eval_steps_per_second: 2.8130\nStored 'all_experiment_results' (defaultdict)\n","output_type":"stream"}],"execution_count":66},{"id":"ner-smoke-run-markdown","cell_type":"markdown","source":"## 10. NER: Smoke Run (Quick Test of Training Pipeline)","metadata":{}},{"id":"ner-smoke-run-code","cell_type":"code","source":"%store -r MODEL_NAME_NER num_ner_labels ner_id_to_label ner_label_to_id ner_tokenized_train ner_tokenized_dev OUTPUT_BASE_DIR SEED all_experiment_results\n\nprint(f\"--- NER Smoke Run for {MODEL_NAME_NER} ---\")\n\nner_smoke_output_dir = Path(OUTPUT_BASE_DIR) / f\"{MODEL_NAME_NER.replace('/', '_')}-ner-smoke-run\"\n\nif len(ner_tokenized_train) < 10 or len(ner_tokenized_dev) < 5: \n    print(\"Skipping NER smoke run as processed datasets are too small.\")\n    smoke_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\nelse:\n    try:\n        ner_smoke_model = AutoModelForTokenClassification.from_pretrained(\n            MODEL_NAME_NER,\n            num_labels=num_ner_labels,\n            id2label=ner_id_to_label,\n            label2id=ner_label_to_id,\n            ignore_mismatched_sizes=True\n        )\n        ner_smoke_args = TrainingArguments(\n            output_dir=str(ner_smoke_output_dir),\n            per_device_train_batch_size=4, \n            per_device_eval_batch_size=8,\n            eval_strategy=\"steps\",       # <--- CORRECTED from evaluation_strategy\n            eval_steps=20, \n            logging_strategy=\"steps\",\n            logging_steps=10,\n            save_strategy=\"no\", \n            max_steps=50, \n            learning_rate=5e-5,\n            fp16=torch.cuda.is_available(),\n            report_to=\"none\",\n            seed=SEED,\n            disable_tqdm=False\n        )\n        smoke_train_subset = ner_tokenized_train.shuffle(seed=SEED).select(range(min(100, len(ner_tokenized_train))))\n        smoke_eval_subset = ner_tokenized_dev.shuffle(seed=SEED).select(range(min(50, len(ner_tokenized_dev))))\n\n        ner_smoke_trainer = Trainer(\n            model=ner_smoke_model,\n            args=ner_smoke_args,\n            train_dataset=smoke_train_subset,\n            eval_dataset=smoke_eval_subset,\n            data_collator=ner_data_collator,\n            compute_metrics=compute_metrics_ner_revised,\n        )\n        print(f\"Starting NER smoke training ({ner_smoke_args.max_steps} steps)...\")\n        ner_smoke_trainer.train()\n        print(\"NER smoke training completed.\")\n        print(\"Evaluating NER smoke model...\")\n        smoke_ner_metrics = ner_smoke_trainer.evaluate()\n        print(f\"NER Smoke-Run Metrics ({MODEL_NAME_NER}):\")\n        for k, v_metric in smoke_ner_metrics.items():\n            print(f\"  {k}: {v_metric:.4f}\")\n    except Exception as e:\n        print(f\"Error during NER smoke run: {e}\")\n        smoke_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\n        import traceback\n        traceback.print_exc()\n\nall_experiment_results['NER Smoke Run'] = smoke_ner_metrics\n%store all_experiment_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:45.763961Z","iopub.execute_input":"2025-05-17T14:51:45.764149Z","iopub.status.idle":"2025-05-17T14:51:56.303495Z","shell.execute_reply.started":"2025-05-17T14:51:45.764135Z","shell.execute_reply":"2025-05-17T14:51:56.302613Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_NER\nno stored variable or alias OUTPUT_BASE_DIR\nno stored variable or alias SEED\n--- NER Smoke Run for t5-small ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting NER smoke training (50 steps)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:08, Epoch 3/4]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Token Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>20</td>\n      <td>7.272800</td>\n      <td>5.832761</td>\n      <td>0.000107</td>\n      <td>0.002275</td>\n      <td>0.000205</td>\n      <td>0.103257</td>\n      <td>0.103257</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.222500</td>\n      <td>4.151319</td>\n      <td>0.000064</td>\n      <td>0.001138</td>\n      <td>0.000122</td>\n      <td>0.239809</td>\n      <td>0.239809</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"NER smoke training completed.\nEvaluating NER smoke model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='4' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [4/4 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"NER Smoke-Run Metrics (t5-small):\n  eval_loss: 3.9380\n  eval_precision: 0.0001\n  eval_recall: 0.0011\n  eval_f1: 0.0001\n  eval_accuracy: 0.2647\n  eval_token_accuracy: 0.2647\n  eval_runtime: 0.6531\n  eval_samples_per_second: 76.5560\n  eval_steps_per_second: 6.1240\n  epoch: 3.8462\nStored 'all_experiment_results' (defaultdict)\n","output_type":"stream"}],"execution_count":67},{"id":"ner-baseline-section-markdown","cell_type":"markdown","source":"## 11. NER: Baseline Training (Full Model, No HPO)","metadata":{}},{"id":"ner-baseline-code","cell_type":"code","source":"%store -r MODEL_NAME_NER num_ner_labels ner_id_to_label ner_label_to_id ner_tokenized_train ner_tokenized_dev NUM_TRAIN_EPOCHS_FINAL_NER OUTPUT_BASE_DIR SEED all_experiment_results\n\nprint(f\"--- NER Baseline Training for {MODEL_NAME_NER} ({NUM_TRAIN_EPOCHS_FINAL_NER} epochs) ---\")\n\nner_baseline_output_dir = Path(OUTPUT_BASE_DIR) / f\"{MODEL_NAME_NER.replace('/', '_')}-ner-baseline\"\n\nif len(ner_tokenized_train) == 0 or len(ner_tokenized_dev) == 0:\n    print(\"Skipping NER baseline training as processed datasets are empty.\")\n    baseline_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\nelse:\n    try:\n        ner_baseline_args = TrainingArguments(\n            output_dir=str(ner_baseline_output_dir),\n            per_device_train_batch_size=8, \n            per_device_eval_batch_size=16,\n            learning_rate=3e-5, \n            num_train_epochs=NUM_TRAIN_EPOCHS_FINAL_NER,\n            weight_decay=0.01,\n            logging_strategy=\"epoch\",\n            eval_strategy=\"epoch\",       # <--- CORRECTED from evaluation_strategy\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_f1\",\n            save_total_limit=1, # Save only the best model based on metric_for_best_model\n            fp16=torch.cuda.is_available(),\n            report_to=\"none\",\n            seed=SEED,\n            disable_tqdm=False\n        )\n        ner_baseline_model = AutoModelForTokenClassification.from_pretrained(\n            MODEL_NAME_NER,\n            num_labels=num_ner_labels,\n            id2label=ner_id_to_label,\n            label2id=ner_label_to_id,\n            ignore_mismatched_sizes=True\n        )\n        ner_baseline_trainer = Trainer(\n            model=ner_baseline_model,\n            args=ner_baseline_args,\n            train_dataset=ner_tokenized_train,\n            eval_dataset=ner_tokenized_dev,\n            data_collator=ner_data_collator,\n            compute_metrics=compute_metrics_ner_revised,\n        )\n        print(\"Starting NER baseline training...\")\n        ner_baseline_trainer.train()\n        print(\"NER baseline training completed.\")\n        print(\"Evaluating NER baseline model (best checkpoint automatically loaded)...\\n(Note: If 'missing keys' warning for T5 embeddings appeared during training, these results might be unreliable)\")\n        baseline_ner_metrics = ner_baseline_trainer.evaluate()\n        print(f\"NER Baseline Metrics ({MODEL_NAME_NER}):\")\n        for k, v_metric in baseline_ner_metrics.items():\n            print(f\"  {k}: {v_metric:.4f}\")\n        \n        ner_baseline_trainer.save_model(str(ner_baseline_output_dir / \"best_model_from_baseline_run\"))\n        print(f\"NER Baseline model (best) saved to {ner_baseline_output_dir / 'best_model_from_baseline_run'}\")\n    except Exception as e:\n        print(f\"Error during NER baseline training: {e}\")\n        baseline_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\n        import traceback\n        traceback.print_exc()\n\nall_experiment_results['NER Baseline (Full FT, No HPO)'] = baseline_ner_metrics\n%store all_experiment_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:51:56.304376Z","iopub.execute_input":"2025-05-17T14:51:56.304753Z","iopub.status.idle":"2025-05-17T14:53:13.198475Z","shell.execute_reply.started":"2025-05-17T14:51:56.304729Z","shell.execute_reply":"2025-05-17T14:53:13.197816Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_NER\nno stored variable or alias NUM_TRAIN_EPOCHS_FINAL_NER\nno stored variable or alias OUTPUT_BASE_DIR\nno stored variable or alias SEED\n--- NER Baseline Training for t5-small (15 epochs) ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting NER baseline training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [180/180 01:14, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Token Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>10.976900</td>\n      <td>9.877737</td>\n      <td>0.000315</td>\n      <td>0.008296</td>\n      <td>0.000607</td>\n      <td>0.007852</td>\n      <td>0.007852</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>9.495100</td>\n      <td>8.396983</td>\n      <td>0.000348</td>\n      <td>0.009050</td>\n      <td>0.000671</td>\n      <td>0.021046</td>\n      <td>0.021046</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>8.217700</td>\n      <td>7.110559</td>\n      <td>0.000360</td>\n      <td>0.009050</td>\n      <td>0.000693</td>\n      <td>0.048048</td>\n      <td>0.048048</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>7.098300</td>\n      <td>5.945067</td>\n      <td>0.000414</td>\n      <td>0.009804</td>\n      <td>0.000795</td>\n      <td>0.100235</td>\n      <td>0.100235</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>6.101100</td>\n      <td>4.926164</td>\n      <td>0.000455</td>\n      <td>0.009804</td>\n      <td>0.000869</td>\n      <td>0.174937</td>\n      <td>0.174937</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>5.226000</td>\n      <td>4.050672</td>\n      <td>0.000480</td>\n      <td>0.009050</td>\n      <td>0.000911</td>\n      <td>0.268869</td>\n      <td>0.268869</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>4.486800</td>\n      <td>3.338073</td>\n      <td>0.000572</td>\n      <td>0.009050</td>\n      <td>0.001075</td>\n      <td>0.372683</td>\n      <td>0.372683</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>3.844600</td>\n      <td>2.786341</td>\n      <td>0.000471</td>\n      <td>0.006033</td>\n      <td>0.000874</td>\n      <td>0.473799</td>\n      <td>0.473799</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>3.401200</td>\n      <td>2.385925</td>\n      <td>0.000746</td>\n      <td>0.007541</td>\n      <td>0.001357</td>\n      <td>0.562737</td>\n      <td>0.562737</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>3.042500</td>\n      <td>2.115217</td>\n      <td>0.000861</td>\n      <td>0.006787</td>\n      <td>0.001529</td>\n      <td>0.635169</td>\n      <td>0.635169</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>2.830700</td>\n      <td>1.944088</td>\n      <td>0.000961</td>\n      <td>0.006033</td>\n      <td>0.001658</td>\n      <td>0.686902</td>\n      <td>0.686902</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>2.642400</td>\n      <td>1.838268</td>\n      <td>0.001015</td>\n      <td>0.005279</td>\n      <td>0.001703</td>\n      <td>0.721035</td>\n      <td>0.721035</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>2.551200</td>\n      <td>1.775482</td>\n      <td>0.001161</td>\n      <td>0.005279</td>\n      <td>0.001904</td>\n      <td>0.741841</td>\n      <td>0.741841</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>2.465400</td>\n      <td>1.742250</td>\n      <td>0.001263</td>\n      <td>0.005279</td>\n      <td>0.002038</td>\n      <td>0.753539</td>\n      <td>0.753539</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>2.439100</td>\n      <td>1.731415</td>\n      <td>0.001298</td>\n      <td>0.005279</td>\n      <td>0.002084</td>\n      <td>0.757144</td>\n      <td>0.757144</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight'].\n","output_type":"stream"},{"name":"stdout","text":"NER baseline training completed.\nEvaluating NER baseline model (best checkpoint automatically loaded)...\n(Note: If 'missing keys' warning for T5 embeddings appeared during training, these results might be unreliable)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"NER Baseline Metrics (t5-small):\n  eval_loss: 1.7314\n  eval_precision: 0.0013\n  eval_recall: 0.0053\n  eval_f1: 0.0021\n  eval_accuracy: 0.7571\n  eval_token_accuracy: 0.7571\n  eval_runtime: 0.8774\n  eval_samples_per_second: 92.3220\n  eval_steps_per_second: 3.4190\n  epoch: 15.0000\nNER Baseline model (best) saved to /kaggle/working/outputs/t5-small-ner-baseline/best_model_from_baseline_run\nStored 'all_experiment_results' (defaultdict)\n","output_type":"stream"}],"execution_count":68},{"id":"ner-hpo-section-markdown","cell_type":"markdown","source":"## 12. NER: Hyperparameter Optimization (Optuna)\n\n**Note:** HPO on a very small dataset can be misleading. The parameters found might not generalize well. These runs are for demonstration.","metadata":{}},{"id":"ner-hpo-full-ft-markdown","cell_type":"markdown","source":"### 12.1 Optuna for Full Fine-Tuning (NER)","metadata":{}},{"id":"ner-hpo-full-ft-code","cell_type":"code","source":"%store -r MODEL_NAME_NER num_ner_labels ner_id_to_label ner_label_to_id ner_tokenized_train ner_tokenized_dev NUM_OPTUNA_TRIALS_NER NUM_EPOCHS_OPTUNA_NER TMP_BASE_DIR SEED\n\ndef ner_ft_objective_optuna(trial):\n    learning_rate = trial.suggest_float(\"learning_rate\", 1e-5, 7e-5, log=True)\n    per_device_train_batch_size = trial.suggest_categorical(\"batch_size\", [4, 8]) \n    weight_decay = trial.suggest_float(\"weight_decay\", 0.0, 0.05)\n\n    trial_output_dir = Path(TMP_BASE_DIR) / f\"ner-ft-hpo-trial-{MODEL_NAME_NER.replace('/', '_')}-{trial.number}\"\n\n    args = TrainingArguments(\n        output_dir=str(trial_output_dir),\n        learning_rate=learning_rate,\n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=per_device_train_batch_size * 2,\n        num_train_epochs=NUM_EPOCHS_OPTUNA_NER, \n        weight_decay=weight_decay,\n        eval_strategy=\"epoch\",       # <--- CORRECTED from evaluation_strategy\n        save_strategy=\"no\",          # No saving during HPO trials\n        logging_strategy=\"epoch\",\n        fp16=torch.cuda.is_available(),\n        report_to=\"none\",\n        seed=SEED,\n        disable_tqdm=True \n    )\n    \n    # It's important to re-initialize model for each trial to start from scratch\n    model_hpo = AutoModelForTokenClassification.from_pretrained(\n        MODEL_NAME_NER, \n        num_labels=num_ner_labels, \n        id2label=ner_id_to_label, \n        label2id=ner_label_to_id,\n        ignore_mismatched_sizes=True # Helpful if config doesn't perfectly match\n    )\n    \n    trainer_hpo = Trainer(\n        model=model_hpo, \n        args=args,\n        train_dataset=ner_tokenized_train,\n        eval_dataset=ner_tokenized_dev,\n        data_collator=ner_data_collator, # Ensure ner_data_collator is defined\n        compute_metrics=compute_metrics_ner_revised # Ensure compute_metrics_ner_revised is defined\n    )\n    \n    print(f\"Optuna Trial {trial.number} (NER-FT): LR={learning_rate:.2e}, BS={per_device_train_batch_size}, WD={weight_decay:.3f}, Epochs={NUM_EPOCHS_OPTUNA_NER}\")\n    trainer_hpo.train()\n    eval_metrics = trainer_hpo.evaluate()\n    # Checkpoint for the warning about missing keys - This check is conceptual here as HPO trials don't load best model by default\n    # The main concern for \"missing keys\" is in the final training runs with load_best_model_at_end=True\n    return eval_metrics.get(\"eval_f1\", 0.0) \n\nprint(f\"--- NER Optuna HPO for Full Fine-Tuning ({MODEL_NAME_NER}) ---\")\nstudy_ner_ft = optuna.create_study(direction=\"maximize\", study_name=f\"ner-ft-{MODEL_NAME_NER.replace('/', '_')}\")\n# Fallback parameters if HPO fails or is skipped\nbest_ner_ft_params_fallback = {\"learning_rate\": 3e-5, \"batch_size\": 8, \"weight_decay\": 0.01}\n\nif NUM_OPTUNA_TRIALS_NER > 0 and \\\n   ('ner_tokenized_train' in globals() and len(ner_tokenized_train) > 0) and \\\n   ('ner_tokenized_dev' in globals() and len(ner_tokenized_dev) > 0):\n    try:\n        study_ner_ft.optimize(ner_ft_objective_optuna, n_trials=NUM_OPTUNA_TRIALS_NER, timeout=10800) # Timeout e.g. 3 hours\n        print(f\"NER Full-FT HPO Best Params ({MODEL_NAME_NER}): {study_ner_ft.best_params}\")\n        print(f\"NER Full-FT HPO Best F1: {study_ner_ft.best_value:.4f}\")\n        best_ner_ft_params = study_ner_ft.best_params\n    except Exception as e: # Catching broader exceptions for HPO stability\n        print(f\"An error occurred during NER FT HPO: {e}. Using fallback params.\")\n        best_ner_ft_params = best_ner_ft_params_fallback\n        # import traceback; traceback.print_exc() # Uncomment for full traceback if needed\nelse:\n    print(\"Skipping NER FT HPO due to NUM_OPTUNA_TRIALS_NER=0 or empty/undefined datasets.\")\n    best_ner_ft_params = best_ner_ft_params_fallback\n\n%store best_ner_ft_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:53:13.199212Z","iopub.execute_input":"2025-05-17T14:53:13.199440Z","iopub.status.idle":"2025-05-17T14:54:12.695628Z","shell.execute_reply.started":"2025-05-17T14:53:13.199414Z","shell.execute_reply":"2025-05-17T14:54:12.695006Z"}},"outputs":[{"name":"stderr","text":"[I 2025-05-17 14:53:13,212] A new study created in memory with name: ner-ft-t5-small\n","output_type":"stream"},{"name":"stdout","text":"no stored variable or alias MODEL_NAME_NER\nno stored variable or alias NUM_OPTUNA_TRIALS_NER\nno stored variable or alias NUM_EPOCHS_OPTUNA_NER\nno stored variable or alias TMP_BASE_DIR\nno stored variable or alias SEED\n--- NER Optuna HPO for Full Fine-Tuning (t5-small) ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Optuna Trial 0 (NER-FT): LR=2.03e-05, BS=4, WD=0.012, Epochs=4\n{'loss': 9.492, 'grad_norm': 1621849.5, 'learning_rate': 1.5410855339738524e-05, 'epoch': 1.0}\n{'eval_loss': 8.231178283691406, 'eval_precision': 0.0004989873492030879, 'eval_recall': 0.01282051282051282, 'eval_f1': 0.0009605876536233932, 'eval_accuracy': 0.02206078735110304, 'eval_token_accuracy': 0.02206078735110304, 'eval_runtime': 1.0191, 'eval_samples_per_second': 79.48, 'eval_steps_per_second': 5.887, 'epoch': 1.0}\n{'loss': 7.8971, 'grad_norm': 1442126.875, 'learning_rate': 1.034728858525301e-05, 'epoch': 2.0}\n{'eval_loss': 6.819033622741699, 'eval_precision': 0.0006089948539934837, 'eval_recall': 0.015082956259426848, 'eval_f1': 0.0011707202856557498, 'eval_accuracy': 0.056006623577800334, 'eval_token_accuracy': 0.056006623577800334, 'eval_runtime': 1.0612, 'eval_samples_per_second': 76.328, 'eval_steps_per_second': 5.654, 'epoch': 2.0}\n{'loss': 6.8458, 'grad_norm': 1416008.0, 'learning_rate': 5.283721830767494e-06, 'epoch': 3.0}\n{'eval_loss': 5.963035583496094, 'eval_precision': 0.0007006146301073215, 'eval_recall': 0.016591251885369532, 'eval_f1': 0.0013444556482415132, 'eval_accuracy': 0.09633566582981679, 'eval_token_accuracy': 0.09633566582981679, 'eval_runtime': 1.0488, 'eval_samples_per_second': 77.232, 'eval_steps_per_second': 5.721, 'epoch': 3.0}\n{'loss': 6.3036, 'grad_norm': 1367709.25, 'learning_rate': 2.201550762819789e-07, 'epoch': 4.0}\n{'eval_loss': 5.667490482330322, 'eval_precision': 0.0006832823583002538, 'eval_recall': 0.01583710407239819, 'eval_f1': 0.001310043668122271, 'eval_accuracy': 0.11479087655573954, 'eval_token_accuracy': 0.11479087655573954, 'eval_runtime': 1.0177, 'eval_samples_per_second': 79.592, 'eval_steps_per_second': 5.896, 'epoch': 4.0}\n{'train_runtime': 18.6849, 'train_samples_per_second': 38.962, 'train_steps_per_second': 4.924, 'train_loss': 7.634607563848081, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-05-17 14:53:33,819] Trial 0 finished with value: 0.001310043668122271 and parameters: {'learning_rate': 2.025426701794206e-05, 'batch_size': 4, 'weight_decay': 0.011924362780870675}. Best is trial 0 with value: 0.001310043668122271.\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 5.667490482330322, 'eval_precision': 0.0006832823583002538, 'eval_recall': 0.01583710407239819, 'eval_f1': 0.001310043668122271, 'eval_accuracy': 0.11479087655573954, 'eval_token_accuracy': 0.11479087655573954, 'eval_runtime': 1.0186, 'eval_samples_per_second': 79.521, 'eval_steps_per_second': 5.89, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Optuna Trial 1 (NER-FT): LR=1.63e-05, BS=4, WD=0.021, Epochs=4\n{'loss': 9.4286, 'grad_norm': 1295028.75, 'learning_rate': 1.2388518963028552e-05, 'epoch': 1.0}\n{'eval_loss': 8.321836471557617, 'eval_precision': 0.0005050655099676164, 'eval_recall': 0.01282051282051282, 'eval_f1': 0.0009718450764613407, 'eval_accuracy': 0.018001175150900058, 'eval_token_accuracy': 0.018001175150900058, 'eval_runtime': 1.0531, 'eval_samples_per_second': 76.913, 'eval_steps_per_second': 5.697, 'epoch': 1.0}\n{'loss': 8.2547, 'grad_norm': 1138477.75, 'learning_rate': 8.318005589462028e-06, 'epoch': 2.0}\n{'eval_loss': 7.240435600280762, 'eval_precision': 0.0005438888049554314, 'eval_recall': 0.013574660633484163, 'eval_f1': 0.0010458731588274602, 'eval_accuracy': 0.035361358901768065, 'eval_token_accuracy': 0.035361358901768065, 'eval_runtime': 1.034, 'eval_samples_per_second': 78.337, 'eval_steps_per_second': 5.803, 'epoch': 2.0}\n{'loss': 7.4147, 'grad_norm': 1244269.375, 'learning_rate': 4.247492215895503e-06, 'epoch': 3.0}\n{'eval_loss': 6.600320816040039, 'eval_precision': 0.0005537439241986094, 'eval_recall': 0.013574660633484163, 'eval_f1': 0.0010640813431071175, 'eval_accuracy': 0.05312216227765611, 'eval_token_accuracy': 0.05312216227765611, 'eval_runtime': 1.0523, 'eval_samples_per_second': 76.973, 'eval_steps_per_second': 5.702, 'epoch': 3.0}\n{'loss': 7.0274, 'grad_norm': 1140847.0, 'learning_rate': 1.769788423289793e-07, 'epoch': 4.0}\n{'eval_loss': 6.379782676696777, 'eval_precision': 0.0005587979634918664, 'eval_recall': 0.013574660633484163, 'eval_f1': 0.001073409267100006, 'eval_accuracy': 0.06174883820308744, 'eval_token_accuracy': 0.06174883820308744, 'eval_runtime': 1.0261, 'eval_samples_per_second': 78.938, 'eval_steps_per_second': 5.847, 'epoch': 4.0}\n{'train_runtime': 18.7951, 'train_samples_per_second': 38.734, 'train_steps_per_second': 4.895, 'train_loss': 8.031336577042289, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-05-17 14:53:54,519] Trial 1 finished with value: 0.001073409267100006 and parameters: {'learning_rate': 1.6282053494266096e-05, 'batch_size': 4, 'weight_decay': 0.021412597760056547}. Best is trial 0 with value: 0.001310043668122271.\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 6.379782676696777, 'eval_precision': 0.0005587979634918664, 'eval_recall': 0.013574660633484163, 'eval_f1': 0.001073409267100006, 'eval_accuracy': 0.06174883820308744, 'eval_token_accuracy': 0.06174883820308744, 'eval_runtime': 1.0378, 'eval_samples_per_second': 78.05, 'eval_steps_per_second': 5.781, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Optuna Trial 2 (NER-FT): LR=6.62e-05, BS=8, WD=0.001, Epochs=4\n{'loss': 8.6314, 'grad_norm': 1226751.5, 'learning_rate': 5.1022509153600764e-05, 'epoch': 1.0}\n{'eval_loss': 6.510069847106934, 'eval_precision': 0.0005550416281221092, 'eval_recall': 0.013574660633484163, 'eval_f1': 0.001066477070742979, 'eval_accuracy': 0.056033331552801664, 'eval_token_accuracy': 0.056033331552801664, 'eval_runtime': 1.0296, 'eval_samples_per_second': 78.672, 'eval_steps_per_second': 2.914, 'epoch': 1.0}\n{'loss': 6.0878, 'grad_norm': 975864.9375, 'learning_rate': 3.4474668347027545e-05, 'epoch': 2.0}\n{'eval_loss': 4.287390232086182, 'eval_precision': 0.0006337608112138383, 'eval_recall': 0.01282051282051282, 'eval_f1': 0.0012078152753108348, 'eval_accuracy': 0.21478553496073927, 'eval_token_accuracy': 0.21478553496073927, 'eval_runtime': 0.9953, 'eval_samples_per_second': 81.382, 'eval_steps_per_second': 3.014, 'epoch': 2.0}\n{'loss': 4.4414, 'grad_norm': 1043700.375, 'learning_rate': 1.792682754045432e-05, 'epoch': 3.0}\n{'eval_loss': 3.1128764152526855, 'eval_precision': 0.0005878030859662013, 'eval_recall': 0.00904977375565611, 'eval_f1': 0.001103905064164482, 'eval_accuracy': 0.3862774424443139, 'eval_token_accuracy': 0.3862774424443139, 'eval_runtime': 0.938, 'eval_samples_per_second': 86.356, 'eval_steps_per_second': 3.198, 'epoch': 3.0}\n{'loss': 3.7346, 'grad_norm': 870359.6875, 'learning_rate': 1.3789867338811016e-06, 'epoch': 4.0}\n{'eval_loss': 2.7551262378692627, 'eval_precision': 0.0006808124361738341, 'eval_recall': 0.00904977375565611, 'eval_f1': 0.001266357112705783, 'eval_accuracy': 0.45822872709791146, 'eval_token_accuracy': 0.45822872709791146, 'eval_runtime': 0.9473, 'eval_samples_per_second': 85.502, 'eval_steps_per_second': 3.167, 'epoch': 4.0}\n{'train_runtime': 16.3682, 'train_samples_per_second': 44.476, 'train_steps_per_second': 2.933, 'train_loss': 5.723785877227783, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-05-17 14:54:12,691] Trial 2 finished with value: 0.001266357112705783 and parameters: {'learning_rate': 6.619136322629288e-05, 'batch_size': 8, 'weight_decay': 0.0012090972115940957}. Best is trial 0 with value: 0.001310043668122271.\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 2.7551262378692627, 'eval_precision': 0.0006808124361738341, 'eval_recall': 0.00904977375565611, 'eval_f1': 0.001266357112705783, 'eval_accuracy': 0.45822872709791146, 'eval_token_accuracy': 0.45822872709791146, 'eval_runtime': 0.9403, 'eval_samples_per_second': 86.147, 'eval_steps_per_second': 3.191, 'epoch': 4.0}\nNER Full-FT HPO Best Params (t5-small): {'learning_rate': 2.025426701794206e-05, 'batch_size': 4, 'weight_decay': 0.011924362780870675}\nNER Full-FT HPO Best F1: 0.0013\nStored 'best_ner_ft_params' (dict)\n","output_type":"stream"}],"execution_count":69},{"id":"ner-hpo-lora-markdown","cell_type":"markdown","source":"### 12.2 Optuna for LoRA Fine-Tuning (NER)","metadata":{}},{"id":"ner-hpo-lora-code","cell_type":"code","source":"%store -r MODEL_NAME_NER num_ner_labels ner_id_to_label ner_label_to_id ner_tokenized_train ner_tokenized_dev NUM_OPTUNA_TRIALS_NER NUM_EPOCHS_OPTUNA_NER TMP_BASE_DIR SEED\n\ndef ner_lora_objective_optuna(trial):\n    learning_rate = trial.suggest_float(\"learning_rate\", 5e-5, 5e-4, log=True)\n    r = trial.suggest_categorical(\"r\", [4, 8, 16])\n    lora_alpha = trial.suggest_categorical(\"lora_alpha\", [r, r * 2])\n    lora_dropout = trial.suggest_float(\"lora_dropout\", 0.05, 0.2)\n    per_device_train_batch_size = trial.suggest_categorical(\"batch_size\", [4, 8])\n\n    trial_output_dir = Path(TMP_BASE_DIR) / f\"ner-lora-hpo-trial-{MODEL_NAME_NER.replace('/', '_')}-{trial.number}\"\n    lora_config = LoraConfig(task_type=TaskType.TOKEN_CLS, r=r, lora_alpha=lora_alpha, lora_dropout=lora_dropout, bias=\"none\")\n    base_model_hpo = AutoModelForTokenClassification.from_pretrained(MODEL_NAME_NER, num_labels=num_ner_labels, id2label=ner_id_to_label, label2id=ner_label_to_id, ignore_mismatched_sizes=True)\n    lora_model_hpo = get_peft_model(base_model_hpo, lora_config)\n    \n    args = TrainingArguments(\n        output_dir=str(trial_output_dir), learning_rate=learning_rate, per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=per_device_train_batch_size * 2, num_train_epochs=NUM_EPOCHS_OPTUNA_NER,\n        evaluation_strategy=\"epoch\", save_strategy=\"no\", logging_strategy=\"epoch\", fp16=torch.cuda.is_available(),\n        report_to=\"none\", seed=SEED, disable_tqdm=True\n    )\n    trainer_hpo = Trainer(\n        model=lora_model_hpo, args=args, train_dataset=ner_tokenized_train, eval_dataset=ner_tokenized_dev,\n        data_collator=ner_data_collator, compute_metrics=compute_metrics_ner_revised\n    )\n    print(f\"Optuna Trial {trial.number} (NER-LoRA): LR={learning_rate:.2e}, R={r}, Alpha={lora_alpha}, Dropout={lora_dropout:.2f}, BS={per_device_train_batch_size}, Epochs={NUM_EPOCHS_OPTUNA_NER}\")\n    trainer_hpo.train()\n    eval_metrics = trainer_hpo.evaluate()\n    return eval_metrics.get(\"eval_f1\", 0.0)\n\nprint(f\"--- NER Optuna HPO for LoRA ({MODEL_NAME_NER}) ---\")\nstudy_ner_lora = optuna.create_study(direction=\"maximize\", study_name=f\"ner-lora-{MODEL_NAME_NER.replace('/', '_')}\")\nbest_ner_lora_params_fallback = {\"learning_rate\": 1e-4, \"r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.1, \"batch_size\": 8}\n\nif NUM_OPTUNA_TRIALS_NER > 0 and len(ner_tokenized_train) > 0 and len(ner_tokenized_dev) > 0:\n    try:\n        study_ner_lora.optimize(ner_lora_objective_optuna, n_trials=NUM_OPTUNA_TRIALS_NER, timeout=10800)\n        print(f\"NER LoRA HPO Best Params ({MODEL_NAME_NER}): {study_ner_lora.best_params}\")\n        print(f\"NER LoRA HPO Best F1: {study_ner_lora.best_value:.4f}\")\n        best_ner_lora_params = study_ner_lora.best_params\n    except Exception as e:\n        print(f\"An error occurred during NER LoRA HPO: {e}. Using fallback params.\")\n        best_ner_lora_params = best_ner_lora_params_fallback\nelse:\n    print(\"Skipping NER LoRA HPO due to NUM_OPTUNA_TRIALS_NER=0 or empty datasets.\")\n    best_ner_lora_params = best_ner_lora_params_fallback\n\n%store best_ner_lora_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:54:12.696635Z","iopub.execute_input":"2025-05-17T14:54:12.696873Z","iopub.status.idle":"2025-05-17T14:54:13.012474Z","shell.execute_reply.started":"2025-05-17T14:54:12.696851Z","shell.execute_reply":"2025-05-17T14:54:13.011891Z"}},"outputs":[{"name":"stderr","text":"[I 2025-05-17 14:54:12,709] A new study created in memory with name: ner-lora-t5-small\n","output_type":"stream"},{"name":"stdout","text":"no stored variable or alias MODEL_NAME_NER\nno stored variable or alias NUM_OPTUNA_TRIALS_NER\nno stored variable or alias NUM_EPOCHS_OPTUNA_NER\nno stored variable or alias TMP_BASE_DIR\nno stored variable or alias SEED\n--- NER Optuna HPO for LoRA (t5-small) ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n[W 2025-05-17 14:54:13,007] Trial 0 failed with parameters: {'learning_rate': 0.0002621560611174821, 'r': 4, 'lora_alpha': 8, 'lora_dropout': 0.16303205936500698, 'batch_size': 8} because of the following error: TypeError(\"TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\").\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.11/dist-packages/optuna/study/_optimize.py\", line 197, in _run_trial\n    value_or_values = func(trial)\n                      ^^^^^^^^^^^\n  File \"/tmp/ipykernel_35/571318928.py\", line 15, in ner_lora_objective_optuna\n    args = TrainingArguments(\n           ^^^^^^^^^^^^^^^^^^\nTypeError: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'\n[W 2025-05-17 14:54:13,008] Trial 0 failed with value None.\n","output_type":"stream"},{"name":"stdout","text":"An error occurred during NER LoRA HPO: TrainingArguments.__init__() got an unexpected keyword argument 'evaluation_strategy'. Using fallback params.\nStored 'best_ner_lora_params' (dict)\n","output_type":"stream"}],"execution_count":70},{"id":"ner-hpo-freeze-markdown","cell_type":"markdown","source":"### 12.3 Optuna for Partial-Freeze Fine-Tuning (NER)","metadata":{}},{"id":"ner-hpo-freeze-code","cell_type":"code","source":"%store -r MODEL_NAME_NER num_ner_labels ner_id_to_label ner_label_to_id ner_tokenized_train ner_tokenized_dev NUM_OPTUNA_TRIALS_NER NUM_EPOCHS_OPTUNA_NER TMP_BASE_DIR SEED\n\ndef ner_lora_objective_optuna(trial):\n    learning_rate = trial.suggest_float(\"learning_rate\", 5e-5, 5e-4, log=True)\n    r = trial.suggest_categorical(\"r\", [4, 8, 16])\n    # lora_alpha = trial.suggest_categorical(\"lora_alpha\", [r, r * 2]) # <-- Incorrect: Dynamic choices\n    lora_alpha = trial.suggest_categorical(\"lora_alpha\", [16, 32, 64]) # <-- Corrected: Fixed choices\n    lora_dropout = trial.suggest_float(\"lora_dropout\", 0.05, 0.2)\n    per_device_train_batch_size = trial.suggest_categorical(\"batch_size\", [4, 8])\n\n    trial_output_dir = Path(TMP_BASE_DIR) / f\"ner-lora-hpo-trial-{MODEL_NAME_NER.replace('/', '_')}-{trial.number}\"\n    \n    lora_config = LoraConfig(\n        task_type=TaskType.TOKEN_CLS, \n        r=r, \n        lora_alpha=lora_alpha, \n        lora_dropout=lora_dropout, \n        bias=\"none\"\n    )\n    \n    base_model_hpo = AutoModelForTokenClassification.from_pretrained(\n        MODEL_NAME_NER, \n        num_labels=num_ner_labels, \n        id2label=ner_id_to_label, \n        label2id=ner_label_to_id, \n        ignore_mismatched_sizes=True\n    )\n    lora_model_hpo = get_peft_model(base_model_hpo, lora_config)\n    \n    args = TrainingArguments(\n        output_dir=str(trial_output_dir), \n        learning_rate=learning_rate, \n        per_device_train_batch_size=per_device_train_batch_size,\n        per_device_eval_batch_size=per_device_train_batch_size * 2, \n        num_train_epochs=NUM_EPOCHS_OPTUNA_NER,\n        eval_strategy=\"epoch\",       # <--- CORRECTED from evaluation_strategy\n        save_strategy=\"no\", \n        logging_strategy=\"epoch\", \n        fp16=torch.cuda.is_available(),\n        report_to=\"none\", \n        seed=SEED, \n        disable_tqdm=True\n    )\n    \n    trainer_hpo = Trainer(\n        model=lora_model_hpo, \n        args=args, \n        train_dataset=ner_tokenized_train, \n        eval_dataset=ner_tokenized_dev,\n        data_collator=ner_data_collator, # Ensure ner_data_collator is defined\n        compute_metrics=compute_metrics_ner_revised # Ensure compute_metrics_ner_revised is defined\n    )\n    \n    print(f\"Optuna Trial {trial.number} (NER-LoRA): LR={learning_rate:.2e}, R={r}, Alpha={lora_alpha}, Dropout={lora_dropout:.2f}, BS={per_device_train_batch_size}, Epochs={NUM_EPOCHS_OPTUNA_NER}\")\n    # It's good practice to check if datasets are empty before training, though the outer block does this\n    if len(ner_tokenized_train) == 0 or len(ner_tokenized_dev) == 0:\n        print(f\"Skipping trial {trial.number} due to empty train/dev dataset for NER LoRA.\")\n        return 0.0 # Return a default low value if datasets are empty\n\n    trainer_hpo.train()\n    eval_metrics = trainer_hpo.evaluate()\n    return eval_metrics.get(\"eval_f1\", 0.0)\n\nprint(f\"--- NER Optuna HPO for LoRA ({MODEL_NAME_NER}) ---\")\nstudy_ner_lora = optuna.create_study(direction=\"maximize\", study_name=f\"ner-lora-{MODEL_NAME_NER.replace('/', '_')}\")\n# Fallback parameters if HPO fails or is skipped\nbest_ner_lora_params_fallback = {\"learning_rate\": 1e-4, \"r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.1, \"batch_size\": 8}\n\nif NUM_OPTUNA_TRIALS_NER > 0 and \\\n   ('ner_tokenized_train' in globals() and len(ner_tokenized_train) > 0) and \\\n   ('ner_tokenized_dev' in globals() and len(ner_tokenized_dev) > 0):\n    try:\n        study_ner_lora.optimize(ner_lora_objective_optuna, n_trials=NUM_OPTUNA_TRIALS_NER, timeout=10800) # Timeout e.g. 3 hours\n        print(f\"NER LoRA HPO Best Params ({MODEL_NAME_NER}): {study_ner_lora.best_params}\")\n        print(f\"NER LoRA HPO Best F1: {study_ner_lora.best_value:.4f}\")\n        best_ner_lora_params = study_ner_lora.best_params\n    except Exception as e: # Catching broader exceptions for HPO stability\n        print(f\"An error occurred during NER LoRA HPO: {e}. Using fallback params.\")\n        best_ner_lora_params = best_ner_lora_params_fallback\n        # import traceback; traceback.print_exc() # Uncomment for full traceback if needed\nelse:\n    print(\"Skipping NER LoRA HPO due to NUM_OPTUNA_TRIALS_NER=0 or empty/undefined datasets.\")\n    best_ner_lora_params = best_ner_lora_params_fallback\n\n%store best_ner_lora_params","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:54:13.013200Z","iopub.execute_input":"2025-05-17T14:54:13.013527Z","iopub.status.idle":"2025-05-17T14:55:04.415389Z","shell.execute_reply.started":"2025-05-17T14:54:13.013426Z","shell.execute_reply":"2025-05-17T14:55:04.414786Z"}},"outputs":[{"name":"stderr","text":"[I 2025-05-17 14:54:13,029] A new study created in memory with name: ner-lora-t5-small\n","output_type":"stream"},{"name":"stdout","text":"no stored variable or alias MODEL_NAME_NER\nno stored variable or alias NUM_OPTUNA_TRIALS_NER\nno stored variable or alias NUM_EPOCHS_OPTUNA_NER\nno stored variable or alias TMP_BASE_DIR\nno stored variable or alias SEED\n--- NER Optuna HPO for LoRA (t5-small) ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Optuna Trial 0 (NER-LoRA): LR=2.65e-04, R=4, Alpha=32, Dropout=0.12, BS=8, Epochs=4\n{'loss': 9.7162, 'grad_norm': 225311.65625, 'learning_rate': 0.0002045208201517386, 'epoch': 1.0}\n{'eval_loss': 8.633600234985352, 'eval_precision': 0.00014693781591630423, 'eval_recall': 0.003770739064856712, 'eval_f1': 0.00028285342535498107, 'eval_accuracy': 0.025292452326264623, 'eval_token_accuracy': 0.025292452326264623, 'eval_runtime': 1.0204, 'eval_samples_per_second': 79.378, 'eval_steps_per_second': 2.94, 'epoch': 1.0}\n{'loss': 8.8647, 'grad_norm': 303229.3125, 'learning_rate': 0.0001381897433457693, 'epoch': 2.0}\n{'eval_loss': 7.520958423614502, 'eval_precision': 0.00018159806295399514, 'eval_recall': 0.004524886877828055, 'eval_f1': 0.00034918233137403245, 'eval_accuracy': 0.04786069120239304, 'eval_token_accuracy': 0.04786069120239304, 'eval_runtime': 1.0189, 'eval_samples_per_second': 79.494, 'eval_steps_per_second': 2.944, 'epoch': 2.0}\n{'loss': 7.9561, 'grad_norm': 418444.65625, 'learning_rate': 7.185866653980004e-05, 'epoch': 3.0}\n{'eval_loss': 6.598164081573486, 'eval_precision': 0.00022052800705689622, 'eval_recall': 0.005279034690799397, 'eval_f1': 0.00042337002540220156, 'eval_accuracy': 0.08041771272902089, 'eval_token_accuracy': 0.08041771272902089, 'eval_runtime': 0.9916, 'eval_samples_per_second': 81.685, 'eval_steps_per_second': 3.025, 'epoch': 3.0}\n{'loss': 7.3079, 'grad_norm': 386978.15625, 'learning_rate': 5.527589733830773e-06, 'epoch': 4.0}\n{'eval_loss': 6.229246616363525, 'eval_precision': 0.00022560995262190995, 'eval_recall': 0.005279034690799397, 'eval_f1': 0.00043272648595184374, 'eval_accuracy': 0.09865925965493297, 'eval_token_accuracy': 0.09865925965493297, 'eval_runtime': 1.0022, 'eval_samples_per_second': 80.818, 'eval_steps_per_second': 2.993, 'epoch': 4.0}\n{'train_runtime': 14.0267, 'train_samples_per_second': 51.901, 'train_steps_per_second': 3.422, 'train_loss': 8.461195627848307, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-05-17 14:54:28,967] Trial 0 finished with value: 0.00043272648595184374 and parameters: {'learning_rate': 0.0002653243072238771, 'r': 4, 'lora_alpha': 32, 'lora_dropout': 0.12113812703836473, 'batch_size': 8}. Best is trial 0 with value: 0.00043272648595184374.\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 6.229246616363525, 'eval_precision': 0.00022560995262190995, 'eval_recall': 0.005279034690799397, 'eval_f1': 0.00043272648595184374, 'eval_accuracy': 0.09865925965493297, 'eval_token_accuracy': 0.09865925965493297, 'eval_runtime': 0.9858, 'eval_samples_per_second': 82.17, 'eval_steps_per_second': 3.043, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Optuna Trial 1 (NER-LoRA): LR=1.29e-04, R=8, Alpha=32, Dropout=0.11, BS=4, Epochs=4\n{'loss': 9.8971, 'grad_norm': 164972.75, 'learning_rate': 9.778508432084414e-05, 'epoch': 1.0}\n{'eval_loss': 9.215785026550293, 'eval_precision': 0.00047239444936521995, 'eval_recall': 0.012066365007541479, 'eval_f1': 0.0009091942266166611, 'eval_accuracy': 0.010416110250520805, 'eval_token_accuracy': 0.010416110250520805, 'eval_runtime': 1.0874, 'eval_samples_per_second': 74.492, 'eval_steps_per_second': 5.518, 'epoch': 1.0}\n{'loss': 9.1802, 'grad_norm': 191356.0625, 'learning_rate': 6.565569947256679e-05, 'epoch': 2.0}\n{'eval_loss': 8.297831535339355, 'eval_precision': 0.0005045558424598581, 'eval_recall': 0.01282051282051282, 'eval_f1': 0.0009709015106085268, 'eval_accuracy': 0.01845521072592276, 'eval_token_accuracy': 0.01845521072592276, 'eval_runtime': 1.0679, 'eval_samples_per_second': 75.853, 'eval_steps_per_second': 5.619, 'epoch': 2.0}\n{'loss': 8.3692, 'grad_norm': 256646.765625, 'learning_rate': 3.352631462428942e-05, 'epoch': 3.0}\n{'eval_loss': 7.5353546142578125, 'eval_precision': 0.0005696297406685654, 'eval_recall': 0.014328808446455505, 'eval_f1': 0.0010957008160087656, 'eval_accuracy': 0.030153303776507667, 'eval_token_accuracy': 0.030153303776507667, 'eval_runtime': 1.0487, 'eval_samples_per_second': 77.236, 'eval_steps_per_second': 5.721, 'epoch': 3.0}\n{'loss': 7.883, 'grad_norm': 260988.15625, 'learning_rate': 1.3969297760120591e-06, 'epoch': 4.0}\n{'eval_loss': 7.2410125732421875, 'eval_precision': 0.0005726513758702794, 'eval_recall': 0.014328808446455505, 'eval_f1': 0.0011012896681640342, 'eval_accuracy': 0.036135890176806795, 'eval_token_accuracy': 0.036135890176806795, 'eval_runtime': 1.0418, 'eval_samples_per_second': 77.752, 'eval_steps_per_second': 5.759, 'epoch': 4.0}\n{'train_runtime': 15.682, 'train_samples_per_second': 46.423, 'train_steps_per_second': 5.867, 'train_loss': 8.832397958506709, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-05-17 14:54:46,591] Trial 1 finished with value: 0.0011012896681640342 and parameters: {'learning_rate': 0.00012851753939310945, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.11065605956967692, 'batch_size': 4}. Best is trial 1 with value: 0.0011012896681640342.\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 7.2410125732421875, 'eval_precision': 0.0005726513758702794, 'eval_recall': 0.014328808446455505, 'eval_f1': 0.0011012896681640342, 'eval_accuracy': 0.036135890176806795, 'eval_token_accuracy': 0.036135890176806795, 'eval_runtime': 1.0597, 'eval_samples_per_second': 76.436, 'eval_steps_per_second': 5.662, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Optuna Trial 2 (NER-LoRA): LR=5.62e-05, R=8, Alpha=64, Dropout=0.12, BS=4, Epochs=4\n{'loss': 9.9808, 'grad_norm': 218381.9375, 'learning_rate': 4.276498496000552e-05, 'epoch': 1.0}\n{'eval_loss': 9.466277122497559, 'eval_precision': 0.0004715451946597507, 'eval_recall': 0.012066365007541479, 'eval_f1': 0.0009076211816093258, 'eval_accuracy': 0.008920463650446023, 'eval_token_accuracy': 0.008920463650446023, 'eval_runtime': 1.048, 'eval_samples_per_second': 77.287, 'eval_steps_per_second': 5.725, 'epoch': 1.0}\n{'loss': 9.612, 'grad_norm': 236333.96875, 'learning_rate': 2.871363275886085e-05, 'epoch': 2.0}\n{'eval_loss': 9.028155326843262, 'eval_precision': 0.0005022601707684581, 'eval_recall': 0.01282051282051282, 'eval_f1': 0.0009666505558240697, 'eval_accuracy': 0.011617969125580899, 'eval_token_accuracy': 0.011617969125580899, 'eval_runtime': 1.0596, 'eval_samples_per_second': 76.441, 'eval_steps_per_second': 5.662, 'epoch': 2.0}\n{'loss': 9.2302, 'grad_norm': 286332.96875, 'learning_rate': 1.4662280557716176e-05, 'epoch': 3.0}\n{'eval_loss': 8.679060935974121, 'eval_precision': 0.0005029734607532768, 'eval_recall': 0.01282051282051282, 'eval_f1': 0.0009679715302491104, 'eval_accuracy': 0.014235350675711767, 'eval_token_accuracy': 0.014235350675711767, 'eval_runtime': 1.0749, 'eval_samples_per_second': 75.356, 'eval_steps_per_second': 5.582, 'epoch': 3.0}\n{'loss': 9.0238, 'grad_norm': 282713.875, 'learning_rate': 6.109283565715073e-07, 'epoch': 4.0}\n{'eval_loss': 8.545991897583008, 'eval_precision': 0.0005034053893988748, 'eval_recall': 0.01282051282051282, 'eval_f1': 0.0009687713699566903, 'eval_accuracy': 0.015410501575770525, 'eval_token_accuracy': 0.015410501575770525, 'eval_runtime': 1.0739, 'eval_samples_per_second': 75.429, 'eval_steps_per_second': 5.587, 'epoch': 4.0}\n{'train_runtime': 15.8937, 'train_samples_per_second': 45.804, 'train_steps_per_second': 5.788, 'train_loss': 9.461686839228092, 'epoch': 4.0}\n","output_type":"stream"},{"name":"stderr","text":"[I 2025-05-17 14:55:04,411] Trial 2 finished with value: 0.0009687713699566903 and parameters: {'learning_rate': 5.620540880457868e-05, 'r': 8, 'lora_alpha': 64, 'lora_dropout': 0.11845149938383173, 'batch_size': 4}. Best is trial 1 with value: 0.0011012896681640342.\n","output_type":"stream"},{"name":"stdout","text":"{'eval_loss': 8.545991897583008, 'eval_precision': 0.0005034053893988748, 'eval_recall': 0.01282051282051282, 'eval_f1': 0.0009687713699566903, 'eval_accuracy': 0.015410501575770525, 'eval_token_accuracy': 0.015410501575770525, 'eval_runtime': 1.0759, 'eval_samples_per_second': 75.284, 'eval_steps_per_second': 5.577, 'epoch': 4.0}\nNER LoRA HPO Best Params (t5-small): {'learning_rate': 0.00012851753939310945, 'r': 8, 'lora_alpha': 32, 'lora_dropout': 0.11065605956967692, 'batch_size': 4}\nNER LoRA HPO Best F1: 0.0011\nStored 'best_ner_lora_params' (dict)\n","output_type":"stream"}],"execution_count":71},{"id":"ner-final-runs-markdown","cell_type":"markdown","source":"## 13. NER: Final Training Runs with Best HPO Parameters","metadata":{}},{"id":"ner-final-ft-markdown","cell_type":"markdown","source":"### 13.1 Final Full Fine-Tuning (NER)","metadata":{}},{"id":"ner-final-ft-code","cell_type":"code","source":"%store -r MODEL_NAME_NER num_ner_labels ner_id_to_label ner_label_to_id ner_tokenized_train ner_tokenized_dev best_ner_ft_params NUM_TRAIN_EPOCHS_FINAL_NER OUTPUT_BASE_DIR SEED all_experiment_results\n\nprint(f\"--- NER Final Full Fine-Tuning for {MODEL_NAME_NER} ({NUM_TRAIN_EPOCHS_FINAL_NER} epochs) ---\")\n\nif 'best_ner_ft_params' not in globals() or not isinstance(best_ner_ft_params, dict):\n    print(\"Warning: best_ner_ft_params not found or invalid from HPO. Using default FT params for final run.\")\n    best_ner_ft_params = {\"learning_rate\": 3e-5, \"batch_size\": 8, \"weight_decay\": 0.01}\n\nner_final_ft_output_dir = Path(OUTPUT_BASE_DIR) / f\"{MODEL_NAME_NER.replace('/', '_')}-ner-ft-final\"\n\nif len(ner_tokenized_train) == 0 or len(ner_tokenized_dev) == 0:\n    print(\"Skipping NER final FT training as processed datasets are empty.\")\n    final_ft_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\nelse:\n    try:\n        ft_final_args_ner = TrainingArguments(\n            output_dir=str(ner_final_ft_output_dir),\n            learning_rate=best_ner_ft_params[\"learning_rate\"],\n            per_device_train_batch_size=best_ner_ft_params[\"batch_size\"],\n            per_device_eval_batch_size=best_ner_ft_params[\"batch_size\"] * 2,\n            num_train_epochs=NUM_TRAIN_EPOCHS_FINAL_NER, \n            weight_decay=best_ner_ft_params.get(\"weight_decay\", 0.01),\n            logging_strategy=\"epoch\",\n            eval_strategy=\"epoch\",       # <--- CORRECTED from evaluation_strategy\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_f1\",\n            save_total_limit=1,\n            fp16=torch.cuda.is_available(),\n            report_to=\"none\",\n            seed=SEED,\n            disable_tqdm=False\n        )\n        ft_final_model_ner = AutoModelForTokenClassification.from_pretrained(\n            MODEL_NAME_NER, num_labels=num_ner_labels, id2label=ner_id_to_label, label2id=ner_label_to_id, ignore_mismatched_sizes=True\n        )\n        ft_final_trainer_ner = Trainer(\n            model=ft_final_model_ner, args=ft_final_args_ner, train_dataset=ner_tokenized_train,\n            eval_dataset=ner_tokenized_dev, data_collator=ner_data_collator, compute_metrics=compute_metrics_ner_revised\n        )\n        print(\"Starting NER final full fine-tuning...\")\n        ft_final_trainer_ner.train()\n        print(\"NER final full fine-tuning completed.\")\n        print(\"Evaluating final NER FT model (best checkpoint automatically loaded)...\")\n        final_ft_ner_metrics = ft_final_trainer_ner.evaluate()\n        print(f\"NER Final Full-FT Metrics ({MODEL_NAME_NER}):\")\n        for k, v_metric in final_ft_ner_metrics.items():\n            print(f\"  {k}: {v_metric:.4f}\")\n        ft_final_trainer_ner.save_model(str(ner_final_ft_output_dir / \"best_model\"))\n        print(f\"Final NER FT model saved to {ner_final_ft_output_dir / 'best_model'}\")\n    except Exception as e:\n        print(f\"Error during NER final FT run: {e}\")\n        final_ft_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\n        import traceback; traceback.print_exc()\n\nall_experiment_results['NER Final Full-FT (HPO)'] = final_ft_ner_metrics\n%store all_experiment_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:55:04.416173Z","iopub.execute_input":"2025-05-17T14:55:04.416449Z","iopub.status.idle":"2025-05-17T14:56:28.922088Z","shell.execute_reply.started":"2025-05-17T14:55:04.416425Z","shell.execute_reply":"2025-05-17T14:56:28.921505Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_NER\nno stored variable or alias NUM_TRAIN_EPOCHS_FINAL_NER\nno stored variable or alias OUTPUT_BASE_DIR\nno stored variable or alias SEED\n--- NER Final Full Fine-Tuning for t5-small (15 epochs) ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting NER final full fine-tuning...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='345' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [345/345 01:22, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Token Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>9.218100</td>\n      <td>7.791439</td>\n      <td>0.000478</td>\n      <td>0.012066</td>\n      <td>0.000920</td>\n      <td>0.025159</td>\n      <td>0.025159</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>7.429000</td>\n      <td>5.880901</td>\n      <td>0.000573</td>\n      <td>0.013575</td>\n      <td>0.001099</td>\n      <td>0.085279</td>\n      <td>0.085279</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>5.715500</td>\n      <td>4.154993</td>\n      <td>0.000537</td>\n      <td>0.010558</td>\n      <td>0.001022</td>\n      <td>0.233908</td>\n      <td>0.233908</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>4.265200</td>\n      <td>2.769305</td>\n      <td>0.000626</td>\n      <td>0.008296</td>\n      <td>0.001165</td>\n      <td>0.458122</td>\n      <td>0.458122</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>3.130800</td>\n      <td>1.894585</td>\n      <td>0.001010</td>\n      <td>0.006787</td>\n      <td>0.001759</td>\n      <td>0.672266</td>\n      <td>0.672266</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>2.455500</td>\n      <td>1.525349</td>\n      <td>0.000792</td>\n      <td>0.002262</td>\n      <td>0.001173</td>\n      <td>0.792212</td>\n      <td>0.792212</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.113600</td>\n      <td>1.321472</td>\n      <td>0.001136</td>\n      <td>0.002262</td>\n      <td>0.001512</td>\n      <td>0.819614</td>\n      <td>0.819614</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.872700</td>\n      <td>1.166644</td>\n      <td>0.001633</td>\n      <td>0.003017</td>\n      <td>0.002119</td>\n      <td>0.825624</td>\n      <td>0.825624</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.739600</td>\n      <td>1.065592</td>\n      <td>0.001353</td>\n      <td>0.002262</td>\n      <td>0.001693</td>\n      <td>0.831286</td>\n      <td>0.831286</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.635600</td>\n      <td>1.000143</td>\n      <td>0.001453</td>\n      <td>0.002262</td>\n      <td>0.001770</td>\n      <td>0.834811</td>\n      <td>0.834811</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.570900</td>\n      <td>0.962141</td>\n      <td>0.002694</td>\n      <td>0.003771</td>\n      <td>0.003143</td>\n      <td>0.838924</td>\n      <td>0.838924</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.524000</td>\n      <td>0.936513</td>\n      <td>0.002880</td>\n      <td>0.003771</td>\n      <td>0.003266</td>\n      <td>0.841461</td>\n      <td>0.841461</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.488000</td>\n      <td>0.919676</td>\n      <td>0.003567</td>\n      <td>0.004525</td>\n      <td>0.003989</td>\n      <td>0.842824</td>\n      <td>0.842824</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.466000</td>\n      <td>0.910755</td>\n      <td>0.003043</td>\n      <td>0.003771</td>\n      <td>0.003368</td>\n      <td>0.843678</td>\n      <td>0.843678</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.464900</td>\n      <td>0.907389</td>\n      <td>0.003045</td>\n      <td>0.003771</td>\n      <td>0.003369</td>\n      <td>0.843652</td>\n      <td>0.843652</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight'].\n","output_type":"stream"},{"name":"stdout","text":"NER final full fine-tuning completed.\nEvaluating final NER FT model (best checkpoint automatically loaded)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"NER Final Full-FT Metrics (t5-small):\n  eval_loss: 0.9197\n  eval_precision: 0.0036\n  eval_recall: 0.0045\n  eval_f1: 0.0040\n  eval_accuracy: 0.8428\n  eval_token_accuracy: 0.8428\n  eval_runtime: 0.8830\n  eval_samples_per_second: 91.7360\n  eval_steps_per_second: 6.7950\n  epoch: 15.0000\nFinal NER FT model saved to /kaggle/working/outputs/t5-small-ner-ft-final/best_model\nStored 'all_experiment_results' (defaultdict)\n","output_type":"stream"}],"execution_count":72},{"id":"ner-final-lora-markdown","cell_type":"markdown","source":"### 13.2 Final LoRA Fine-Tuning (NER)","metadata":{}},{"id":"ner-final-lora-code","cell_type":"code","source":"%store -r MODEL_NAME_NER num_ner_labels ner_id_to_label ner_label_to_id ner_tokenized_train ner_tokenized_dev best_ner_lora_params NUM_TRAIN_EPOCHS_FINAL_NER OUTPUT_BASE_DIR SEED all_experiment_results\n\nprint(f\"--- NER Final LoRA Fine-Tuning for {MODEL_NAME_NER} ({NUM_TRAIN_EPOCHS_FINAL_NER} epochs) ---\")\n\nif 'best_ner_lora_params' not in globals() or not isinstance(best_ner_lora_params, dict):\n    print(\"Warning: best_ner_lora_params not found from HPO. Using default LoRA params for final run.\")\n    best_ner_lora_params = {\"learning_rate\": 1e-4, \"r\": 8, \"lora_alpha\": 16, \"lora_dropout\": 0.1, \"batch_size\": 8}\n\nner_final_lora_output_dir = Path(OUTPUT_BASE_DIR) / f\"{MODEL_NAME_NER.replace('/', '_')}-ner-lora-final\"\n\nif len(ner_tokenized_train) == 0 or len(ner_tokenized_dev) == 0:\n    print(\"Skipping NER final LoRA training as processed datasets are empty.\")\n    final_lora_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\nelse:\n    try:\n        lora_final_config_ner = LoraConfig(\n            task_type=TaskType.TOKEN_CLS, \n            r=best_ner_lora_params[\"r\"], \n            lora_alpha=best_ner_lora_params[\"lora_alpha\"], \n            lora_dropout=best_ner_lora_params[\"lora_dropout\"], \n            bias=\"none\"\n        )\n        base_model_ner_lora_final = AutoModelForTokenClassification.from_pretrained(\n            MODEL_NAME_NER, \n            num_labels=num_ner_labels, \n            id2label=ner_id_to_label, \n            label2id=ner_label_to_id, \n            ignore_mismatched_sizes=True\n        )\n        lora_final_model_ner_peft = get_peft_model(base_model_ner_lora_final, lora_final_config_ner)\n        \n        lora_final_args_ner = TrainingArguments(\n            output_dir=str(ner_final_lora_output_dir), \n            learning_rate=best_ner_lora_params[\"learning_rate\"],\n            per_device_train_batch_size=best_ner_lora_params[\"batch_size\"], \n            per_device_eval_batch_size=best_ner_lora_params[\"batch_size\"] * 2,\n            num_train_epochs=NUM_TRAIN_EPOCHS_FINAL_NER, \n            logging_strategy=\"epoch\", \n            eval_strategy=\"epoch\",       # <--- CORRECTED from evaluation_strategy\n            save_strategy=\"epoch\", \n            load_best_model_at_end=True, \n            metric_for_best_model=\"eval_f1\",\n            save_total_limit=1, \n            fp16=torch.cuda.is_available(), \n            report_to=\"none\", \n            seed=SEED, \n            disable_tqdm=False\n        )\n        lora_final_trainer_ner = Trainer(\n            model=lora_final_model_ner_peft, \n            args=lora_final_args_ner, \n            train_dataset=ner_tokenized_train,\n            eval_dataset=ner_tokenized_dev, \n            data_collator=ner_data_collator, # Ensure ner_data_collator is defined\n            compute_metrics=compute_metrics_ner_revised # Ensure compute_metrics_ner_revised is defined\n        )\n        print(\"Starting NER final LoRA training...\")\n        lora_final_trainer_ner.train()\n        print(\"NER final LoRA training completed.\")\n        print(\"Evaluating final NER LoRA model (best checkpoint automatically loaded)...\")\n        final_lora_ner_metrics = lora_final_trainer_ner.evaluate()\n        print(f\"NER Final LoRA Metrics ({MODEL_NAME_NER}):\")\n        for k, v_metric in final_lora_ner_metrics.items():\n            print(f\"  {k}: {v_metric:.4f}\")\n        lora_final_trainer_ner.save_model(str(ner_final_lora_output_dir / \"best_model\"))\n        print(f\"Final NER LoRA model saved to {ner_final_lora_output_dir / 'best_model'}\")\n    except Exception as e:\n        print(f\"Error during NER final LoRA run: {e}\")\n        final_lora_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\n        import traceback; traceback.print_exc()\n\nall_experiment_results['NER Final LoRA (HPO)'] = final_lora_ner_metrics\n%store all_experiment_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:56:28.922945Z","iopub.execute_input":"2025-05-17T14:56:28.923191Z","iopub.status.idle":"2025-05-17T14:57:36.335419Z","shell.execute_reply.started":"2025-05-17T14:56:28.923175Z","shell.execute_reply":"2025-05-17T14:57:36.334792Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_NER\nno stored variable or alias NUM_TRAIN_EPOCHS_FINAL_NER\nno stored variable or alias OUTPUT_BASE_DIR\nno stored variable or alias SEED\n--- NER Final LoRA Fine-Tuning for t5-small (15 epochs) ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\nNo label_names provided for model class `PeftModelForTokenClassification`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n","output_type":"stream"},{"name":"stdout","text":"Starting NER final LoRA training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='345' max='345' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [345/345 01:04, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Token Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>10.168500</td>\n      <td>9.504787</td>\n      <td>0.000492</td>\n      <td>0.012821</td>\n      <td>0.000948</td>\n      <td>0.010069</td>\n      <td>0.010069</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>9.074400</td>\n      <td>7.858705</td>\n      <td>0.000504</td>\n      <td>0.012821</td>\n      <td>0.000970</td>\n      <td>0.028417</td>\n      <td>0.028417</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>7.204200</td>\n      <td>5.282507</td>\n      <td>0.000713</td>\n      <td>0.015837</td>\n      <td>0.001364</td>\n      <td>0.144784</td>\n      <td>0.144784</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>4.636900</td>\n      <td>2.384498</td>\n      <td>0.001042</td>\n      <td>0.010558</td>\n      <td>0.001898</td>\n      <td>0.569815</td>\n      <td>0.569815</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>2.568100</td>\n      <td>1.391037</td>\n      <td>0.000502</td>\n      <td>0.000754</td>\n      <td>0.000603</td>\n      <td>0.835292</td>\n      <td>0.835292</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>1.868900</td>\n      <td>1.130109</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.863469</td>\n      <td>0.863469</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>1.639500</td>\n      <td>1.027583</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.871481</td>\n      <td>0.871481</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>1.496100</td>\n      <td>0.971962</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.872149</td>\n      <td>0.872149</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>1.443000</td>\n      <td>0.914114</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.872176</td>\n      <td>0.872176</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>1.380800</td>\n      <td>0.886717</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.872202</td>\n      <td>0.872202</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.364300</td>\n      <td>0.875542</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.872202</td>\n      <td>0.872202</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.349600</td>\n      <td>0.858019</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.872202</td>\n      <td>0.872202</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.325300</td>\n      <td>0.843117</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.872202</td>\n      <td>0.872202</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.314800</td>\n      <td>0.842891</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.872202</td>\n      <td>0.872202</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.311800</td>\n      <td>0.841861</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.872202</td>\n      <td>0.872202</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"NER final LoRA training completed.\nEvaluating final NER LoRA model (best checkpoint automatically loaded)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [6/6 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"NER Final LoRA Metrics (t5-small):\n  eval_loss: 2.3845\n  eval_precision: 0.0010\n  eval_recall: 0.0106\n  eval_f1: 0.0019\n  eval_accuracy: 0.5698\n  eval_token_accuracy: 0.5698\n  eval_runtime: 0.9937\n  eval_samples_per_second: 81.5150\n  eval_steps_per_second: 6.0380\n  epoch: 15.0000\nFinal NER LoRA model saved to /kaggle/working/outputs/t5-small-ner-lora-final/best_model\nStored 'all_experiment_results' (defaultdict)\n","output_type":"stream"}],"execution_count":73},{"id":"ner-final-freeze-markdown","cell_type":"markdown","source":"### 13.3 Final Partial-Freeze Fine-Tuning (NER)","metadata":{}},{"id":"ner-final-freeze-code","cell_type":"code","source":"%store -r MODEL_NAME_NER num_ner_labels ner_id_to_label ner_label_to_id ner_tokenized_train ner_tokenized_dev best_ner_freeze_params NUM_TRAIN_EPOCHS_FINAL_NER OUTPUT_BASE_DIR SEED all_experiment_results\n\nprint(f\"--- NER Final Partial-Freeze for {MODEL_NAME_NER} ({NUM_TRAIN_EPOCHS_FINAL_NER} epochs) ---\")\n\nif 'best_ner_freeze_params' not in globals() or not isinstance(best_ner_freeze_params, dict):\n    print(\"Warning: best_ner_freeze_params not found from HPO. Using default Freeze params for final run.\")\n    model_config_temp_f_final = AutoConfig.from_pretrained(MODEL_NAME_NER)\n    num_total_encoder_layers_f_final = getattr(model_config_temp_f_final, 'num_layers', getattr(model_config_temp_f_final, 'num_hidden_layers', 6))\n    best_ner_freeze_params = {\"learning_rate\": 3e-5, \"num_layers_to_freeze\": num_total_encoder_layers_f_final // 2, \"batch_size\": 8}\n\nner_final_freeze_output_dir = Path(OUTPUT_BASE_DIR) / f\"{MODEL_NAME_NER.replace('/', '_')}-ner-freeze-final\"\n\nif len(ner_tokenized_train) == 0 or len(ner_tokenized_dev) == 0:\n    print(\"Skipping NER final Freeze training as processed datasets are empty.\")\n    final_freeze_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\nelse:\n    try:\n        freeze_final_model_ner = AutoModelForTokenClassification.from_pretrained(\n            MODEL_NAME_NER, num_labels=num_ner_labels, id2label=ner_id_to_label, label2id=ner_label_to_id, ignore_mismatched_sizes=True\n        )\n        num_layers_to_freeze_final = best_ner_freeze_params[\"num_layers_to_freeze\"]\n        param_prefix_to_freeze = \"encoder.block.\" if \"t5\" in MODEL_NAME_NER.lower() else \"encoder.layer.\"\n        print(f\"Freezing first {num_layers_to_freeze_final} encoder blocks (prefix: '{param_prefix_to_freeze}') for NER model...\")\n        for name, param in freeze_final_model_ner.named_parameters():\n            param.requires_grad = True\n            if name.startswith(param_prefix_to_freeze):\n                try:\n                    layer_idx = int(name.split('.')[2])\n                    if layer_idx < num_layers_to_freeze_final: param.requires_grad = False\n                except: pass\n            elif 'classifier' in name: param.requires_grad = True\n            elif 'shared.weight' in name and \"t5\" in MODEL_NAME_NER.lower(): param.requires_grad = True\n            elif 'embeddings' in name and \"bert\" in MODEL_NAME_NER.lower(): param.requires_grad = True\n\n        freeze_final_args_ner = TrainingArguments(\n            output_dir=str(ner_final_freeze_output_dir), \n            learning_rate=best_ner_freeze_params[\"learning_rate\"],\n            per_device_train_batch_size=best_ner_freeze_params[\"batch_size\"], \n            per_device_eval_batch_size=best_ner_freeze_params[\"batch_size\"] * 2,\n            num_train_epochs=NUM_TRAIN_EPOCHS_FINAL_NER, \n            logging_strategy=\"epoch\", \n            eval_strategy=\"epoch\",       # <--- CORRECTED from evaluation_strategy\n            save_strategy=\"epoch\", \n            load_best_model_at_end=True, \n            metric_for_best_model=\"eval_f1\",\n            save_total_limit=1, \n            fp16=torch.cuda.is_available(), \n            report_to=\"none\", \n            seed=SEED, \n            disable_tqdm=False\n        )\n        freeze_final_trainer_ner = Trainer(\n            model=freeze_final_model_ner, \n            args=freeze_final_args_ner, \n            train_dataset=ner_tokenized_train,\n            eval_dataset=ner_tokenized_dev, \n            data_collator=ner_data_collator, # Ensure ner_data_collator is defined\n            compute_metrics=compute_metrics_ner_revised # Ensure compute_metrics_ner_revised is defined\n        )\n        print(\"Starting NER final partial-freeze training...\")\n        freeze_final_trainer_ner.train()\n        print(\"NER final partial-freeze training completed.\")\n        print(\"Evaluating final NER Partial-Freeze model (best checkpoint automatically loaded)...\")\n        final_freeze_ner_metrics = freeze_final_trainer_ner.evaluate()\n        print(f\"NER Final Partial-Freeze Metrics ({MODEL_NAME_NER}):\")\n        for k, v_metric in final_freeze_ner_metrics.items():\n            print(f\"  {k}: {v_metric:.4f}\")\n        freeze_final_trainer_ner.save_model(str(ner_final_freeze_output_dir / \"best_model\"))\n        print(f\"Final NER Partial-Freeze model saved to {ner_final_freeze_output_dir / 'best_model'}\")\n    except Exception as e:\n        print(f\"Error during NER final Freeze run: {e}\")\n        final_freeze_ner_metrics = {key: 0.0 for key in ['eval_loss', 'eval_precision', 'eval_recall', 'eval_f1', 'eval_accuracy', 'eval_token_accuracy']}\n        import traceback; traceback.print_exc()\n\nall_experiment_results['NER Final Partial-Freeze (HPO)'] = final_freeze_ner_metrics\n%store all_experiment_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:57:36.336096Z","iopub.execute_input":"2025-05-17T14:57:36.336275Z","iopub.status.idle":"2025-05-17T14:58:52.491986Z","shell.execute_reply.started":"2025-05-17T14:57:36.336261Z","shell.execute_reply":"2025-05-17T14:58:52.491207Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_NER\nno stored variable or alias best_ner_freeze_params\nno stored variable or alias NUM_TRAIN_EPOCHS_FINAL_NER\nno stored variable or alias OUTPUT_BASE_DIR\nno stored variable or alias SEED\n--- NER Final Partial-Freeze for t5-small (15 epochs) ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForTokenClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Freezing first 3 encoder blocks (prefix: 'encoder.block.') for NER model...\nStarting NER final partial-freeze training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='180' max='180' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [180/180 01:13, Epoch 15/15]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Precision</th>\n      <th>Recall</th>\n      <th>F1</th>\n      <th>Accuracy</th>\n      <th>Token Accuracy</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>9.689800</td>\n      <td>8.464564</td>\n      <td>0.000497</td>\n      <td>0.012821</td>\n      <td>0.000958</td>\n      <td>0.019337</td>\n      <td>0.019337</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>8.095900</td>\n      <td>6.837008</td>\n      <td>0.000608</td>\n      <td>0.015083</td>\n      <td>0.001170</td>\n      <td>0.055205</td>\n      <td>0.055205</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>6.635200</td>\n      <td>5.348951</td>\n      <td>0.000733</td>\n      <td>0.016591</td>\n      <td>0.001405</td>\n      <td>0.135703</td>\n      <td>0.135703</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>5.385600</td>\n      <td>4.015586</td>\n      <td>0.000875</td>\n      <td>0.016591</td>\n      <td>0.001663</td>\n      <td>0.267160</td>\n      <td>0.267160</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>4.233900</td>\n      <td>2.963552</td>\n      <td>0.001183</td>\n      <td>0.016591</td>\n      <td>0.002208</td>\n      <td>0.440575</td>\n      <td>0.440575</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>3.364300</td>\n      <td>2.210903</td>\n      <td>0.001003</td>\n      <td>0.009050</td>\n      <td>0.001805</td>\n      <td>0.609743</td>\n      <td>0.609743</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>2.751200</td>\n      <td>1.785083</td>\n      <td>0.000886</td>\n      <td>0.004525</td>\n      <td>0.001482</td>\n      <td>0.732653</td>\n      <td>0.732653</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>2.340400</td>\n      <td>1.591673</td>\n      <td>0.001061</td>\n      <td>0.003017</td>\n      <td>0.001570</td>\n      <td>0.800465</td>\n      <td>0.800465</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>2.178200</td>\n      <td>1.481024</td>\n      <td>0.001636</td>\n      <td>0.003017</td>\n      <td>0.002121</td>\n      <td>0.828668</td>\n      <td>0.828668</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>2.020000</td>\n      <td>1.380553</td>\n      <td>0.000989</td>\n      <td>0.001508</td>\n      <td>0.001194</td>\n      <td>0.837322</td>\n      <td>0.837322</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>1.952000</td>\n      <td>1.298220</td>\n      <td>0.001093</td>\n      <td>0.001508</td>\n      <td>0.001268</td>\n      <td>0.841568</td>\n      <td>0.841568</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>1.850600</td>\n      <td>1.237916</td>\n      <td>0.001156</td>\n      <td>0.001508</td>\n      <td>0.001309</td>\n      <td>0.844106</td>\n      <td>0.844106</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>1.831300</td>\n      <td>1.196617</td>\n      <td>0.001199</td>\n      <td>0.001508</td>\n      <td>0.001336</td>\n      <td>0.845735</td>\n      <td>0.845735</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>1.765600</td>\n      <td>1.174720</td>\n      <td>0.001224</td>\n      <td>0.001508</td>\n      <td>0.001351</td>\n      <td>0.846536</td>\n      <td>0.846536</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>1.759100</td>\n      <td>1.167180</td>\n      <td>0.000618</td>\n      <td>0.000754</td>\n      <td>0.000679</td>\n      <td>0.846910</td>\n      <td>0.846910</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"There were missing keys in the checkpoint model loaded: ['transformer.encoder.embed_tokens.weight'].\n","output_type":"stream"},{"name":"stdout","text":"NER final partial-freeze training completed.\nEvaluating final NER Partial-Freeze model (best checkpoint automatically loaded)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [3/3 00:00]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"NER Final Partial-Freeze Metrics (t5-small):\n  eval_loss: 2.9636\n  eval_precision: 0.0012\n  eval_recall: 0.0166\n  eval_f1: 0.0022\n  eval_accuracy: 0.4406\n  eval_token_accuracy: 0.4406\n  eval_runtime: 0.9458\n  eval_samples_per_second: 85.6410\n  eval_steps_per_second: 3.1720\n  epoch: 15.0000\nFinal NER Partial-Freeze model saved to /kaggle/working/outputs/t5-small-ner-freeze-final/best_model\nStored 'all_experiment_results' (defaultdict)\n","output_type":"stream"}],"execution_count":74},{"id":"re-section-start-markdown","cell_type":"markdown","source":"# Part II: Relation Extraction (RE)","metadata":{}},{"id":"re-label-mapping-markdown","cell_type":"markdown","source":"## 14. RE: Label Mapping","metadata":{}},{"id":"re-labels-code","cell_type":"code","source":"print(\"--- RE Label Mapping ---\")\nall_re_relation_types_from_data = []\nif 'train_docs_raw' in globals() and train_docs_raw:\n    for doc in train_docs_raw:\n        for triple in doc.get('triples', []):\n            all_re_relation_types_from_data.append(triple.get('relation', 'UNKNOWN_REL'))\nelse:\n    print(\"Warning: train_docs_raw not found or empty. RE label set will be minimal.\")\n\nif not all_re_relation_types_from_data:\n    print(\"CRITICAL WARNING: No relation types found in training data. Using minimal fallback for RE.\")\n    unique_re_relations_from_data = [\"RELATED_TO\"]\nelse:\n    re_relation_counts = Counter(all_re_relation_types_from_data)\n    unique_re_relations_from_data = sorted(list(re_relation_counts.keys() - {'UNKNOWN_REL'}))\n\nNO_RELATION_LABEL_RE = \"NO_RELATION\"\nre_labels_list = [NO_RELATION_LABEL_RE] + [r for r in unique_re_relations_from_data if r != NO_RELATION_LABEL_RE]\nre_labels_list = sorted(list(set(re_labels_list)), key=lambda x: (x != NO_RELATION_LABEL_RE, x))\n\nre_label_to_id = {label: i for i, label in enumerate(re_labels_list)}\nre_id_to_label = {i: label for i, label in enumerate(re_labels_list)}\nnum_re_labels = len(re_labels_list)\n\nprint(f\"Number of unique RE Labels (including {NO_RELATION_LABEL_RE}): {num_re_labels}\")\nprint(f\"Sample RE Labels (first 10): {re_labels_list[:min(10, num_re_labels)]}\")\nprint(f\"re_label_to_id['{NO_RELATION_LABEL_RE}'] = {re_label_to_id.get(NO_RELATION_LABEL_RE, 'ERROR')}\")\n\n%store re_label_to_id\n%store re_id_to_label\n%store num_re_labels\n%store NO_RELATION_LABEL_RE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:58:52.492849Z","iopub.execute_input":"2025-05-17T14:58:52.493102Z","iopub.status.idle":"2025-05-17T14:58:52.508046Z","shell.execute_reply.started":"2025-05-17T14:58:52.493076Z","shell.execute_reply":"2025-05-17T14:58:52.507491Z"}},"outputs":[{"name":"stdout","text":"--- RE Label Mapping ---\nNumber of unique RE Labels (including NO_RELATION): 69\nSample RE Labels (first 10): ['NO_RELATION', 'AcademicDegree', 'AdjacentStation', 'Affiliation', 'AppliesToPeople', 'ApprovedBy', 'Author', 'BasedOn', 'Capital', 'CitesWork']\nre_label_to_id['NO_RELATION'] = 0\nStored 're_label_to_id' (dict)\nStored 're_id_to_label' (dict)\nStored 'num_re_labels' (int)\nStored 'NO_RELATION_LABEL_RE' (str)\n","output_type":"stream"}],"execution_count":75},{"id":"re-tokenizer-markdown","cell_type":"markdown","source":"## 15. RE: Tokenizer Initialization","metadata":{}},{"id":"re-tokenizer-code","cell_type":"code","source":"%store -r MODEL_NAME_RE MODEL_NAME_NER \n\nprint(f\"--- Initializing Tokenizer for RE model: {MODEL_NAME_RE} ---\")\nif 'ner_tokenizer' in globals() and MODEL_NAME_RE == MODEL_NAME_NER:\n    re_tokenizer = ner_tokenizer\n    print(f\"Reusing tokenizer from NER for RE model: {MODEL_NAME_RE}\")\nelse:\n    re_tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME_RE, use_fast=True)\n    print(f\"Initialized new tokenizer for RE model: {MODEL_NAME_RE}\")\n\n# Add special tokens if a more structured input is desired for RE, e.g., entity markers.\n# For this version, we embed type and text directly into the input string. Example:\n# special_tokens_re = {'additional_special_tokens': ['<H>', '</H>', '<T>', '</T>', '<ENT>']}\n# num_added_toks_re = re_tokenizer.add_special_tokens(special_tokens_re)\n# if num_added_toks_re > 0:\n#    print(f\"Added {num_added_toks_re} special tokens to RE tokenizer: {special_tokens_re['additional_special_tokens']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:58:52.508770Z","iopub.execute_input":"2025-05-17T14:58:52.508978Z","iopub.status.idle":"2025-05-17T14:58:52.524528Z","shell.execute_reply.started":"2025-05-17T14:58:52.508956Z","shell.execute_reply":"2025-05-17T14:58:52.523948Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_RE\nno stored variable or alias MODEL_NAME_NER\n--- Initializing Tokenizer for RE model: t5-small ---\nReusing tokenizer from NER for RE model: t5-small\n","output_type":"stream"}],"execution_count":76},{"id":"re-data-prep-func-markdown","cell_type":"markdown","source":"## 16. RE: Data Preparation Function","metadata":{}},{"id":"re-data-prep-func-code","cell_type":"code","source":"import nltk.data \nfrom tqdm.auto import tqdm\n\n# Ensure sentence tokenizer is loaded (might have been done in RE.1)\ntry:\n    sent_tokenizer_re_nltk = nltk.data.load('tokenizers/punkt/english.pickle')\nexcept LookupError:\n    nltk.download('punkt', quiet=True)\n    sent_tokenizer_re_nltk = nltk.data.load('tokenizers/punkt/english.pickle')\n\ndef get_sentence_spans_re(doc_text):\n    \"\"\"Returns a list of (start_char, end_char, sentence_text) tuples.\"\"\"\n    sentence_spans = []\n    try: # Add try-except for robustness if doc_text is not a string\n        if not isinstance(doc_text, str):\n            # print(f\"Warning: doc_text is not a string, it's {type(doc_text)}. Skipping sentence tokenization for this doc.\")\n            return []\n        for start, end in sent_tokenizer_re_nltk.span_tokenize(doc_text):\n            sentence_spans.append((start, end, doc_text[start:end]))\n    except Exception as e:\n        # print(f\"Error during sentence tokenization: {e}\")\n        pass\n    return sentence_spans\n\ndef create_re_examples_for_classification(documents, relation_map, no_relation_val):\n    re_formatted_examples = []\n    max_entities_per_sentence_heuristic = 10 \n\n    for doc_idx_prog, doc in enumerate(tqdm(documents, desc=\"Processing documents for RE examples\")):\n        doc_id = doc.get('id', f'doc_{doc_idx_prog}')\n        # Use 'document_text' as standardized in the data loading cell\n        doc_text = doc.get('document_text', '') \n        entities_in_doc = doc.get('entities', [])\n        triples_in_doc = doc.get('triples', [])\n\n        if not doc_text or not entities_in_doc:\n            continue\n\n        gold_relation_lookup = {(triple['head'], triple['tail']): triple['relation'] for triple in triples_in_doc}\n        \n        # Map original entity index to its details (first mention text, type, AND DYNAMICALLY FOUND start/end)\n        entity_details_map = {}\n        for orig_entity_idx, entity_data in enumerate(entities_in_doc):\n            if entity_data.get('mentions') and isinstance(entity_data['mentions'], list) and entity_data['mentions']:\n                # *** CORRECTION STARTS HERE ***\n                first_mention_text = entity_data['mentions'][0] # Assuming this is a string\n                \n                if not isinstance(first_mention_text, str):\n                    # print(f\"Warning: Expected first_mention_text to be a string, but got {type(first_mention_text)} in doc {doc_id}. Skipping entity.\")\n                    continue\n\n                # Find start/end using the document text\n                mention_char_start = doc_text.find(first_mention_text)\n                if mention_char_start == -1:\n                    # print(f\"Warning: Mention '{first_mention_text}' not found in doc_text of doc {doc_id}. Skipping entity.\")\n                    continue\n                mention_char_end = mention_char_start + len(first_mention_text)\n                # *** CORRECTION ENDS HERE ***\n\n                entity_details_map[orig_entity_idx] = {\n                    'text': first_mention_text, # Store the string itself\n                    'type': entity_data.get('type', 'UNK_TYPE'),\n                    'start': mention_char_start, # Store the found start\n                    'end': mention_char_end      # Store the found end\n                }\n        \n        sentence_spans = get_sentence_spans_re(doc_text)\n\n        for sent_idx, (sent_start_char, sent_end_char, sentence_text) in enumerate(sentence_spans):\n            entities_in_current_sentence = []\n            for orig_idx, details in entity_details_map.items():\n                # Check if the entity's (dynamically found) mention is within this sentence\n                if details['start'] != -1 and sent_start_char <= details['start'] < sent_end_char:\n                    entities_in_current_sentence.append((orig_idx, details))\n            \n            if len(entities_in_current_sentence) < 2 or len(entities_in_current_sentence) > max_entities_per_sentence_heuristic:\n                continue\n\n            for i in range(len(entities_in_current_sentence)):\n                for j in range(len(entities_in_current_sentence)):\n                    if i == j: continue \n\n                    head_original_idx, head_details = entities_in_current_sentence[i]\n                    tail_original_idx, tail_details = entities_in_current_sentence[j]\n                    \n                    input_text_re = (\n                        f\"relación: {head_details['type']} \"\n                        f\"\\\\\\\"{head_details['text']}\\\\\\\" y {tail_details['type']} \"\n                        f\"\\\\\\\"{tail_details['text']}\\\\\\\". contexto: {sentence_text}\"\n                    )\n                    \n                    relation_type = gold_relation_lookup.get((head_original_idx, tail_original_idx), no_relation_val)\n                    label_id = relation_map.get(relation_type, relation_map[no_relation_val])\n                                            \n                    re_formatted_examples.append({\"text\": input_text_re, \"label\": label_id})\n                            \n    return re_formatted_examples\n\nprint(\"RE example creation function (create_re_examples_for_classification) defined with correction for string mentions.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:58:52.525312Z","iopub.execute_input":"2025-05-17T14:58:52.526066Z","iopub.status.idle":"2025-05-17T14:58:52.559566Z","shell.execute_reply.started":"2025-05-17T14:58:52.526040Z","shell.execute_reply":"2025-05-17T14:58:52.558964Z"}},"outputs":[{"name":"stdout","text":"RE example creation function (create_re_examples_for_classification) defined with correction for string mentions.\n","output_type":"stream"}],"execution_count":77},{"id":"re-create-hf-datasets-markdown","cell_type":"markdown","source":"## 17. RE: Create Hugging Face Datasets","metadata":{}},{"id":"re-create-hf-datasets-code","cell_type":"code","source":"%store -r re_label_to_id NO_RELATION_LABEL_RE re_id_to_label # Retrieve from RE label mapping cell\n\nprint(\"--- Creating RE examples for training set ---\")\nre_train_examples = create_re_examples_for_classification(train_docs_raw, re_label_to_id, NO_RELATION_LABEL_RE)\nprint(f\"Generated {len(re_train_examples)} RE examples for training.\")\n\nprint(\"\\n--- Creating RE examples for development set ---\")\nre_dev_examples = create_re_examples_for_classification(dev_docs_raw, re_label_to_id, NO_RELATION_LABEL_RE)\nprint(f\"Generated {len(re_dev_examples)} RE examples for development.\")\n\nif not re_train_examples:\n    print(\"CRITICAL WARNING: No RE training examples generated. RE training will be skipped or fail.\")\n    re_hf_train = Dataset.from_dict({\"text\": [], \"label\": []})\nelse:\n    re_hf_train = Dataset.from_list(re_train_examples)\n\nif not re_dev_examples:\n    print(\"WARNING: No RE development examples generated. RE evaluation will be skipped or produce trivial results.\")\n    re_hf_dev = Dataset.from_dict({\"text\": [], \"label\": []})\nelse:\n    re_hf_dev = Dataset.from_list(re_dev_examples)\n\nprint(\"\\nSample RE training example:\")\nif len(re_hf_train) > 0:\n    print(re_hf_train[0])\n    re_label_counts_train = Counter(re_hf_train['label'])\n    print(f\"\\nRE Training label distribution (Top 10 relations by count):\")\n    for label_id_count, count_val in sorted(re_label_counts_train.items(), key=lambda item: item[1], reverse=True)[:10]:\n        print(f\"  Label '{re_id_to_label.get(label_id_count, 'UnknownLabel')}': {count_val}\")\nelse:\n    print(\"No RE training examples available.\")\n\n%store re_hf_train\n%store re_hf_dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:58:52.561970Z","iopub.execute_input":"2025-05-17T14:58:52.562516Z","iopub.status.idle":"2025-05-17T14:58:52.726774Z","shell.execute_reply.started":"2025-05-17T14:58:52.562497Z","shell.execute_reply":"2025-05-17T14:58:52.726184Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias #\nno stored variable or alias Retrieve\nno stored variable or alias from\nno stored variable or alias RE\nno stored variable or alias label\nno stored variable or alias mapping\nno stored variable or alias cell\n--- Creating RE examples for training set ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing documents for RE examples:   0%|          | 0/51 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"33276c85de1c481ea84bbec9edea7037"}},"metadata":{}},{"name":"stdout","text":"Generated 6284 RE examples for training.\n\n--- Creating RE examples for development set ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing documents for RE examples:   0%|          | 0/23 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fa5a364fde6145c8988d49949a00f389"}},"metadata":{}},{"name":"stdout","text":"Generated 1390 RE examples for development.\n\nSample RE training example:\n{'text': 'relación: PERSON \\\\\"William Thomson, 1st Baron Kelvin\\\\\" y ORDINAL \\\\\"1st\\\\\". contexto: The  automatic curb sender was a kind of telegraph key, invented by William Thomson, 1st Baron Kelvin for sending messages on a submarine communications cable, as the well-known Wheatstone transmitter sends them on a land line.', 'label': 0}\n\nRE Training label distribution (Top 10 relations by count):\n  Label 'NO_RELATION': 6284\nStored 're_hf_train' (Dataset)\nStored 're_hf_dev' (Dataset)\n","output_type":"stream"}],"execution_count":78},{"id":"re-tokenize-datasets-markdown","cell_type":"markdown","source":"## 18. RE: Tokenize Datasets","metadata":{}},{"id":"re-tokenize-datasets-code","cell_type":"code","source":"MAX_SEQ_LENGTH_RE = 256 # Max sequence length for RE model inputs\n\ndef tokenize_re_function(examples):\n    return re_tokenizer(examples[\"text\"], truncation=True, max_length=MAX_SEQ_LENGTH_RE, padding=False)\n\nprint(\"--- Tokenizing RE datasets ---\")\nif len(re_hf_train) > 0:\n    re_tokenized_train = re_hf_train.map(tokenize_re_function, batched=True, remove_columns=[\"text\"])\n    print(f\"Tokenized RE training set: {len(re_tokenized_train)} examples.\")\nelse:\n    re_tokenized_train = re_hf_train \n    print(\"RE training set is empty, tokenization skipped.\")\n\nif len(re_hf_dev) > 0:\n    re_tokenized_dev = re_hf_dev.map(tokenize_re_function, batched=True, remove_columns=[\"text\"])\n    print(f\"Tokenized RE development set: {len(re_tokenized_dev)} examples.\")\nelse:\n    re_tokenized_dev = re_hf_dev \n    print(\"RE development set is empty, tokenization skipped.\")\n\nif len(re_tokenized_train) > 0:\n    print(f\"Features in tokenized RE train set: {re_tokenized_train.features}\")\n\n%store re_tokenized_train\n%store re_tokenized_dev","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:58:52.727475Z","iopub.execute_input":"2025-05-17T14:58:52.727652Z","iopub.status.idle":"2025-05-17T14:58:53.831080Z","shell.execute_reply.started":"2025-05-17T14:58:52.727638Z","shell.execute_reply":"2025-05-17T14:58:53.830507Z"}},"outputs":[{"name":"stdout","text":"--- Tokenizing RE datasets ---\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/6284 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"313e331d0ac34f57b4b72aff15a0e4fb"}},"metadata":{}},{"name":"stdout","text":"Tokenized RE training set: 6284 examples.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/1390 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"32aed4c8ad9548b9be69634ac4c2d693"}},"metadata":{}},{"name":"stdout","text":"Tokenized RE development set: 1390 examples.\nFeatures in tokenized RE train set: {'label': Value(dtype='int64', id=None), 'input_ids': Sequence(feature=Value(dtype='int32', id=None), length=-1, id=None), 'attention_mask': Sequence(feature=Value(dtype='int8', id=None), length=-1, id=None)}\nStored 're_tokenized_train' (Dataset)\nStored 're_tokenized_dev' (Dataset)\n","output_type":"stream"}],"execution_count":79},{"id":"re-model-collator-metrics-markdown","cell_type":"markdown","source":"## 19. RE: Model, Data Collator, and Metrics Function","metadata":{}},{"id":"re-model-collator-metrics-code","cell_type":"code","source":"%store -r MODEL_NAME_RE num_re_labels re_id_to_label re_label_to_id NO_RELATION_LABEL_RE # Retrieve variables\n\nprint(f\"--- Initializing RE Model: {MODEL_NAME_RE} ---\")\nre_model_config = AutoConfig.from_pretrained(\n    MODEL_NAME_RE, \n    num_labels=num_re_labels, \n    id2label=re_id_to_label, \n    label2id=re_label_to_id\n)\n# This model instance is used for the raw evaluation\nre_model_for_raw_eval = AutoModelForSequenceClassification.from_pretrained(\n    MODEL_NAME_RE, \n    config=re_model_config, \n    ignore_mismatched_sizes=True # Helpful if base model not originally for seq classification\n)\nprint(f\"RE Model for Raw Eval ({type(re_model_for_raw_eval)}) loaded.\")\n\n\nre_data_collator = DataCollatorWithPadding(tokenizer=re_tokenizer, padding='longest')\nprint(\"RE Data Collator initialized.\")\n\ndef compute_metrics_re_revised(p):\n    predictions_logits_input, labels = p\n    \n    print(f\"Inside compute_metrics_re_revised:\")\n    print(f\"  Type of predictions_logits_input: {type(predictions_logits_input)}\")\n    \n    processed_logits = []\n    valid_labels_list = []\n\n    if isinstance(predictions_logits_input, tuple):\n        # Common if model outputs multiple things (e.g. logits, hidden_states)\n        # Assuming the first element is the actual logits\n        print(f\"  predictions_logits_input is a tuple. Using the first element.\")\n        actual_logits = predictions_logits_input[0]\n    else:\n        actual_logits = predictions_logits_input\n\n    if isinstance(actual_logits, np.ndarray):\n        print(f\"  Shape of actual_logits (as ndarray): {actual_logits.shape}\")\n        # If it's already a 2D numpy array (num_samples, num_classes), it should be fine\n        if actual_logits.ndim == 2 and actual_logits.shape[1] == num_re_labels:\n            predictions = np.argmax(actual_logits, axis=1)\n            # Ensure labels align with the number of prediction samples\n            if len(labels) != actual_logits.shape[0]:\n                 print(f\"  Warning: Mismatch between labels length ({len(labels)}) and predictions_logits samples ({actual_logits.shape[0]})\")\n                 # Attempt to align if it's a batching artifact, otherwise metrics will be wrong\n                 # This part might need more sophisticated handling depending on why the mismatch occurs\n                 min_len = min(len(labels), actual_logits.shape[0])\n                 labels = labels[:min_len]\n                 predictions = predictions[:min_len]\n        elif actual_logits.ndim == 1 and len(actual_logits) == num_re_labels: # Single sample passed directly\n            print(f\"  actual_logits looks like a single sample's logits. Reshaping.\")\n            predictions = np.array([np.argmax(actual_logits)]) # Make it a 1-element array\n            labels = np.array([labels[0]]) if isinstance(labels, (list, np.ndarray)) and len(labels)>0 else np.array([])\n        else:\n            print(f\"  Error: actual_logits is an ndarray but has unexpected shape: {actual_logits.shape}. Expected (num_samples, {num_re_labels}).\")\n            return {'accuracy': 0.0, 'f1_weighted': 0.0, 'precision_weighted': 0.0, 'recall_weighted': 0.0, 'f1_actual_relations_micro': 0.0}\n            \n    elif isinstance(actual_logits, list): # Potentially a list of lists or list of arrays\n        print(f\"  actual_logits is a list. Length: {len(actual_logits)}\")\n        # Try to convert to a proper numpy array, assuming it's a list of logit arrays (one per sample)\n        # This is where the inhomogeneous error likely originates if inner lists/arrays have different structures\n        try:\n            # We expect each item in the list to be a 1D array/list of logits of length num_re_labels\n            # Filter out any elements that are not lists/arrays or don't have the correct length\n            filtered_logits = []\n            corresponding_labels = []\n            for idx, logit_item in enumerate(actual_logits):\n                if hasattr(logit_item, '__len__') and len(logit_item) == num_re_labels:\n                    filtered_logits.append(logit_item)\n                    if idx < len(labels):\n                        corresponding_labels.append(labels[idx])\n                    else:\n                        print(f\"  Warning: Label missing for logit item at index {idx}\")\n                else:\n                    print(f\"  Warning: Skipping logit item at index {idx} due to unexpected structure/length. Item: {logit_item}\")\n\n            if not filtered_logits:\n                print(\"  Error: No valid logits found after filtering list items.\")\n                return {'accuracy': 0.0, 'f1_weighted': 0.0, 'precision_weighted': 0.0, 'recall_weighted': 0.0, 'f1_actual_relations_micro': 0.0}\n\n            predictions_logits_np = np.array(filtered_logits) # This might still fail if items had sub-structure\n            print(f\"  Shape of predictions_logits_np after converting list: {predictions_logits_np.shape}\")\n            if predictions_logits_np.ndim != 2 or predictions_logits_np.shape[1] != num_re_labels:\n                print(f\"  Error: Converted predictions_logits_np has unexpected shape: {predictions_logits_np.shape}. Expected (num_samples, {num_re_labels}).\")\n                return {'accuracy': 0.0, 'f1_weighted': 0.0, 'precision_weighted': 0.0, 'recall_weighted': 0.0, 'f1_actual_relations_micro': 0.0}\n            \n            predictions = np.argmax(predictions_logits_np, axis=1)\n            labels = np.array(corresponding_labels) # Use the labels that correspond to the filtered logits\n\n        except ValueError as e:\n            print(f\"  ValueError converting list of logits to numpy array: {e}\")\n            print(f\"  First 5 items of actual_logits list (lengths): {[len(li) if hasattr(li, '__len__') else 'NotIterable' for li in actual_logits[:5]]}\")\n            return {'accuracy': 0.0, 'f1_weighted': 0.0, 'precision_weighted': 0.0, 'recall_weighted': 0.0, 'f1_actual_relations_micro': 0.0}\n    else:\n        print(f\"  Error: Unexpected type for actual_logits: {type(actual_logits)}\")\n        return {'accuracy': 0.0, 'f1_weighted': 0.0, 'precision_weighted': 0.0, 'recall_weighted': 0.0, 'f1_actual_relations_micro': 0.0}\n\n    if len(predictions) == 0 or len(labels) == 0 or len(predictions) != len(labels):\n        print(f\"  Error: Predictions (len {len(predictions)}) and labels (len {len(labels)}) are empty or have mismatched lengths after processing. Cannot compute metrics.\")\n        return {'accuracy': 0.0, 'f1_weighted': 0.0, 'precision_weighted': 0.0, 'recall_weighted': 0.0, 'f1_actual_relations_micro': 0.0}\n\n    # --- Metric calculation continues from here ---\n    accuracy = accuracy_score(labels, predictions)\n    precision_w, recall_w, f1_w, _ = precision_recall_fscore_support(\n        labels, predictions, average='weighted', zero_division=0\n    )\n    \n    no_relation_class_id = re_label_to_id.get(NO_RELATION_LABEL_RE, -1) \n    actual_relations_mask = (labels != no_relation_class_id)\n    f1_actual_micro = 0.0\n    precision_actual = 0.0\n    recall_actual = 0.0\n\n    if np.sum(actual_relations_mask) > 0:\n        labels_actual = labels[actual_relations_mask]\n        preds_actual = predictions[actual_relations_mask]\n        if len(labels_actual) > 0 and len(np.unique(labels_actual)) > 0 : \n            # Ensure there are some actual relation instances and more than one class (or handle single class case)\n            # Also, ensure labels parameter for precision_recall_fscore_support contains only present labels\n            unique_present_labels_actual = np.unique(labels_actual)\n            if len(unique_present_labels_actual) == 1 and len(np.unique(preds_actual)) == 1 and unique_present_labels_actual[0] == np.unique(preds_actual)[0] :\n                 # Handle case where only one class is present and correctly predicted for all instances of it.\n                 # This might require careful thought if this is the only \"actual\" relation class.\n                 # For micro, if all are correct, P/R/F1 = 1. If not, 0.\n                 if np.array_equal(labels_actual, preds_actual):\n                     precision_actual, recall_actual, f1_actual = 1.0, 1.0, 1.0\n                 else: # Some are wrong if not all equal\n                     p_a, r_a, f_a, _ = precision_recall_fscore_support(\n                        labels_actual, preds_actual, average='micro', zero_division=0, labels=unique_present_labels_actual\n                     )\n                     precision_actual, recall_actual, f1_actual = p_a, r_a, f_a\n\n            elif len(unique_present_labels_actual) > 0 :\n                p_a, r_a, f_a, _ = precision_recall_fscore_support(\n                    labels_actual, preds_actual, average='micro', zero_division=0, labels=unique_present_labels_actual\n                )\n                precision_actual, recall_actual, f1_actual = p_a, r_a, f_a\n\n\n    return {\n        'accuracy_overall': accuracy,\n        'f1_weighted_overall': f1_w,\n        'precision_weighted_overall': precision_w,\n        'recall_weighted_overall': recall_w,\n        'f1_actual_relations_micro': f1_actual_micro, # <--- CORRECTED to use f1_actual_micro\n        'precision_actual_relations_micro': precision_actual,\n        'recall_actual_relations_micro': recall_actual\n    \n    }\n\nprint(\"RE Model and Metrics function defined (with enhanced debugging for predictions_logits).\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:58:53.832209Z","iopub.execute_input":"2025-05-17T14:58:53.832408Z","iopub.status.idle":"2025-05-17T14:58:54.191523Z","shell.execute_reply.started":"2025-05-17T14:58:53.832394Z","shell.execute_reply":"2025-05-17T14:58:54.190944Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_RE\nno stored variable or alias #\nno stored variable or alias Retrieve\nno stored variable or alias variables\n--- Initializing RE Model: t5-small ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"RE Model for Raw Eval (<class 'transformers.models.t5.modeling_t5.T5ForSequenceClassification'>) loaded.\nRE Data Collator initialized.\nRE Model and Metrics function defined (with enhanced debugging for predictions_logits).\n","output_type":"stream"}],"execution_count":80},{"id":"re-raw-model-eval-markdown","cell_type":"markdown","source":"## 20. RE: Raw Pre-trained Model Evaluation (Baseline before Fine-tuning)","metadata":{}},{"id":"re-raw-model-eval-code","cell_type":"code","source":"%store -r MODEL_NAME_RE re_tokenized_dev OUTPUT_BASE_DIR SEED all_experiment_results\n\nprint(f\"--- RE Raw Model Evaluation for {MODEL_NAME_RE} ---\")\n\nif len(re_tokenized_dev) == 0:\n    print(\"Skipping RE raw model evaluation as tokenized dev dataset is empty.\")\n    raw_re_metrics = {key: 0.0 for key in ['eval_loss', 'eval_accuracy', 'eval_f1_weighted', 'eval_precision_weighted', 'eval_recall_weighted', 'eval_f1_actual_relations_micro']}\nelse:\n    try:\n        # Model already loaded as re_model_for_raw_eval in the previous cell\n        raw_re_output_dir = Path(OUTPUT_BASE_DIR) / f\"{MODEL_NAME_RE.replace('/', '_')}-re-raw-eval\"\n        raw_re_eval_args = TrainingArguments(\n            output_dir=str(raw_re_output_dir),\n            per_device_eval_batch_size=16,\n            do_train=False, do_eval=True, report_to=\"none\", seed=SEED\n        )\n        raw_re_trainer = Trainer(\n            model=re_model_for_raw_eval, args=raw_re_eval_args, eval_dataset=re_tokenized_dev,\n            data_collator=re_data_collator, compute_metrics=compute_metrics_re_revised\n        )\n        print(\"Evaluating raw RE model...\")\n        raw_re_metrics = raw_re_trainer.evaluate()\n        print(f\"RE Raw Model Metrics ({MODEL_NAME_RE}):\")\n        for k, v_metric in raw_re_metrics.items():\n            print(f\"  {k}: {v_metric:.4f}\")\n    except Exception as e:\n        print(f\"Error during RE raw model evaluation: {e}\")\n        raw_re_metrics = {key: 0.0 for key in ['eval_loss', 'eval_accuracy', 'eval_f1_weighted', 'eval_precision_weighted', 'eval_recall_weighted', 'eval_f1_actual_relations_micro']}\n        import traceback; traceback.print_exc()\n\nall_experiment_results['RE Raw Model'] = raw_re_metrics\n%store all_experiment_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:58:54.192226Z","iopub.execute_input":"2025-05-17T14:58:54.192503Z","iopub.status.idle":"2025-05-17T14:58:58.566908Z","shell.execute_reply.started":"2025-05-17T14:58:54.192478Z","shell.execute_reply":"2025-05-17T14:58:58.566204Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_RE\nno stored variable or alias OUTPUT_BASE_DIR\nno stored variable or alias SEED\n--- RE Raw Model Evaluation for t5-small ---\nEvaluating raw RE model...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='44' max='44' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [44/44 00:03]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"Inside compute_metrics_re_revised:\n  Type of predictions_logits_input: <class 'tuple'>\n  predictions_logits_input is a tuple. Using the first element.\n  Shape of actual_logits (as ndarray): (1390, 69)\nRE Raw Model Metrics (t5-small):\n  eval_loss: 4.3210\n  eval_accuracy_overall: 0.0000\n  eval_f1_weighted_overall: 0.0000\n  eval_precision_weighted_overall: 0.0000\n  eval_recall_weighted_overall: 0.0000\n  eval_f1_actual_relations_micro: 0.0000\n  eval_precision_actual_relations_micro: 0.0000\n  eval_recall_actual_relations_micro: 0.0000\n  eval_runtime: 4.1743\n  eval_samples_per_second: 332.9900\n  eval_steps_per_second: 10.5410\nStored 'all_experiment_results' (defaultdict)\n","output_type":"stream"}],"execution_count":81},{"id":"re-training-section-markdown","cell_type":"markdown","source":"## 21. RE: Model Training (Baseline/Final)","metadata":{}},{"id":"re-baseline-train-code","cell_type":"code","source":"%store -r MODEL_NAME_RE re_tokenized_train re_tokenized_dev NUM_TRAIN_EPOCHS_FINAL_RE OUTPUT_BASE_DIR SEED re_id_to_label re_label_to_id num_re_labels all_experiment_results\n\nprint(f\"--- RE Final Training for {MODEL_NAME_RE} ({NUM_TRAIN_EPOCHS_FINAL_RE} epochs) ---\")\n\nre_final_output_dir = Path(OUTPUT_BASE_DIR) / f\"{MODEL_NAME_RE.replace('/', '_')}-re-final\"\n\nif len(re_tokenized_train) == 0 or len(re_tokenized_dev) == 0:\n    print(\"Skipping RE final training as tokenized train or dev dataset is empty.\")\n    final_re_metrics = {key: 0.0 for key in ['eval_loss', 'eval_accuracy', 'eval_f1_weighted', 'eval_precision_weighted', 'eval_recall_weighted', 'eval_f1_actual_relations_micro']}\nelse:\n    try:\n        re_final_args = TrainingArguments(\n            output_dir=str(re_final_output_dir),\n            per_device_train_batch_size=8, \n            per_device_eval_batch_size=16,\n            learning_rate=2e-5, \n            num_train_epochs=NUM_TRAIN_EPOCHS_FINAL_RE,\n            weight_decay=0.01,\n            logging_strategy=\"epoch\",\n            eval_strategy=\"epoch\",       # <--- CORRECTED from evaluation_strategy\n            save_strategy=\"epoch\",\n            load_best_model_at_end=True,\n            metric_for_best_model=\"eval_f1_weighted_overall\", # Ensure this key matches output of compute_metrics_re_revised\n            save_total_limit=1,\n            fp16=torch.cuda.is_available(),\n            report_to=\"none\",\n            seed=SEED,\n            disable_tqdm=False\n        )\n        re_final_model_config = AutoConfig.from_pretrained(\n            MODEL_NAME_RE, \n            num_labels=num_re_labels, \n            id2label=re_id_to_label, \n            label2id=re_label_to_id\n        )\n        re_final_model = AutoModelForSequenceClassification.from_pretrained(\n            MODEL_NAME_RE, \n            config=re_final_model_config, \n            ignore_mismatched_sizes=True\n        )\n        \n        # Check if tokenizer and model vocab size match, especially if special tokens were added to re_tokenizer\n        # This is a common source of errors if not handled.\n        # if 're_tokenizer' in globals() and hasattr(re_tokenizer, 'vocab_size') and \\\n        #   len(re_tokenizer) != re_final_model.config.vocab_size:\n        #    print(f\"Warning: RE Tokenizer vocab size ({len(re_tokenizer)}) and model vocab size ({re_final_model.config.vocab_size}) mismatch.\")\n        #    print(\"Attempting to resize model token embeddings for RE final run. This is needed if special tokens were added to the tokenizer AFTER the base model was loaded for another task (e.g. NER).\")\n        #    re_final_model.resize_token_embeddings(len(re_tokenizer))\n\n\n        re_final_trainer = Trainer(\n            model=re_final_model, \n            args=re_final_args, \n            train_dataset=re_tokenized_train,\n            eval_dataset=re_tokenized_dev, \n            data_collator=re_data_collator, # Ensure re_data_collator is defined\n            compute_metrics=compute_metrics_re_revised # Ensure compute_metrics_re_revised is defined\n        )\n        print(\"Starting RE final training...\")\n        re_final_trainer.train()\n        print(\"RE final training completed.\")\n        print(\"Evaluating final RE model (best checkpoint automatically loaded)...\")\n        final_re_metrics = re_final_trainer.evaluate()\n        print(f\"RE Final Metrics ({MODEL_NAME_RE}):\")\n        for k, v_metric in final_re_metrics.items():\n            print(f\"  {k}: {v_metric:.4f}\")\n        re_final_trainer.save_model(str(re_final_output_dir / \"best_model\"))\n        print(f\"Final RE model saved to {re_final_output_dir / 'best_model'}\")\n    except Exception as e:\n        print(f\"Error during RE final training: {e}\")\n        final_re_metrics = {key: 0.0 for key in ['eval_loss', 'eval_accuracy', 'eval_f1_weighted', 'eval_precision_weighted', 'eval_recall_weighted', 'eval_f1_actual_relations_micro']}\n        import traceback; traceback.print_exc()\n\nall_experiment_results['RE Final Training'] = final_re_metrics\n%store all_experiment_results","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-17T14:58:58.567730Z","iopub.execute_input":"2025-05-17T14:58:58.568001Z"}},"outputs":[{"name":"stdout","text":"no stored variable or alias MODEL_NAME_RE\nno stored variable or alias NUM_TRAIN_EPOCHS_FINAL_RE\nno stored variable or alias OUTPUT_BASE_DIR\nno stored variable or alias SEED\n--- RE Final Training for t5-small (10 epochs) ---\n","output_type":"stream"},{"name":"stderr","text":"Some weights of T5ForSequenceClassification were not initialized from the model checkpoint at t5-small and are newly initialized: ['classification_head.dense.bias', 'classification_head.dense.weight', 'classification_head.out_proj.bias', 'classification_head.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"name":"stdout","text":"Starting RE final training...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='2202' max='3930' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [2202/3930 08:40 < 06:48, 4.23 it/s, Epoch 5.60/10]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Epoch</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Accuracy Overall</th>\n      <th>F1 Weighted Overall</th>\n      <th>Precision Weighted Overall</th>\n      <th>Recall Weighted Overall</th>\n      <th>F1 Actual Relations Micro</th>\n      <th>Precision Actual Relations Micro</th>\n      <th>Recall Actual Relations Micro</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>0.831800</td>\n      <td>0.000004</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>1.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n      <td>0.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Inside compute_metrics_re_revised:\n  Type of predictions_logits_input: <class 'tuple'>\n  predictions_logits_input is a tuple. Using the first element.\n  Shape of actual_logits (as ndarray): (1390, 69)\nInside compute_metrics_re_revised:\n  Type of predictions_logits_input: <class 'tuple'>\n  predictions_logits_input is a tuple. Using the first element.\n  Shape of actual_logits (as ndarray): (1390, 69)\nInside compute_metrics_re_revised:\n  Type of predictions_logits_input: <class 'tuple'>\n  predictions_logits_input is a tuple. Using the first element.\n  Shape of actual_logits (as ndarray): (1390, 69)\nInside compute_metrics_re_revised:\n  Type of predictions_logits_input: <class 'tuple'>\n  predictions_logits_input is a tuple. Using the first element.\n  Shape of actual_logits (as ndarray): (1390, 69)\nInside compute_metrics_re_revised:\n  Type of predictions_logits_input: <class 'tuple'>\n  predictions_logits_input is a tuple. Using the first element.\n  Shape of actual_logits (as ndarray): (1390, 69)\n","output_type":"stream"}],"execution_count":null},{"id":"re-hpo-section-markdown","cell_type":"markdown","source":"## 22. RE: Hyperparameter Optimization (Optuna) - Placeholder\n\nIf `NUM_OPTUNA_TRIALS_RE` is set > 0, this section would run HPO for the RE task, similar to NER HPO. This involves defining an `re_objective_optuna` function and using Optuna to find the best hyperparameters. For this iteration, it's a placeholder but the structure would mirror the NER HPO cells.","metadata":{}},{"id":"final-results-table-markdown","cell_type":"markdown","source":"## 23. Final Results Summary","metadata":{}},{"id":"final-results-table-code","cell_type":"code","source":"%store -r all_experiment_results\n\nprint(\"--- Aggregated Experiment Results ---\")\n\nresults_data = []\nfor experiment_name, metrics in all_experiment_results.items():\n    if isinstance(metrics, dict): # Ensure metrics is a dict\n        results_data.append({\n            \"Experiment\": experiment_name,\n            \"Eval Loss\": metrics.get('eval_loss', float('nan')),\n            \"F1\": metrics.get('eval_f1', metrics.get('eval_f1_weighted', metrics.get('eval_f1_weighted_overall', float('nan')))), # Try different F1 keys\n            \"Accuracy\": metrics.get('eval_accuracy', metrics.get('eval_accuracy_overall', float('nan'))),\n            \"Token Acc (NER)\": metrics.get('eval_token_accuracy', float('nan')),\n            \"Precision\": metrics.get('eval_precision', metrics.get('eval_precision_weighted', metrics.get('eval_precision_weighted_overall', float('nan')))),\n            \"Recall\": metrics.get('eval_recall', metrics.get('eval_recall_weighted', metrics.get('eval_recall_weighted_overall', float('nan'))))\n        })\n    else:\n        print(f\"Warning: Metrics for '{experiment_name}' is not a dictionary: {metrics}\")\n\nif results_data:\n    results_df = pd.DataFrame(results_data)\n    results_df = results_df.set_index(\"Experiment\")\n    # Format float columns\n    float_cols = results_df.select_dtypes(include=['float']).columns\n    for col in float_cols:\n        results_df[col] = results_df[col].apply(lambda x: f\"{x:.4f}\" if not pd.isna(x) else \"N/A\")\n    \n    print(\"\\nFinal Performance Summary Table:\")\n    print(results_df.to_markdown())\nelse:\n    print(\"No results were collected to display in the summary table.\")\n\nprint(\"\\n--- Notes on Results ---\")\nprint(\"- 'F1' for NER refers to seqeval's overall_f1 (entity-level). For RE, it's 'f1_weighted_overall'.\")\nprint(\"- 'Accuracy' for NER refers to seqeval's overall_accuracy (entity-level). For RE, it's 'accuracy_overall'.\")\nprint(\"- 'Token Acc (NER)' is a separate token-wise accuracy for NER.\")\nprint(\"- Extremely low scores are expected due to the very small dataset size.\")\nprint(\"- If T5 models show 'missing keys' for embeddings during 'load_best_model_at_end', NER results are unreliable.\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"id":"final-test-eval-markdown","cell_type":"markdown","source":"## 24. Final Test Set Evaluation (Conceptual Placeholder)\n\nAfter identifying the best performing NER and RE models on the development set, those specific saved models should be evaluated **once** on the `test_docs_raw` to get a final, unbiased performance measure. This step is crucial for reporting final results but is not automated in this notebook to prevent accidental multiple evaluations on the test set.\n\n**Conceptual Steps for Test Evaluation:**\n1.  Load your best saved NER model and its tokenizer.\n2.  Preprocess `test_docs_raw` for NER (tokenize, align labels if available for scoring, otherwise just tokenize for prediction).\n3.  Run NER predictions and evaluate if gold test labels exist.\n4.  Load your best saved RE model and its tokenizer.\n5.  Prepare RE test examples: Use entities (either gold from test data or predicted by your NER model) and sentence context from `test_docs_raw`. Tokenize these examples.\n6.  Run RE predictions and evaluate if gold test relations exist.","metadata":{}},{"id":"conclusion-markdown","cell_type":"markdown","source":"## 25. Conclusion and Next Steps\n\nThis notebook has been structured to perform NER and RE tasks, including baseline evaluations, various fine-tuning strategies for NER, and a baseline for RE. It also includes a framework for hyperparameter optimization and a summary of results.\n\n**Key Considerations for Improving Results Beyond this Structural Revision:**\n1.  **CRITICAL: DATASET SIZE & QUALITY:** The single most impactful factor will be using a significantly larger (thousands of examples), high-quality, and diverse training dataset for both NER and RE.\n2.  **SUFFICIENT TRAINING DURATION:** With more data, increase `NUM_TRAIN_EPOCHS_FINAL_NER` and `NUM_TRAIN_EPOCHS_FINAL_RE` (e.g., 10-50+ epochs, or use `max_steps`) to allow models to converge.\n3.  **ROBUST HYPERPARAMETER OPTIMIZATION (HPO):** Conduct more extensive HPO (more trials, longer training per trial) for both NER and RE once you have sufficient data and adequate training time per trial.\n4.  **MODEL SELECTION:** Experiment with different pre-trained models. For NER, BERT-based models (`bert-base-cased`, `roberta-base`, etc.) are often strong. For RE, various architectures can be explored.\n5.  **T5 EMBEDDING WARNING (NER):** If the `missing keys` warning for T5 embeddings persists during `load_best_model_at_end`, this indicates a fundamental issue with how embeddings are handled or saved/loaded for `T5ForTokenClassification` in your setup. This will severely hamper NER performance and needs to be resolved (e.g., by checking library versions, model checkpoint structure, or switching to a different base model for NER).\n6.  **RE DATA PREPARATION:** The strategy for generating RE examples (especially negative `NO_RELATION` instances and constructing input sequences with entity information) is crucial and can be refined.\n7.  **ERROR ANALYSIS:** After training, thoroughly analyze the errors made by your models on the development set to understand weaknesses and guide improvements (e.g., more data for specific classes, feature engineering, rule-based post-processing).\n\nThis notebook provides a foundation. True high-performance NLP requires iterative experimentation, a strong understanding of your data, and careful tuning.","metadata":{}},{"id":"c36db3be-dd8d-4f63-90eb-987e706330b9","cell_type":"markdown","source":"What this means together:\n\nYour model has very quickly learned that the safest bet to minimize loss and maximize overall accuracy on your highly imbalanced dataset is to predict \"NO_RELATION\" for almost every entity pair. It's doing an excellent job at that, hence the perfect overall scores. However, it has learned nothing about the actual, meaningful relationships between entities, hence the zero scores for \"actual relations.\"\n\nWhy is this happening?\n\nExtreme Class Imbalance: As seen in your EDA (and the sample output for RE data generation in cell #42 of the notebook, which showed 6284 \"NO_RELATION\" examples in training), the \"NO_RELATION\" class likely dwarfs all other individual relation types. The model has many more examples of \"NO_RELATION\" to learn from.\nInsufficient Signal for Minority Classes: With only 51 original documents to source your RE examples, the number of instances for each specific relation type (e.g., \"Founded\", \"Creator\") is probably very small. There isn't enough data for the model to learn the complex patterns that distinguish these specific relations.\nLoss Function Domination: Standard cross-entropy loss will be dominated by the majority class. The model gets high rewards for correctly predicting \"NO_RELATION\" and little penalty for misclassifying the rare actual relations as \"NO_RELATION\".\nIs this \"bad\"?\n\nIf your goal is to identify specific relationships, then yes, these results indicate the model is not achieving that goal, despite the misleadingly perfect overall scores.\nThe eval_f1_actual_relations_micro is the metric you should focus on if you care about identifying true relations.","metadata":{}}]}